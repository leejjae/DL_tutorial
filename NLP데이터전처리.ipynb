{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6613d3-7878-49ae-80fd-48e35ff065dc",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eebfce-6a31-4306-b27c-ecaedc30715d",
   "metadata": {},
   "source": [
    "## 영어의 Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96197a4-c068-435a-9ad9-99124b3d4267",
   "metadata": {},
   "source": [
    "####  **NLTK의 토그나이저 1. word_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca65849b-9122-41f3-895d-fb4e778b353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244dab3d-f190-4ac1-bf0a-3a18b64a25c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1f8c50-f4f0-41fc-abef-7fe9d1c44c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b78e6b-52c3-4ba6-a2bf-36d2d6b54407",
   "metadata": {},
   "source": [
    "#### Q. 아포스트로피가 들어간 상황에서 단어 토근화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e444c283-dae6-4328-9d33-f0e33bbb6afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d86a4e-d1c5-4e92-8174-59841415f48e",
   "metadata": {},
   "source": [
    "#### A. Don't를 Do와 n't로 분리, Jone's는 Jone과 's로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce24f43-8f87-4ecc-8663-b5d4ace5eed0",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1b7fe-dab7-4449-b43d-92b66cec0c17",
   "metadata": {},
   "source": [
    "#### **NLTK의 토그나이저 2. WordPunctTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7b9a9c-e45c-412a-b970-bb0aeccefaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54aba04-cc4c-4a0f-bf77-8d7198e1f45c",
   "metadata": {},
   "source": [
    "#### Don't를 Don, ', t로 분리. Jone's를 Jone, ', s로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453192fd-4cd4-4130-8dcf-6402fcd1d38f",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ad051-585a-4ff0-9d44-b2c8021da8b9",
   "metadata": {},
   "source": [
    "#### **NLTK의 토그나이저 3. TreebankWordTokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2a7ac-c368-4faf-80b6-40346da90ff3",
   "metadata": {},
   "source": [
    "Penn Treebank Tokenizer의 규칙\\\n",
    "규칙1. 하이푼으로 구서된 단어는 하나로 유지\\\n",
    "규칙2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcda2c96-8399-4834-936d-b472fa6a7f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec1763-0aec-4e8e-a249-7230ced0efec",
   "metadata": {},
   "source": [
    "## 한글의 Word Tokenization(KoNLPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b42376-6857-4ffb-b2cf-3b86899a348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad06704-6661-44ce-ad8e-9330def75510",
   "metadata": {},
   "source": [
    "**위 형태소 분석기들은 공통적으로 아래의 함수를 제공합니다.**\\\n",
    "nouns: 명사 추출\\\n",
    "morphs: 형태소 추출\\\n",
    "pos: 품사 부착/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc4c5ab-3cd3-4699-bd4c-96cda098c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45bc58-5f5a-4f5e-8f45-b1da416315da",
   "metadata": {},
   "source": [
    "### 형태소 분석 Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabededb-d6f1-48ce-bb8c-612f057bf749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n",
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(sentence_))\n",
    "print(okt.morphs(sentence_))\n",
    "print(okt.pos(sentence_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b359de-de28-4bb8-bdaf-c64ca717aef3",
   "metadata": {},
   "source": [
    "### 형태소 분석기 꼬꼬마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9d0004-03a1-4b04-be98-457f047b32e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n",
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(sentence_))\n",
    "print(kkma.morphs(sentence_))\n",
    "print(kkma.pos(sentence_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a9517-bc5d-4bb2-8e9f-0b322393e5d5",
   "metadata": {},
   "source": [
    "### 형태소 분석기 코모란"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c9c205-bf94-4789-bdf8-d6fc1726ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코', '당신', '연휴', '여행']\n",
      "['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '아', '보', '아요']\n",
      "[('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "print(komoran.nouns(sentence_))\n",
    "print(komoran.morphs(sentence_))\n",
    "print(komoran.pos(sentence_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3624c-c9a5-49b7-bbfb-4e4304ddd70e",
   "metadata": {},
   "source": [
    "### 형태소 분석기 한나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2b73f7-8232-4961-9d21-929dfd052c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n",
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에는', '여행', '을', '가', '아', '보', '아']\n",
      "[('열심히', 'M'), ('코딩', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('당신', 'N'), (',', 'S'), ('연휴', 'N'), ('에는', 'J'), ('여행', 'N'), ('을', 'J'), ('가', 'P'), ('아', 'E'), ('보', 'P'), ('아', 'E')]\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.nouns(sentence_))\n",
    "print(hannanum.morphs(sentence_))\n",
    "print(hannanum.pos(sentence_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02666e9f-f8db-4553-bd4d-4fbe20deb07e",
   "metadata": {},
   "source": [
    "### 형태소 분석기 Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f1471d-7bf5-447d-9de1-2790fef8d1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n",
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '봐요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')]\n"
     ]
    }
   ],
   "source": [
    "print(mecab.nouns(sentence_))\n",
    "print(mecab.morphs(sentence_))\n",
    "print(mecab.pos(sentence_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50a7cf-f6c2-4615-b999-a9db7ff6584e",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e196f-1f3a-4e07-b5f9-dbf8aa97b1f5",
   "metadata": {},
   "source": [
    "## 영어의 Sentence Tokenization(NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3abd1aa-f085-4c7c-a715-ccfad99e514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 'Yonsei University is a private research university in Seoul, South Korea. Yonsei University is deemed as one of the three most prestigious institutions in the country. It is particularly respected in the studies of medicine and business administration.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe5a5ab6-b8ac-4450-a62a-a09a7a713da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yonsei University is a private research university in Seoul, South Korea',\n",
       " 'Yonsei University is deemed as one of the three most prestigious institutions in the country',\n",
       " 'It is particularly respected in the studies of medicine and business administration.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.split('. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8515a-3f20-4e0b-89f6-39eee8c79483",
   "metadata": {},
   "source": [
    "**\"온점을 기준으로 문장을 구분할 경우에는 예외사항이 너무 많다.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f019a8-58d6-4995-9313-0d3ef2a1b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830de2ce-be0b-4570-9def-23b3b28e517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23e0f9-08ba-4350-b6c4-5fa682c013dd",
   "metadata": {},
   "source": [
    "## 한국어의 Sentence Tokenization(KSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7514ba61-2d43-4d97-ac4a-c79286a11096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2503b07f-e0a4-4357-bc9d-25b82d64997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77395d4-3551-49b0-9027-6d86a77c74cc",
   "metadata": {},
   "source": [
    "### IDBM 리뷰 데이터를 이용한 정수 인코딩과 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0370e150-8ea4-4f0c-9870-a25273dc073c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ffb75af-f940-41f8-bb7a-3694f6fd5391",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
