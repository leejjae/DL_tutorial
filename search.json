[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LMTM.html",
    "href": "posts/LMTM.html",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\nÏòÅÌôî Î¶¨Î∑∞ ÌÖçÏä§Ìä∏(many)Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏïÑ Í∏çÏ†ï ÎòêÎäî Î∂ÄÏ†ï(one)ÏùÑ Ï∂úÎ†•ÌïòÎäî Íµ¨Ï°∞\nEmbedding: ÏòÅÌôî Î¶¨Î∑∞(text)Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÎäî Ïó∞ÏÇ∞\nLSTM: ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÍ∏∞ ÏúÑÌïú Íµ¨Ï°∞\nLinear: Í≤∞Í≥º Ï∂úÎ†•"
  },
  {
    "objectID": "posts/LMTM.html#sentimental-analysis",
    "href": "posts/LMTM.html#sentimental-analysis",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\nÏòÅÌôî Î¶¨Î∑∞ ÌÖçÏä§Ìä∏(many)Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏïÑ Í∏çÏ†ï ÎòêÎäî Î∂ÄÏ†ï(one)ÏùÑ Ï∂úÎ†•ÌïòÎäî Íµ¨Ï°∞\nEmbedding: ÏòÅÌôî Î¶¨Î∑∞(text)Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÎäî Ïó∞ÏÇ∞\nLSTM: ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÍ∏∞ ÏúÑÌïú Íµ¨Ï°∞\nLinear: Í≤∞Í≥º Ï∂úÎ†•"
  },
  {
    "objectID": "posts/LMTM.html#step-1-load-libraries-datsets",
    "href": "posts/LMTM.html#step-1-load-libraries-datsets",
    "title": "LMTM",
    "section": "Step 1 : Load libraries & Datsets",
    "text": "Step 1 : Load libraries & Datsets\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport os\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom collections import Counter\n\n\ndata = pd.read_csv('exercise4.csv')\ndata.head()\n\n\n\n\n\n\n\n\nprocessed\nlabel\n\n\n\n\n0\nOne reviewer mentioned watching Oz episode hoo...\n1\n\n\n1\nA wonderful little production . The filming te...\n1\n\n\n2\nI thought wonderful way spend time hot summer ...\n1\n\n\n3\nBasically family little boy Jake think zombie ...\n0\n\n\n4\nPetter Mattei Love Time Money visually stunnin...\n1\n\n\n\n\n\n\n\n\ndata['processed'][0]\n\n'One reviewer mentioned watching Oz episode hooked . They right , exactly happened . The first thing struck Oz brutality unflinching scene violence , set right word GO . Trust , show faint hearted timid . This show pull punch regard drug , sex violence . Its hardcore , classic use word . It called OZ nickname given Oswald Maximum Security State Penitentary . It focus mainly Emerald City , experimental section prison cell glass front face inwards , privacy high agenda . Em City home many . . Aryans , Muslims , gangsta , Latinos , Christians , Italians , Irish . . . . scuffle , death stare , dodgy dealing shady agreement never far away . I would say main appeal show due fact go show dare . Forget pretty picture painted mainstream audience , forget charm , forget romance . . . OZ mess around . The first episode I ever saw struck nasty surreal , I say I ready , I watched , I developed taste Oz , got accustomed high level graphic violence . Not violence , injustice crooked guard sold nickel , inmate kill order get away , well mannered , middle class inmate turned prison bitch due lack street skill prison experience Watching Oz , may become comfortable uncomfortable viewing . . . . thats get touch darker side .'\n\n\n\ndata['processed'] = data['processed'].str.lower().replace(r\"[^a-zA-Z ]\", \"\", regex=True)\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\n- ÏÇ¨Ï†ÑÏÉùÏÑ±\n\nÎ¶¨Î∑∞ Î¨∏Ïû•Ïóê Îì§Ïñ¥ÏûàÎäî Îã®Ïñ¥Îì§ÏùÑ Ï∂îÏ∂úÌïòÍ≥†, Í∞ÅÍ∞ÅÏùò Îã®Ïñ¥Ïóê Ïà´ÏûêÎ•º Î∂ÄÏó¨ÌïòÎäî ÏûëÏóÖ\n[‚Äòone‚Äô, ‚Äòreviewer‚Äô, ‚Äòmentioned‚Äô, ‚Äòwatching‚Äô, ‚Äòoz‚Äô, ‚Äòepisode‚Äô, ‚Äòhooked‚Äô]\n\n\n# Î¨∏Ïû•Ïóê Ìè¨Ìï®Îêú Îã®Ïñ¥ ÌÜ†ÌÅ∞Ìôî\nreviews = data['processed'].values\nwords = ' '.join(reviews).split()\nwords[:10]\n\n['one',\n 'reviewer',\n 'mentioned',\n 'watching',\n 'oz',\n 'episode',\n 'hooked',\n 'they',\n 'right',\n 'exactly']\n\n\n\ncounter = Counter(words)\nvocab = sorted(counter, key=counter.get, reverse=True)\nint2word = dict(enumerate(vocab, 1))\nint2word[0] = '&lt;PAD&gt;'\nword2int = {word: id for id, word in int2word.items()}\n#word2int\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\n\n- Î¶¨Î∑∞ Ïù∏ÏΩîÎî©\n\nÎ¶¨Î∑∞Ïóê Ìè¨Ìï®Îêú Îã®Ïñ¥Î•º Ïà´ÏûêÌòïÌÉúÎ°ú Î≥ÄÌôòÌïòÎäî ÏûëÏóÖ\n{‚Äòi‚Äô: 1, ‚Äòmovie‚Äô: 2, ‚Äòfilm‚Äô: 3, ‚Äòthe‚Äô: 4, ‚Äòone‚Äô: 5, ‚Äòlike‚Äô: 6, ‚Äòit‚Äô: 7, ‚Äòtime‚Äô: 8, ‚Äòthis‚Äô: 9, ‚Äògood‚Äô: 10, ‚Äòcharacter‚Äô: 11,‚Ä¶}\n\n\nreviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:02&lt;00:00, 24930.36it/s]\n\n\n\nreviews_enc[0][1:10]\n\n[1095, 972, 74, 2893, 186, 2982, 119, 114, 538]\n\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\nword2int['one'], word2int['reviewer'], word2int['mentioned']\n\n(5, 1095, 972)\n\n\n\ndata['encoded'] = reviews_enc\ndata['encoded']\n\n0        [5, 1095, 972, 74, 2893, 186, 2982, 119, 114, ...\n1        [45, 311, 53, 247, 4, 1270, 1633, 16086, 78, 8...\n2        [1, 97, 311, 28, 1053, 8, 763, 1343, 2345, 112...\n3        [591, 130, 53, 221, 3123, 33, 565, 3653, 608, ...\n4        [57645, 9676, 39, 8, 203, 1993, 1312, 3, 37, 3...\n                               ...                        \n49995    [1, 97, 2, 114, 10, 191, 7, 1413, 128, 26, 913...\n49996    [22, 40, 22, 307, 22, 50, 2847, 869, 545, 1364...\n49997    [1, 3168, 4064, 34678, 7571, 269, 4234, 4064, ...\n49998    [1, 86, 2839, 825, 369, 348, 9292, 5, 9, 208, ...\n49999    [264, 5, 5445, 109, 1941, 2, 213, 328, 123, 44...\nName: encoded, Length: 50000, dtype: object\n\n\n\n\n- Í∏∏Ïù¥ ÎßûÏ∂∞Ï£ºÍ∏∞(padding or trim)\n\nÏã†Í≤ΩÎßùÏùò ÏûÖÎ†•ÏúºÎ°ú ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ ÏùºÏ†ï Í∏∏Ïù¥ÎßåÌÅº ÎßûÏ∂∞Ï£ºÎäî ÏûëÏóÖ\nÍ∏∏Ïù¥Í∞Ä Í∏¥ Î¨∏Ïû•ÏùÄ ÏûòÎùºÏ£ºÍ≥†(trim), Í∏∏Ïù¥Í∞Ä ÏßßÏùÄ Î¨∏Ïû•ÏùÄ Ï±ÑÏõåÏ£ºÎäî(padding) ÏûëÏóÖ\n\n\ndef pad_features(reviews, pad_id, seq_length=128):\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        features[i, :len(row)] = np.array(row)[:seq_length]\n\n    return features\n\nseq_length = 256\nfeatures = pad_features(reviews_enc, pad_id=word2int['&lt;PAD&gt;'], seq_length=seq_length)\n\nassert len(features) == len(reviews_enc)\nassert len(features[0]) == seq_length\n\n\nnp.full((5,3),2)\n\narray([[2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2]])\n\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\nlabels = data['label'].to_numpy()\nlabels\n\narray([1, 1, 1, ..., 0, 0, 0])\n\n\n\n\n- Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n\n# train test split\ntrain_size = .8\nsplit_id = int(len(features) * train_size)\ntrain_x, test_x, train_y, test_y = features[:split_id], features[split_id:], labels[:split_id], labels[split_id:]\n\nsplit_id = int(len(train_x) * train_size)\ntrain_x, valid_x, train_y, valid_y = train_x[:split_id], train_x[split_id:], train_y[:split_id], train_y[split_id:]\n\nprint('Train X shape: {}, Valid X shape: {}, Test X shape: {}'.format(train_x.shape, valid_x.shape, test_x.shape))\nprint('Train y shape: {}, Valid y shape: {}, Test y shape: {}'.format(train_y.shape, valid_y.shape, test_y.shape))\n\nTrain X shape: (32000, 256), Valid X shape: (8000, 256), Test X shape: (10000, 256)\nTrain y shape: (32000,), Valid y shape: (8000,), Test y shape: (10000,)"
  },
  {
    "objectID": "posts/LMTM.html#step-2-create-dataloader",
    "href": "posts/LMTM.html#step-2-create-dataloader",
    "title": "LMTM",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# set hyperparameter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nlr = 0.001\nbatch_size = 128\nvocab_size = len(word2int)\nembedding_size = 256\ndropout = 0.25\n\nepochs = 8\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'epochs': epochs\n}\n\nes_limit = 5\n\ncuda\n\n\n\ntrainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalidset = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntestset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\ntrainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\nvalloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\ntestloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
  },
  {
    "objectID": "posts/LMTM.html#step-3-set-network-structure",
    "href": "posts/LMTM.html#step-3-set-network-structure",
    "title": "LMTM",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_size=400):\n        super(LSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, 512, 2, dropout=0.25, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(512, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = x.long()\n        x = self.embedding(x)\n        o, _ = self.lstm(x)\n        o = o[:, -1, :]\n        o = self.dropout(o)\n        o = self.fc(o)\n        o = self.sigmoid(o)\n\n        return o"
  },
  {
    "objectID": "posts/LMTM.html#step-4-create-model-instance",
    "href": "posts/LMTM.html#step-4-create-model-instance",
    "title": "LMTM",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\nmodel = LSTMClassifier(vocab_size, embedding_size).to(device)\nprint(model)\n\nLSTMClassifier(\n  (embedding): Embedding(96140, 256)\n  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
  },
  {
    "objectID": "posts/LMTM.html#step-5-model-compile",
    "href": "posts/LMTM.html#step-5-model-compile",
    "title": "LMTM",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\ncriterion = nn.BCELoss()\noptim = Adam(model.parameters(), lr=lr)"
  },
  {
    "objectID": "posts/LMTM.html#step-6-set-train-loop",
    "href": "posts/LMTM.html#step-6-set-train-loop",
    "title": "LMTM",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train(model, trainloader):\n    model.train()\n\n    train_loss = 0\n    train_acc = 0\n\n    for id, (X, y) in enumerate(trainloader):\n        X, y = X.to(device), y.to(device)\n        optim.zero_grad()\n        y_pred = model(X)\n        loss = criterion(y_pred.squeeze(), y.float())\n        loss.backward()\n        optim.step()\n\n        train_loss += loss.item()\n        y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n        equals = y_pred == y\n        acc = torch.mean(equals.type(torch.FloatTensor))\n        train_acc += acc.item()\n\n    history['train_loss'].append(train_loss / len(trainloader))\n    history['train_acc'].append(train_acc / len(trainloader))\n\n    return train_loss, train_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-7-set-test-loop",
    "href": "posts/LMTM.html#step-7-set-test-loop",
    "title": "LMTM",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef validation(model, valloader):\n    model.eval()\n\n    val_loss = 0\n    val_acc = 0\n\n    with torch.no_grad():\n        for id, (X,y) in enumerate(valloader):\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss = criterion(y_pred.squeeze(), y.float())\n\n            val_loss += loss.item()\n\n            y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n            equals = y_pred == y\n            acc = torch.mean(equals.type(torch.FloatTensor))\n            val_acc += acc.item()\n\n        history['val_loss'].append(val_loss / len(valloader))\n        history['val_acc'].append(val_acc / len(valloader))\n\n    return val_loss, val_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-8-run-model",
    "href": "posts/LMTM.html#step-8-run-model",
    "title": "LMTM",
    "section": "Step 8 : Run Model",
    "text": "Step 8 : Run Model\n\n# train loop\nepochloop = tqdm(range(epochs), desc='Training')\n\n# early stop trigger\nes_trigger = 0\nval_loss_min = torch.inf\n\nfor e in epochloop:\n    train_loss, train_acc = train(model, trainloader)\n    val_loss, val_acc = validation(model, valloader)\n    epochloop.write(f'Epoch[{e+1}/{epochs}] Train Loss: {train_loss / len(trainloader):.3f}, Train Acc: {train_acc / len(trainloader):.3f}, Val Loss: {val_loss / len(valloader):.3f}, Val Acc: {val_acc / len(valloader):.3f}')\n\n    # save model if validation loss decrease\n    if val_loss / len(valloader) &lt;= val_loss_min:\n        torch.save(model.state_dict(), './sentiment_lstm.pt')\n        val_loss_min=val_loss / len(valloader)\n        es_trigger = 0\n\n    else:\n        es_trigger += 1\n\n    # early stop\n    if es_trigger &gt;= es_limit:\n        epochloop.write(f'Early stopped at Epoch-{e+1}')\n        history['epochs'] = e+1\n        break\n\nTraining:   0%|                                                                                   | 0/8 [00:12&lt;?, ?it/s]Training:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                        | 2/8 [00:23&lt;01:10, 11.75s/it]Training:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                        | 2/8 [00:32&lt;01:10, 11.75s/it]Training:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                              | 3/8 [00:42&lt;00:53, 10.67s/it]Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 4/8 [00:51&lt;00:40, 10.24s/it]Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 5/8 [01:01&lt;00:29,  9.82s/it]Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 7/8 [01:11&lt;00:09,  9.95s/it]Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:21&lt;00:00, 10.20s/it]\n\n\nEpoch[1/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.496\nEpoch[2/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.514\nEpoch[3/8] Train Loss: 0.669, Train Acc: 0.570, Val Loss: 0.650, Val Acc: 0.706\nEpoch[4/8] Train Loss: 0.478, Train Acc: 0.800, Val Loss: 0.601, Val Acc: 0.772\nEpoch[5/8] Train Loss: 0.448, Train Acc: 0.811, Val Loss: 0.405, Val Acc: 0.818\nEpoch[6/8] Train Loss: 0.314, Train Acc: 0.879, Val Loss: 0.357, Val Acc: 0.847\nEpoch[7/8] Train Loss: 0.237, Train Acc: 0.914, Val Loss: 0.384, Val Acc: 0.842\nEpoch[8/8] Train Loss: 0.203, Train Acc: 0.932, Val Loss: 0.426, Val Acc: 0.856\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_acc'][:history['epochs']], label='Train Acc')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Acc')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_loss'][:history['epochs']], label='Train Loss')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/VGGNet.html",
    "href": "posts/VGGNet.html",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet Íµ¨Ï°∞ ÏÇ¥Ìé¥Î≥¥Í∏∞\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#vggnet",
    "href": "posts/VGGNet.html#vggnet",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet Íµ¨Ï°∞ ÏÇ¥Ìé¥Î≥¥Í∏∞\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "href": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "title": "VGGNet",
    "section": "Step 1 : Load libraries & Datasets",
    "text": "Step 1 : Load libraries & Datasets\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-2-data-preprocessing",
    "href": "posts/VGGNet.html#step-2-data-preprocessing",
    "title": "VGGNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\nÎ∂àÎü¨Ïò® Ïù¥ÎØ∏ÏßÄÏùò Ï¶ùÍ∞ïÏùÑ ÌÜµÌï¥ ÌïôÏäµ Ï†ïÌôïÎèÑÎ•º Ìñ•ÏÉÅÏãúÌÇ§ÎèÑÎ°ù Ìï©ÎãàÎã§.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform,\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ntrain_img.data.shape\n\n(50000, 32, 32, 3)"
  },
  {
    "objectID": "posts/VGGNet.html#step-3-set-hyperparameters",
    "href": "posts/VGGNet.html#step-3-set-hyperparameters",
    "title": "VGGNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 32\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/VGGNet.html#step-4-create-dataloader",
    "href": "posts/VGGNet.html#step-4-create-dataloader",
    "title": "VGGNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\ntrain_loader = DataLoader(train_img, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img, batch_size = batch_sizes, shuffle = False)"
  },
  {
    "objectID": "posts/VGGNet.html#step-5-set-network-structure",
    "href": "posts/VGGNet.html#step-5-set-network-structure",
    "title": "VGGNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\n# Model\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 360),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(360, 100),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(100, 10),\n        )\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),  # Ï∂îÍ∞Ä\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n                \n        return nn.Sequential(*layers)"
  },
  {
    "objectID": "posts/VGGNet.html#step-6-create-model-instance",
    "href": "posts/VGGNet.html#step-6-create-model-instance",
    "title": "VGGNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\nmodel = VGG('VGG16').to(device)\nprint(model)\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=360, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=360, out_features=100, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=100, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/VGGNet.html#step-7-model-compile",
    "href": "posts/VGGNet.html#step-7-model-compile",
    "title": "VGGNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
  },
  {
    "objectID": "posts/VGGNet.html#step-8-set-train-loop",
    "href": "posts/VGGNet.html#step-8-set-train-loop",
    "title": "VGGNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}   [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/VGGNet.html#step-9-set-test-loop",
    "href": "posts/VGGNet.html#step-9-set-test-loop",
    "title": "VGGNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y  = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-10-run-model",
    "href": "posts/VGGNet.html#step-10-run-model",
    "title": "VGGNet",
    "section": "Step 10 : Run model",
    "text": "Step 10 : Run model\n\nfor i in range(epochs):\n    print(f\"Epoch {i+1} \\n---------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n---------------------------\nloss: 2.336054   [    0]/50000\nloss: 2.201652   [ 3200]/50000\nloss: 1.913308   [ 6400]/50000\nloss: 1.833156   [ 9600]/50000\nloss: 2.025965   [12800]/50000\nloss: 1.458568   [16000]/50000\nloss: 1.454589   [19200]/50000\nloss: 1.586300   [22400]/50000\nloss: 1.777550   [25600]/50000\nloss: 1.902973   [28800]/50000\nloss: 1.534656   [32000]/50000\nloss: 1.808721   [35200]/50000\nloss: 1.449842   [38400]/50000\nloss: 1.357022   [41600]/50000\nloss: 1.378457   [44800]/50000\nloss: 1.557457   [48000]/50000\nTest Error: \n Accuracy: 50.6%, Avg loss: 1.338251\n\nEpoch 2 \n---------------------------\nloss: 1.564054   [    0]/50000\nloss: 1.571676   [ 3200]/50000\nloss: 1.477239   [ 6400]/50000\nloss: 1.520172   [ 9600]/50000\nloss: 1.256163   [12800]/50000\nloss: 1.066607   [16000]/50000\nloss: 1.460073   [19200]/50000\nloss: 1.077838   [22400]/50000\nloss: 1.215548   [25600]/50000\nloss: 0.884830   [28800]/50000\nloss: 1.028723   [32000]/50000\nloss: 1.288996   [35200]/50000\nloss: 1.299563   [38400]/50000\nloss: 0.939404   [41600]/50000\nloss: 0.955451   [44800]/50000\nloss: 1.098657   [48000]/50000\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.923354\n\nEpoch 3 \n---------------------------\nloss: 0.796024   [    0]/50000\nloss: 0.677388   [ 3200]/50000\nloss: 0.617442   [ 6400]/50000\nloss: 1.173936   [ 9600]/50000\nloss: 0.786573   [12800]/50000\nloss: 0.798586   [16000]/50000\nloss: 1.181702   [19200]/50000\nloss: 0.897227   [22400]/50000\nloss: 0.735924   [25600]/50000\nloss: 1.028793   [28800]/50000\nloss: 0.834691   [32000]/50000\nloss: 1.081767   [35200]/50000\nloss: 0.828031   [38400]/50000\nloss: 1.046338   [41600]/50000\nloss: 0.828228   [44800]/50000\nloss: 1.146716   [48000]/50000\nTest Error: \n Accuracy: 70.4%, Avg loss: 0.880181\n\nEpoch 4 \n---------------------------\nloss: 0.617879   [    0]/50000\nloss: 0.876245   [ 3200]/50000\nloss: 0.673582   [ 6400]/50000\nloss: 0.556679   [ 9600]/50000\nloss: 0.699025   [12800]/50000\nloss: 1.006697   [16000]/50000\nloss: 0.683750   [19200]/50000\nloss: 1.277821   [22400]/50000\nloss: 0.562071   [25600]/50000\nloss: 0.686587   [28800]/50000\nloss: 0.873322   [32000]/50000\nloss: 0.719097   [35200]/50000\nloss: 0.457578   [38400]/50000\nloss: 0.514047   [41600]/50000\nloss: 0.729195   [44800]/50000\nloss: 0.858265   [48000]/50000\nTest Error: \n Accuracy: 75.3%, Avg loss: 0.712957\n\nEpoch 5 \n---------------------------\nloss: 0.855375   [    0]/50000\nloss: 0.854320   [ 3200]/50000\nloss: 0.695397   [ 6400]/50000\nloss: 0.525759   [ 9600]/50000\nloss: 0.381186   [12800]/50000\nloss: 0.587416   [16000]/50000\nloss: 0.511339   [19200]/50000\nloss: 1.319725   [22400]/50000\nloss: 0.649993   [25600]/50000\nloss: 0.508207   [28800]/50000\nloss: 0.585140   [32000]/50000\nloss: 0.794928   [35200]/50000\nloss: 0.799448   [38400]/50000\nloss: 0.417046   [41600]/50000\nloss: 0.498251   [44800]/50000\nloss: 0.779942   [48000]/50000\nTest Error: \n Accuracy: 76.6%, Avg loss: 0.682496\n\nEpoch 6 \n---------------------------\nloss: 0.719160   [    0]/50000\nloss: 0.627115   [ 3200]/50000\nloss: 0.255042   [ 6400]/50000\nloss: 0.400026   [ 9600]/50000\nloss: 0.737379   [12800]/50000\nloss: 0.741243   [16000]/50000\nloss: 0.726986   [19200]/50000\nloss: 0.266388   [22400]/50000\nloss: 0.633677   [25600]/50000\nloss: 0.482972   [28800]/50000\nloss: 0.444857   [32000]/50000\nloss: 0.513320   [35200]/50000\nloss: 0.529961   [38400]/50000\nloss: 0.784853   [41600]/50000\nloss: 0.560646   [44800]/50000\nloss: 0.426722   [48000]/50000\nTest Error: \n Accuracy: 75.4%, Avg loss: 0.737327\n\nEpoch 7 \n---------------------------\nloss: 0.456941   [    0]/50000\nloss: 0.552954   [ 3200]/50000\nloss: 0.588921   [ 6400]/50000\nloss: 0.359172   [ 9600]/50000\nloss: 0.380740   [12800]/50000\nloss: 0.230270   [16000]/50000\nloss: 0.544868   [19200]/50000\nloss: 0.470449   [22400]/50000\nloss: 0.716484   [25600]/50000\nloss: 0.427520   [28800]/50000\nloss: 0.485696   [32000]/50000\nloss: 0.250514   [35200]/50000\nloss: 0.619605   [38400]/50000\nloss: 0.534625   [41600]/50000\nloss: 0.294415   [44800]/50000\nloss: 0.676517   [48000]/50000\nTest Error: \n Accuracy: 81.8%, Avg loss: 0.560385\n\nEpoch 8 \n---------------------------\nloss: 0.584514   [    0]/50000\nloss: 0.433411   [ 3200]/50000\nloss: 0.360651   [ 6400]/50000\nloss: 0.707992   [ 9600]/50000\nloss: 0.449344   [12800]/50000\nloss: 0.380623   [16000]/50000\nloss: 0.333079   [19200]/50000\nloss: 0.420316   [22400]/50000\nloss: 0.414326   [25600]/50000\nloss: 0.539709   [28800]/50000\nloss: 0.425368   [32000]/50000\nloss: 0.610167   [35200]/50000\nloss: 0.427243   [38400]/50000\nloss: 0.787724   [41600]/50000\nloss: 0.561038   [44800]/50000\nloss: 0.456995   [48000]/50000\nTest Error: \n Accuracy: 81.5%, Avg loss: 0.566010\n\nEpoch 9 \n---------------------------\nloss: 0.383341   [    0]/50000\nloss: 0.381802   [ 3200]/50000\nloss: 0.258570   [ 6400]/50000\nloss: 0.421782   [ 9600]/50000\nloss: 0.455991   [12800]/50000\nloss: 0.531303   [16000]/50000\nloss: 0.758386   [19200]/50000\nloss: 0.285010   [22400]/50000\nloss: 0.314713   [25600]/50000\nloss: 0.419822   [28800]/50000\nloss: 0.278820   [32000]/50000\nloss: 0.553399   [35200]/50000\nloss: 0.416081   [38400]/50000\nloss: 0.365547   [41600]/50000\nloss: 0.686296   [44800]/50000\nloss: 0.814931   [48000]/50000\nTest Error: \n Accuracy: 83.5%, Avg loss: 0.502169\n\nEpoch 10 \n---------------------------\nloss: 0.275047   [    0]/50000\nloss: 0.573386   [ 3200]/50000\nloss: 0.737843   [ 6400]/50000\nloss: 0.478916   [ 9600]/50000\nloss: 0.429536   [12800]/50000\nloss: 0.238580   [16000]/50000\nloss: 0.406505   [19200]/50000\nloss: 0.228436   [22400]/50000\nloss: 0.370529   [25600]/50000\nloss: 0.344406   [28800]/50000\nloss: 0.301163   [32000]/50000\nloss: 0.257651   [35200]/50000\nloss: 0.833092   [38400]/50000\nloss: 0.897587   [41600]/50000\nloss: 0.397286   [44800]/50000\nloss: 0.637342   [48000]/50000\nTest Error: \n Accuracy: 84.8%, Avg loss: 0.467348\n\nDone!"
  },
  {
    "objectID": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "href": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "title": "VGGNet",
    "section": "## CIFAR Classifier(Pretrained VGGNet)",
    "text": "## CIFAR Classifier(Pretrained VGGNet)\nImageNet Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÌïú VGGNetÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Fine tuning Ìï¥Î¥ÖÎãàÎã§."
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html",
    "href": "posts/CV_ObjectDetection_0.html",
    "title": "CV_ObjectDetection_0",
    "section": "",
    "text": "# Pascal VOC 2007 Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú\n!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n\n--2024-02-09 07:27:15--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n--2024-02-09 07:27:16--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 460032000 (439M) [application/x-tar]\nSaving to: ‚ÄòVOCtrainval_06-Nov-2007.tar.3‚Äô\n\nVOCtrainval_06-Nov- 100%[===================&gt;] 438.72M  15.0MB/s    in 30s     \n\n2024-02-09 07:27:47 (14.5 MB/s) - ‚ÄòVOCtrainval_06-Nov-2007.tar.3‚Äô saved [460032000/460032000]\n\n--2024-02-09 07:27:47--  http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar [following]\n--2024-02-09 07:27:47--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 451020800 (430M) [application/x-tar]\nSaving to: ‚ÄòVOCtest_06-Nov-2007.tar.3‚Äô\n\nVOCtest_06-Nov-2007 100%[===================&gt;] 430.13M  16.1MB/s    in 26s     \n\n2024-02-09 07:29:06 (16.3 MB/s) - ‚ÄòVOCtest_06-Nov-2007.tar.3‚Äô saved [451020800/451020800]\nfrom pathlib import Path\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval').mkdir(parents=True, exist_ok=True)\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/test').mkdir(parents=True, exist_ok=True)\nroot=Path('/root/2024winter/DL_tutorial/posts/pascal_datasets')\n\nfor path1 in ('images', 'labels'):\n    for path2 in ('train2007', 'val2007', 'test2007'):\n        new_path = root / 'VOC' / path1 / path2\n        new_path.mkdir(parents=True, exist_ok=True)\n# Îç∞Ïù¥ÌÑ∞ÏÖã ÏïïÏ∂ï Ìï¥Ï†ú\n!tar -xvf VOCtrainval_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/ &gt; /dev/null 2&gt;&1\n!tar -xvf VOCtest_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/test/ &gt; /dev/null 2&gt;&1\n# XML ÌòïÏãùÏùÑ YOLO FormatÏúºÎ°ú Î≥ÄÍ≤ΩÌï¥Ï£ºÎäî ÍπÉÌóàÎ∏å ÌÅ¥Î°† \n!git clone https://github.com/ssaru/convert2Yolo.git\n\nfatal: destination path 'convert2Yolo' already exists and is not an empty directory.\n# ÌïÑÏöî ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n%cd convert2Yolo\n%pip install -qr requirements.txt\n\n[Errno 2] No such file or directory: 'convert2Yolo'\n/root/2024 winter/DL_tutorial/posts/yolov5\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\nNote: you may need to restart the kernel to use updated packages.\n!wget 'https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79'\n\n--2024-02-08 00:13:13--  https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.google.com (drive.google.com)... 142.250.206.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|142.250.206.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79 [following]\n--2024-02-08 00:13:14--  https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.76.129, 2404:6800:400a:80e::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.76.129|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 134 [application/octet-stream]\nSaving to: ‚Äòuc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79‚Äô\n\nuc?id=1WCodEuV9giZ9 100%[===================&gt;]     134  --.-KB/s    in 0s      \n\n2024-02-08 00:13:15 (13.2 MB/s) - ‚Äòuc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79‚Äô saved [134/134]\n# trainval Îç∞Ïù¥ÌÑ∞ yolo format Î≥ÄÌôò\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n# test Îç∞Ïù¥ÌÑ∞ yolo format Î≥ÄÌôò\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/test2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n\nVOC Parsing:   |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (5011/5011)  Complete\n\n\nYOLO Generating:|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (5011/5011)  Completeerating:|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-------------------------------| 22.7% (1136/5011)  Complete\n\n\nYOLO Saving:   |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (5011/5011)  Complete\n\n\nVOC Parsing:   |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (4952/4952)  Complete\n\n\nYOLO Generating:|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (4952/4952)  Complete\n\n\nYOLO Saving:   |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% (4952/4952)  Complete\n# Pasal VOC Ï†úÍ≥µ ÌååÏùºÎ°ú train, val ÎùºÎ≤® Î∂ÑÌï†\nimport shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\nwith open(path) as f:\n    image_ids = f.read().strip().split()\n    for id in image_ids:\n        ori_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007'\n        mv_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/val2007'\n        shutil.move(f\"{ori_path}/{id}.txt\", f\"{mv_path}/{id}.txt\")\n# train / val / test Ïù¥ÎØ∏ÏßÄ ÌååÏùº Î≥µÏÇ¨\nimport os, shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets'\nfor folder, subset in ('trainval', 'train2007'), ('trainval', 'val2007'), ('test', 'test2007'):\n    ex_imgs_path = f'{path}/{folder}/VOCdevkit/VOC2007/JPEGImages'\n    label_path = f'{path}/VOC/labels/{subset}'\n    img_path = f'{path}/VOC/images/{subset}'\n    print(subset,\": \", len(os.listdir(label_path)))\n    for lbs_list in os.listdir(label_path):\n        shutil.move(os.path.join(ex_imgs_path,lbs_list.split('.')[0]+'.jpg'), os.path.join(img_path,lbs_list.split('.')[0]+'.jpg'))\n    \n\ntrain2007 :  2501\nval2007 :  2510\ntest2007 :  4952"
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html#Ïª§Ïä§ÌÖÄ-Îç∞Ïù¥ÌÑ∞-Ï§ÄÎπÑ",
    "href": "posts/CV_ObjectDetection_0.html#Ïª§Ïä§ÌÖÄ-Îç∞Ïù¥ÌÑ∞-Ï§ÄÎπÑ",
    "title": "CV_ObjectDetection_0",
    "section": "Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ",
    "text": "Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n\n# ÎùºÎ≤®ÎßÅÌïú Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\n!wget 'https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ'\n\n--2024-02-08 09:16:46--  https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ [following]\n--2024-02-08 09:16:46--  https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.222.33, 2404:6800:4004:818::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.222.33|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 443876 (433K) [application/octet-stream]\nSaving to: ‚Äòuc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ‚Äô\n\nuc?id=1avRwo9y3M1Op 100%[===================&gt;] 433.47K  1.34MB/s    in 0.3s    \n\n2024-02-08 09:16:48 (1.34 MB/s) - ‚Äòuc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ‚Äô saved [443876/443876]\n\n\n\n\n# ÌååÏùº ÏïïÏ∂ï Ìï¥Ï†ú\nimport zipfile\nwith zipfile.ZipFile(\"04_detection_custom_datasets.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"04_detection_custom_datasets\")\n\n\n# Yolo 6.2 release version ÌÅ¥Î°†\n%cd /root/2024\\ winter/DL_tutorial/posts\n! git clone -b v6.2 https://github.com/ultralytics/yolov5.git\n%cd yolov5\n# ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ Îã§Ïö¥Î°úÎìú\n!pip install numpy==1.23.0\n%pip install -qr requirements.txt\nimport torch\nimport utils\ndisplay = utils.notebook_init()\n\nYOLOv5 üöÄ v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\n\nSetup complete ‚úÖ (128 CPUs, 503.7 GB RAM, 1001.9/1757.9 GB disk)\n\n\n\n# Weights & Biases ÏÖãÌåÖ\n%pip install -q wandb\nimport wandb\nwandb.login()\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nNote: you may need to restart the kernel to use updated packages.\n\n\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n# Ïû¨ÌòÑÏùÑ ÏúÑÌïú ÎûúÎç§ ÏãúÎìú Í≥†Ï†ï\nimport random\nimport numpy as np\nimport torch\n\nseed = 2024\ndeterministic = True\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nif deterministic:\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n%cd /root/2024winter/DL_tutorial/posts/yolov5\n# Pasal VOC Îç∞Ïù¥ÌÑ∞Î°ú YOLO v5 ÌõàÎ†®\n!python train.py --img 640 --batch 32 --epochs 30 --data custom_voc.yaml --weights '' --cfg yolov5s.yaml --seed 2024\n\n/root/2024winter/DL_tutorial/posts/yolov5\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\ntrain: weights=, cfg=yolov5s.yaml, data=custom_voc.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=2024, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\ngithub: skipping check (offline), for updates see https://github.com/ultralytics/yolov5\nrequirements: Pillow==7.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install Pillow==7.2.0' skipped (offline)\nrequirements: cycler==0.10.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install cycler==0.10.0' skipped (offline)\nrequirements: kiwisolver==1.0.1 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install kiwisolver==1.0.1' skipped (offline)\nrequirements: matplotlib==2.2.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install matplotlib==2.2.2' skipped (offline)\nrequirements: numpy==1.14.3 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install numpy==1.14.3' skipped (offline)\nrequirements: pyparsing==2.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pyparsing==2.2.0' skipped (offline)\nrequirements: python-dateutil==2.7.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install python-dateutil==2.7.2' skipped (offline)\nrequirements: pytz==2018.4 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pytz==2018.4' skipped (offline)\nrequirements: six==1.11.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install six==1.11.0' skipped (offline)\nYOLOv5 üöÄ v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nwandb: Tracking run with wandb version 0.16.3\nwandb: Run data is saved locally in /root/2024winter/DL_tutorial/posts/yolov5/wandb/run-20240209_073845-bf46v602\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run exalted-thunder-9\nwandb: ‚≠êÔ∏è View project at https://wandb.ai/dlwjdwo1109/YOLOv5\nwandb: üöÄ View run at https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\n\nDataset not found ‚ö†Ô∏è, missing paths ['/content/pascal_datasets/VOC/images/test2007']\nTraceback (most recent call last):\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 632, in &lt;module&gt;\n    main(opt)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 528, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 90, in train\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/__init__.py\", line 95, in __init__\n    self.wandb = WandbLogger(self.opt, run_id)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 187, in __init__\n    self.data_dict = check_wandb_dataset(opt.data)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 59, in check_wandb_dataset\n    return check_dataset(data_file)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/general.py\", line 505, in check_dataset\n    raise Exception('Dataset not found ‚ùå')\nException: Dataset not found ‚ùå\nwandb: üöÄ View run exalted-thunder-9 at: https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\nwandb: Ô∏è‚ö° View job at https://wandb.ai/dlwjdwo1109/YOLOv5/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODM4NjQzMg==/version_details/v0\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240209_073845-bf46v602/logs"
  },
  {
    "objectID": "posts/ANN_2.html",
    "href": "posts/ANN_2.html",
    "title": "ANN_2",
    "section": "",
    "text": "Step 1 : Load libraries & Datasts\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n\n# FashionMNIST Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\ntraining_data = datasets.FashionMNIST(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = ToTensor()\n)\n\n\ntest_data = datasets.FashionMNIST(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = ToTensor()\n)\n\n\n\nStep 2 : Create DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\n\ndevice = cuda\n\n\n\n\nEDA\n\nprint(training_data, '\\n--------------------------\\n', test_data)\n\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n--------------------------\n Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f'Feature batch shape: {train_features.size()}')\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\n\n\n\nlen(training_data)\n\n60000\n\n\n\nimg, label = training_data[0]\nplt.imshow(img.squeeze(), cmap='gray')\nprint(f'label={label}')\n\nlabel=9\n\n\n\n\n\n\n\n\n\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\n\nfigure = plt.figure(figsize = (20, 8))\ncols, rows = 5, 2\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    print(labels_map[label])\n    plt.axis('off')\n    plt.imshow(img.squeeze(), cmap='gray')\nplt.show()\n\nBag\nBag\nTrouser\nAnkle Boot\nSneaker\nCoat\nCoat\nBag\nAnkle Boot\nShirt\n\n\n\n\n\n\n\n\n\n\n\nStep 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.classifier = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 10)\n        )\n    def forward(self, x):\n        x = self.flatten(x)\n        output = self.classifier(x)\n        return output\n\n\n\nStep 4 : Create Model instacne\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\n\nModel test\n\nX = torch.rand(1, 28, 28, device=device)\noutput = model(X)\nprint(f'Î™®Îç∏ Ï∂úÎ†• Í≤∞Í≥º: {output}\\n')\npred_probab = nn.Softmax(dim=1)(output)\nprint(f'Softmax Í≤∞Í≥º: {pred_probab}\\n')\ny_pred = pred_probab.argmax()\nprint(y_pred)\n\nÎ™®Îç∏ Ï∂úÎ†• Í≤∞Í≥º: tensor([[ 0.2122, -0.0533,  0.4609, -0.1348, -0.2897,  0.0426,  0.2330, -0.0539,\n         -0.2371,  0.3135]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\nSoftmax Í≤∞Í≥º: tensor([[0.1144, 0.0878, 0.1468, 0.0809, 0.0693, 0.0966, 0.1169, 0.0877, 0.0730,\n         0.1267]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)\n\ntensor(2, device='cuda:0')\n\n\n\n\nStep 5 : Model compile\n\n# Loss\nloss = nn.CrossEntropyLoss()\n# Optimizer\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\nStep 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:7&gt;f} [{current:&gt;5d}]/{size:5d}')\n\n\n\nStep 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')\n\n\n\nStep 8 : Run model\n\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n--------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\nprint(\"Done\")\n\nEpoch 1 \n--------------------------\nloss: 0.431373 [    0]/60000\nloss: 0.587412 [ 6400]/60000\nloss: 0.583656 [12800]/60000\nloss: 0.526607 [19200]/60000\nloss: 0.425904 [25600]/60000\nloss: 0.559104 [32000]/60000\nloss: 0.255318 [38400]/60000\nloss: 0.401609 [44800]/60000\nloss: 0.359597 [51200]/60000\nloss: 0.415519 [57600]/60000\nTest Error: \n Accuracy: 84.0%, Avg loss: 0.441000\n\nEpoch 2 \n--------------------------\nloss: 0.396597 [    0]/60000\nloss: 0.282736 [ 6400]/60000\nloss: 0.348952 [12800]/60000\nloss: 0.394269 [19200]/60000\nloss: 0.309174 [25600]/60000\nloss: 0.406731 [32000]/60000\nloss: 0.255115 [38400]/60000\nloss: 0.464652 [44800]/60000\nloss: 0.266001 [51200]/60000\nloss: 0.316354 [57600]/60000\nTest Error: \n Accuracy: 86.0%, Avg loss: 0.396377\n\nEpoch 3 \n--------------------------\nloss: 0.343163 [    0]/60000\nloss: 0.417957 [ 6400]/60000\nloss: 0.361057 [12800]/60000\nloss: 0.290214 [19200]/60000\nloss: 0.439867 [25600]/60000\nloss: 0.573623 [32000]/60000\nloss: 0.358405 [38400]/60000\nloss: 0.280711 [44800]/60000\nloss: 0.403463 [51200]/60000\nloss: 0.364402 [57600]/60000\nTest Error: \n Accuracy: 85.9%, Avg loss: 0.389022\n\nEpoch 4 \n--------------------------\nloss: 0.269444 [    0]/60000\nloss: 0.317166 [ 6400]/60000\nloss: 0.371522 [12800]/60000\nloss: 0.389202 [19200]/60000\nloss: 0.290166 [25600]/60000\nloss: 0.557429 [32000]/60000\nloss: 0.463130 [38400]/60000\nloss: 0.320192 [44800]/60000\nloss: 0.230014 [51200]/60000\nloss: 0.221253 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.380155\n\nEpoch 5 \n--------------------------\nloss: 0.378960 [    0]/60000\nloss: 0.463637 [ 6400]/60000\nloss: 0.203711 [12800]/60000\nloss: 0.376729 [19200]/60000\nloss: 0.376216 [25600]/60000\nloss: 0.302987 [32000]/60000\nloss: 0.401179 [38400]/60000\nloss: 0.283290 [44800]/60000\nloss: 0.314023 [51200]/60000\nloss: 0.380436 [57600]/60000\nTest Error: \n Accuracy: 86.1%, Avg loss: 0.391101\n\nEpoch 6 \n--------------------------\nloss: 0.324956 [    0]/60000\nloss: 0.376954 [ 6400]/60000\nloss: 0.286267 [12800]/60000\nloss: 0.302243 [19200]/60000\nloss: 0.253790 [25600]/60000\nloss: 0.259367 [32000]/60000\nloss: 0.489078 [38400]/60000\nloss: 0.244414 [44800]/60000\nloss: 0.335011 [51200]/60000\nloss: 0.284465 [57600]/60000\nTest Error: \n Accuracy: 86.4%, Avg loss: 0.392578\n\nEpoch 7 \n--------------------------\nloss: 0.470740 [    0]/60000\nloss: 0.352301 [ 6400]/60000\nloss: 0.215639 [12800]/60000\nloss: 0.214226 [19200]/60000\nloss: 0.198628 [25600]/60000\nloss: 0.290468 [32000]/60000\nloss: 0.396786 [38400]/60000\nloss: 0.337734 [44800]/60000\nloss: 0.209183 [51200]/60000\nloss: 0.446850 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.375429\n\nEpoch 8 \n--------------------------\nloss: 0.325714 [    0]/60000\nloss: 0.252096 [ 6400]/60000\nloss: 0.266274 [12800]/60000\nloss: 0.404169 [19200]/60000\nloss: 0.398270 [25600]/60000\nloss: 0.240059 [32000]/60000\nloss: 0.345725 [38400]/60000\nloss: 0.339798 [44800]/60000\nloss: 0.232539 [51200]/60000\nloss: 0.371980 [57600]/60000\nTest Error: \n Accuracy: 87.3%, Avg loss: 0.363998\n\nEpoch 9 \n--------------------------\nloss: 0.275302 [    0]/60000\nloss: 0.459145 [ 6400]/60000\nloss: 0.477752 [12800]/60000\nloss: 0.368110 [19200]/60000\nloss: 0.166666 [25600]/60000\nloss: 0.340989 [32000]/60000\nloss: 0.412024 [38400]/60000\nloss: 0.255185 [44800]/60000\nloss: 0.370468 [51200]/60000\nloss: 0.383511 [57600]/60000\nTest Error: \n Accuracy: 87.2%, Avg loss: 0.362548\n\nEpoch 10 \n--------------------------\nloss: 0.163738 [    0]/60000\nloss: 0.234758 [ 6400]/60000\nloss: 0.498840 [12800]/60000\nloss: 0.502605 [19200]/60000\nloss: 0.460932 [25600]/60000\nloss: 0.459421 [32000]/60000\nloss: 0.243460 [38400]/60000\nloss: 0.231028 [44800]/60000\nloss: 0.239964 [51200]/60000\nloss: 0.273228 [57600]/60000\nTest Error: \n Accuracy: 87.0%, Avg loss: 0.363198\n\nDone\n\n\n\n\nStep 9 : Save & load model\n\nparameterÎßå Ï†ÄÏû•ÌïòÍ≥† Î∂àÎü¨Ïò§Í∏∞\n\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n\nmodel2 = NeuralNetwork().to(device)\nprint(model2)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\nmodel2.load_state_dict(torch.load('model_weights.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel2.eval()\ntest_loop(test_dataloader, model2, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797\n\n\n\n\n\n\nModel Ï†ÑÏ≤¥Î•º Ï†ÄÏû•ÌïòÍ≥† Î∂àÎü¨Ïò§Í∏∞\n\ntorch.save(model, 'model.pth')\n\n\nmodel3 = torch.load('model.pth')\n\n\nmodel3.eval()\ntest_loop(test_dataloader, model3, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797"
  },
  {
    "objectID": "posts/ANN_1.html",
    "href": "posts/ANN_1.html",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-1-import",
    "href": "posts/ANN_1.html#step-1-import",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-2-create-dataloader",
    "href": "posts/ANN_1.html#step-2-create-dataloader",
    "title": "ANN_1",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['label'] = iris.target\n\n# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\ny = df['label']\nX = df.drop(['label'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values,\n                                                   random_state=42, stratify=y)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.int64)\ny_test = torch.tensor(y_test, dtype=torch.int64)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
  },
  {
    "objectID": "posts/ANN_1.html#step-3-set-network-structure",
    "href": "posts/ANN_1.html#step-3-set-network-structure",
    "title": "ANN_1",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.input_layer = nn.Linear(4, 16)\n        self.hidden_layer1 = nn.Linear(16, 32)\n        self.output_layer = nn.Linear(32, 3)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.relu(self.input_layer(x))\n        out = self.relu(self.hidden_layer1(out))\n        out = self.output_layer(out)\n        return out"
  },
  {
    "objectID": "posts/ANN_1.html#step-4-create-model-instance",
    "href": "posts/ANN_1.html#step-4-create-model-instance",
    "title": "ANN_1",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\nmodel = NeuralNetwork().to(device)\n\ndevice = cuda"
  },
  {
    "objectID": "posts/ANN_1.html#step-5-model-compile",
    "href": "posts/ANN_1.html#step-5-model-compile",
    "title": "ANN_1",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\n# Î™®Îç∏ Ïª¥ÌååÏùº\nlearning_rate = 0.001\nloss = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
  },
  {
    "objectID": "posts/ANN_1.html#step-6-set-train-loop",
    "href": "posts/ANN_1.html#step-6-set-train-loop",
    "title": "ANN_1",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss, current = loss.item(), batch * len(X)\n        print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/ANN_1.html#step-7-set-test-loop",
    "href": "posts/ANN_1.html#step-7-set-test-loop",
    "title": "ANN_1",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error : \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')"
  },
  {
    "objectID": "posts/ANN_1.html#step-8-run-model",
    "href": "posts/ANN_1.html#step-8-run-model",
    "title": "ANN_1",
    "section": "Step 8 : Run model",
    "text": "Step 8 : Run model\n\n# Î™®Îç∏ Ïã§Ìñâ\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n------------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n------------------------------\nloss: 1.104175  [    0]/  112\nloss: 1.063668  [   10]/  112\nloss: 1.001417  [   20]/  112\nloss: 0.974181  [   30]/  112\nloss: 1.099196  [   40]/  112\nloss: 1.131157  [   50]/  112\nloss: 1.029493  [   60]/  112\nloss: 1.046669  [   70]/  112\nloss: 0.991781  [   80]/  112\nloss: 0.980483  [   90]/  112\nloss: 0.995154  [  100]/  112\nloss: 1.178336  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 1.013189\n\nEpoch 2 \n------------------------------\nloss: 0.998811  [    0]/  112\nloss: 1.072228  [   10]/  112\nloss: 0.861224  [   20]/  112\nloss: 1.145309  [   30]/  112\nloss: 0.923511  [   40]/  112\nloss: 1.029394  [   50]/  112\nloss: 1.038562  [   60]/  112\nloss: 0.923659  [   70]/  112\nloss: 1.022684  [   80]/  112\nloss: 0.942879  [   90]/  112\nloss: 1.011915  [  100]/  112\nloss: 0.944836  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 0.978484\n\nEpoch 3 \n------------------------------\nloss: 0.973278  [    0]/  112\nloss: 0.857192  [   10]/  112\nloss: 1.025384  [   20]/  112\nloss: 0.974613  [   30]/  112\nloss: 0.966374  [   40]/  112\nloss: 0.901531  [   50]/  112\nloss: 1.019788  [   60]/  112\nloss: 0.999571  [   70]/  112\nloss: 0.854874  [   80]/  112\nloss: 1.000906  [   90]/  112\nloss: 0.900930  [  100]/  112\nloss: 1.044610  [   22]/  112\nTest Error : \n Accuracy: 39.5%, Avg loss: 0.925748\n\nEpoch 4 \n------------------------------\nloss: 1.016448  [    0]/  112\nloss: 0.965601  [   10]/  112\nloss: 0.949101  [   20]/  112\nloss: 0.891714  [   30]/  112\nloss: 0.936245  [   40]/  112\nloss: 0.809627  [   50]/  112\nloss: 0.898013  [   60]/  112\nloss: 0.915918  [   70]/  112\nloss: 0.908495  [   80]/  112\nloss: 0.837218  [   90]/  112\nloss: 0.851940  [  100]/  112\nloss: 1.014478  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.887178\n\nEpoch 5 \n------------------------------\nloss: 0.929125  [    0]/  112\nloss: 0.860350  [   10]/  112\nloss: 0.894982  [   20]/  112\nloss: 0.848951  [   30]/  112\nloss: 0.874831  [   40]/  112\nloss: 0.867019  [   50]/  112\nloss: 0.865059  [   60]/  112\nloss: 0.850359  [   70]/  112\nloss: 0.778505  [   80]/  112\nloss: 0.894542  [   90]/  112\nloss: 0.853558  [  100]/  112\nloss: 0.779387  [   22]/  112\nTest Error : \n Accuracy: 68.4%, Avg loss: 0.844504\n\nEpoch 6 \n------------------------------\nloss: 0.901229  [    0]/  112\nloss: 0.800752  [   10]/  112\nloss: 0.798276  [   20]/  112\nloss: 0.743961  [   30]/  112\nloss: 0.844832  [   40]/  112\nloss: 0.806112  [   50]/  112\nloss: 0.788561  [   60]/  112\nloss: 0.879695  [   70]/  112\nloss: 0.851877  [   80]/  112\nloss: 0.764796  [   90]/  112\nloss: 0.764707  [  100]/  112\nloss: 0.804097  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.798580\n\nEpoch 7 \n------------------------------\nloss: 0.797247  [    0]/  112\nloss: 0.836456  [   10]/  112\nloss: 0.829977  [   20]/  112\nloss: 0.725152  [   30]/  112\nloss: 0.715440  [   40]/  112\nloss: 0.670871  [   50]/  112\nloss: 0.751514  [   60]/  112\nloss: 0.811848  [   70]/  112\nloss: 0.754924  [   80]/  112\nloss: 0.847931  [   90]/  112\nloss: 0.647542  [  100]/  112\nloss: 0.773911  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.743698\n\nEpoch 8 \n------------------------------\nloss: 0.745911  [    0]/  112\nloss: 0.756747  [   10]/  112\nloss: 0.697434  [   20]/  112\nloss: 0.770861  [   30]/  112\nloss: 0.675138  [   40]/  112\nloss: 0.756205  [   50]/  112\nloss: 0.699248  [   60]/  112\nloss: 0.632746  [   70]/  112\nloss: 0.680969  [   80]/  112\nloss: 0.731905  [   90]/  112\nloss: 0.697679  [  100]/  112\nloss: 0.612848  [   22]/  112\nTest Error : \n Accuracy: 73.7%, Avg loss: 0.692100\n\nEpoch 9 \n------------------------------\nloss: 0.709632  [    0]/  112\nloss: 0.619151  [   10]/  112\nloss: 0.675694  [   20]/  112\nloss: 0.595196  [   30]/  112\nloss: 0.690987  [   40]/  112\nloss: 0.679168  [   50]/  112\nloss: 0.648694  [   60]/  112\nloss: 0.642780  [   70]/  112\nloss: 0.661847  [   80]/  112\nloss: 0.611258  [   90]/  112\nloss: 0.658153  [  100]/  112\nloss: 0.781440  [   22]/  112\nTest Error : \n Accuracy: 71.1%, Avg loss: 0.642173\n\nEpoch 10 \n------------------------------\nloss: 0.675161  [    0]/  112\nloss: 0.570343  [   10]/  112\nloss: 0.511772  [   20]/  112\nloss: 0.692174  [   30]/  112\nloss: 0.523068  [   40]/  112\nloss: 0.557191  [   50]/  112\nloss: 0.715832  [   60]/  112\nloss: 0.556040  [   70]/  112\nloss: 0.620830  [   80]/  112\nloss: 0.581826  [   90]/  112\nloss: 0.604469  [  100]/  112\nloss: 0.707001  [   22]/  112\nTest Error : \n Accuracy: 81.6%, Avg loss: 0.595252\n\nDone!"
  },
  {
    "objectID": "posts/AlexNet.html",
    "href": "posts/AlexNet.html",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "href": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-2-data-preprocessing",
    "href": "posts/AlexNet.html#step-2-data-preprocessing",
    "title": "AlexNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\nÎ∂àÎü¨Ïò® Ïù¥ÎØ∏ÏßÄÏùò Ï¶ùÍ∞ïÏùÑ ÌÜµÌï¥ ÌïôÏäµ Ï†ïÌôïÎèÑÎ•º Ìñ•ÏÉÅÏãúÌÇ§ÎèÑÎ°ù Ìï©ÎãàÎã§.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\nmean = train_img.data.mean(axis=(0,1,2)) / 255\nstd = train_img.data.std(axis=(0,1,2)) / 255\nprint(f'ÌèâÍ∑†: {mean}, ÌëúÏ§ÄÌé∏Ï∞®:{std}')\n\nÌèâÍ∑†: [0.49139968 0.48215841 0.44653091], ÌëúÏ§ÄÌé∏Ï∞®:[0.24703223 0.24348513 0.26158784]\n\n\n\ntrain_img.data.shape , test_img.data.shape\n\n((50000, 32, 32, 3), (10000, 32, 32, 3))\n\n\n\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n    transforms.RandomCrop(size=train_img.data.shape[1], padding=4),\n    transforms.RandomHorizontalFlip(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\n\ntrain_img2  = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform_train,\n)\n\ntest_img2 = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform_test,\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified"
  },
  {
    "objectID": "posts/AlexNet.html#step-3-set-hyperparameters",
    "href": "posts/AlexNet.html#step-3-set-hyperparameters",
    "title": "AlexNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 128\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/AlexNet.html#step-4-create-dataloader",
    "href": "posts/AlexNet.html#step-4-create-dataloader",
    "title": "AlexNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\n# Create DataLoader\ntrain_loader = DataLoader(train_img2, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img2, batch_size = batch_sizes, shuffle = True)"
  },
  {
    "objectID": "posts/AlexNet.html#eda",
    "href": "posts/AlexNet.html#eda",
    "title": "AlexNet",
    "section": "EDA",
    "text": "EDA\n\nprint(train_img, '\\n-----------------------\\n', test_img)\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n-----------------------\n Dataset CIFAR10\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_loader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([128, 3, 32, 32])\nLabels batch shape: torch.Size([128])\n\n\n\nlabels_map = {\n    0: \"plane\",\n    1: \"car\",\n    2: \"bird\",\n    3: \"cat\",\n    4: \"deer\",\n    5: \"dog\",\n    6: \"frog\",\n    7: \"horse\",\n    8: \"ship\",\n    9: \"truck\",\n}\n\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 5, 5\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(train_img), size=(1,)).item()\n    img, label = train_img[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis('off')\n    plt.imshow(to_pil_image(img))\nplt.show()"
  },
  {
    "objectID": "posts/AlexNet.html#step-5-set-network-structure",
    "href": "posts/AlexNet.html#step-5-set-network-structure",
    "title": "AlexNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 4096),\n            nn.Dropout(0.5),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return(x)"
  },
  {
    "objectID": "posts/AlexNet.html#step-6-create-model-instance",
    "href": "posts/AlexNet.html#step-6-create-model-instance",
    "title": "AlexNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\n# Create Moedl instance\nmodel = AlexNet().to(device)\nprint(model)\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=256, out_features=4096, bias=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/AlexNet.html#step-7-model-compile",
    "href": "posts/AlexNet.html#step-7-model-compile",
    "title": "AlexNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "objectID": "posts/AlexNet.html#step-8-set-train-loop",
    "href": "posts/AlexNet.html#step-8-set-train-loop",
    "title": "AlexNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n        \n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/AlexNet.html#step-9-set-test-loop",
    "href": "posts/AlexNet.html#step-9-set-test-loop",
    "title": "AlexNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss:  {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/AlexNet.html#step-10-run-model",
    "href": "posts/AlexNet.html#step-10-run-model",
    "title": "AlexNet",
    "section": "Step 10 : Run Model",
    "text": "Step 10 : Run Model\n\nfor i in range(epochs):\n    print(f\"epochs {i+1} \\n-----------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\nprint('Done!')\n\nepochs 1 \n-----------------------------\nloss: 2.300106  [    0]/50000\nloss: 1.961412  [12800]/50000\nloss: 1.877241  [25600]/50000\nloss: 1.800212  [38400]/50000\nTest Error: \n Accuracy: 31.4%, Avg loss:  1.762373\n\nepochs 2 \n-----------------------------\nloss: 1.795186  [    0]/50000\nloss: 1.576234  [12800]/50000\nloss: 1.639605  [25600]/50000\nloss: 1.649888  [38400]/50000\nTest Error: \n Accuracy: 44.0%, Avg loss:  1.494407\n\nepochs 3 \n-----------------------------\nloss: 1.534194  [    0]/50000\nloss: 1.496532  [12800]/50000\nloss: 1.524187  [25600]/50000\nloss: 1.440067  [38400]/50000\nTest Error: \n Accuracy: 49.1%, Avg loss:  1.400005\n\nepochs 4 \n-----------------------------\nloss: 1.554884  [    0]/50000\nloss: 1.389567  [12800]/50000\nloss: 1.310677  [25600]/50000\nloss: 1.434475  [38400]/50000\nTest Error: \n Accuracy: 51.9%, Avg loss:  1.308864\n\nepochs 5 \n-----------------------------\nloss: 1.384561  [    0]/50000\nloss: 1.448307  [12800]/50000\nloss: 1.549824  [25600]/50000\nloss: 1.296622  [38400]/50000\nTest Error: \n Accuracy: 54.0%, Avg loss:  1.252349\n\nepochs 6 \n-----------------------------\nloss: 1.339483  [    0]/50000\nloss: 1.216769  [12800]/50000\nloss: 1.371759  [25600]/50000\nloss: 1.374979  [38400]/50000\nTest Error: \n Accuracy: 56.6%, Avg loss:  1.201752\n\nepochs 7 \n-----------------------------\nloss: 1.369484  [    0]/50000\nloss: 1.234365  [12800]/50000\nloss: 1.124013  [25600]/50000\nloss: 1.045216  [38400]/50000\nTest Error: \n Accuracy: 55.2%, Avg loss:  1.240022\n\nepochs 8 \n-----------------------------\nloss: 1.255546  [    0]/50000\nloss: 1.335250  [12800]/50000\nloss: 1.238086  [25600]/50000\nloss: 1.284560  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.171398\n\nepochs 9 \n-----------------------------\nloss: 1.246493  [    0]/50000\nloss: 1.291452  [12800]/50000\nloss: 1.238421  [25600]/50000\nloss: 1.272063  [38400]/50000\nTest Error: \n Accuracy: 59.5%, Avg loss:  1.131185\n\nepochs 10 \n-----------------------------\nloss: 1.220213  [    0]/50000\nloss: 1.313022  [12800]/50000\nloss: 1.362849  [25600]/50000\nloss: 1.224882  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.141315\n\nDone!\n\n\n/root/anaconda3/envs/py/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv2d(input, weight, bias, self.stride,"
  },
  {
    "objectID": "posts/AlexNet.html#step-11-confusion-matrix",
    "href": "posts/AlexNet.html#step-11-confusion-matrix",
    "title": "AlexNet",
    "section": "Step 11 : Confusion Matrix",
    "text": "Step 11 : Confusion Matrix\n\nimport itertools\ndef plot_confusion_matrix(cm, target_names=None, cmap=None, \n                          normalize=True, labels=True, title='Confusion matrix'):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    \n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names)\n        plt.yticks(tick_marks, target_names)\n    \n    if labels:\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n            else:\n                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f};\\\n                         misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n\nfrom sklearn.metrics import confusion_matrix\n\nmodel.eval()\nylabel = []\nypred_label = []\n\nfor batch_idx, (inputs, targets) in enumerate(test_loader):\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    _, predicted = outputs.max(1)\n    ylabel = np.concatenate((ylabel, targets.cpu().numpy()))\n    ypred_label = np.concatenate((ypred_label, predicted.cpu().numpy()))\n\ncnf_matrix = confusion_matrix(ylabel, ypred_label)\n\n\nplot_confusion_matrix(cnf_matrix, \n                      target_names=labels_map.values(), \n                      title='Confusion matrix, trained by AlexNet')"
  },
  {
    "objectID": "posts/CV_classification_0.html",
    "href": "posts/CV_classification_0.html",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#import-library",
    "href": "posts/CV_classification_0.html#import-library",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#dataset",
    "href": "posts/CV_classification_0.html#dataset",
    "title": "CV_classification_0",
    "section": "- Dataset",
    "text": "- Dataset\n\n# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        #transforms.RandomChoice([\n        #    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n        #    transforms.RandomResizedCrop(224),\n        #    transforms.RandomAffine(\n        #        degrees=15, translate=(0.2, 0.2),\n        #        scale = (0.8, 1.2), shear=15, resample=Image.BILINEAR)\n        #]),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\nval_transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\n\n# Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶àÏôÄ train:validation ÎπÑÏú® Ï†ïÏùò\nbatch_size = 256\nval_size = 0.2\n\n# torchvisionÏóêÏÑú Ï†úÍ≥µÌïòÎäî CIFAR10 ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download=True, transform=train_transform)\nval_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download = True, transform=val_transform)\n\n# train Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏùºÏ†ï ÎπÑÏú® validation data Î∂ÑÎ¶¨\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nsplit = int(np.floor(val_size * num_train))\ntrain_idx, val_idx = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n# Îç∞Ïù¥ÌÑ∞Î°úÎçî Ï†ïÏùò\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \nsampler=train_sampler, num_workers=2)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n\n\n\n                                          sampler=val_sampler, num_workers=2)\n\n# torchvisionÏóêÏÑú Ï†úÍ≥µÌïòÎäî CIFAR10 ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                           download=True, transform=val_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                           shuffle=False, num_workers=2)\n\n# ÌÅ¥ÎûòÏä§ Ï†ïÏùò\nclasses = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n# Îç∞Ïù¥ÌÑ∞ÏÖã ÌôïÏù∏\ntrain_dataset\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n           )\n\n\n\n# Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî\ndef imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# ÌïôÏäµ Ïù¥ÎØ∏ÏßÄ ÏñªÍ∏∞\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n# Ïù¥ÎØ∏ÏßÄ Ï∂úÎ†•\nimshow(torchvision.utils.make_grid(images))\n# ÎùºÎ≤® ÌîÑÎ¶∞Ìä∏\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\ncat   plane plane cat   frog  dog   deer  dog   car   bird  plane dog   ship  deer  frog  frog  horse cat   horse truck frog  horse car   plane ship  frog  horse frog  bird  horse plane truck frog  frog  deer  dog   plane car   plane truck car   dog   frog  deer  cat   frog  bird  ship  dog   ship  frog  plane plane ship  bird  car   car   cat   cat   horse truck ship  ship  plane car   truck plane dog   cat   deer  ship  dog   plane frog  horse cat   dog   bird  plane bird  dog   ship  ship  truck bird  horse horse plane truck bird  horse deer  deer  deer  horse horse dog   ship  ship  ship  cat   truck deer  dog   deer  dog   plane cat   cat   bird  car   car   horse deer  plane truck bird  plane ship  deer  ship  truck dog   horse ship  deer  cat   truck deer  dog   ship  frog  dog   car   car   plane deer  dog   plane horse dog   truck bird  dog   deer  bird  bird  bird  dog   car   dog   plane deer  horse plane truck car   plane bird  frog  plane frog  truck truck bird  bird  cat   dog   bird  dog   car   horse cat   car   frog  ship  truck frog  cat   horse plane ship  horse car   cat   cat   horse ship  truck dog   frog  plane horse plane deer  frog  ship  cat   plane cat   frog  car   bird  horse truck dog   plane horse plane frog  truck ship  cat   bird  frog  plane ship  bird  frog  car   frog  truck bird  ship  horse horse ship  bird  plane horse bird  cat   car   car   dog   frog  bird  deer  car   ship  frog  truck ship  cat   truck ship  ship  dog   ship  plane frog  dog   dog   car   plane car  \n\n\n\n# ÌÖåÏä§Ìä∏Î•º ÏúÑÌïú Custom Dataset Îã§Ïö¥Î°úÎìú\n!wget https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\n\n--2024-02-05 04:58:20--  https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc [following]\n--2024-02-05 04:58:21--  https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.207.97, 2404:6800:400a:805::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.207.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 77122 (75K) [application/octet-stream]\nSaving to: ‚Äòuc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1‚Äô\n\nuc?id=1GTES_wxB8b-j 100%[===================&gt;]  75.31K  --.-KB/s    in 0.1s    \n\n2024-02-05 04:58:22 (506 KB/s) - ‚Äòuc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1‚Äô saved [77122/77122]\n\n\n\n\n# ÌååÏùº ÏïïÏ∂ï Ìï¥Ï†ú\nwith zipfile.ZipFile(\"03_classification_custom_dataset.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"03_classification_custom_dataset\")\n\n\nfor folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n    print(folder)\n\nplane\ndog\n.DS_Store\ncat\nbird\ncar\n\n\n\n# Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§\nclass CUSTOMDataset(Dataset):\n    def __init__(self, mode: str = 'test', transforms: transforms = None):\n        self.mode = mode\n        self.transforms = transforms\n        self.images = []\n        self.labels = []\n\n        for folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n            files = os.path.join('03_classification_custom_dataset/custom_dataset',folder)\n            if folder == '.DS_Store':\n                continue\n            files_path = os.listdir(files)\n            for file in files_path:\n                self.images.append(os.path.join(files,file))\n                self.labels.append(classes.index(folder))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index: int) -&gt; Tuple[Tensor]:\n        image = Image.open(self.images[index]).convert('RGB')\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        image = np.array(image)\n        label = self.labels[index]\n        return image, label\n                \n            \n\n\n# Ïª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã & Î°úÎçî\ncustom_dataset = CUSTOMDataset('test', transforms = val_transform)\ncustom_loader = DataLoader(\n    custom_dataset, batch_size=16, shuffle=False, num_workers=2\n)"
  },
  {
    "objectID": "posts/CV_classification_0.html#Î™®Îç∏-Î∂àÎü¨Ïò§Í∏∞",
    "href": "posts/CV_classification_0.html#Î™®Îç∏-Î∂àÎü¨Ïò§Í∏∞",
    "title": "CV_classification_0",
    "section": "- Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞",
    "text": "- Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n\n# ÎîîÎ∞îÏù¥Ïä§ Ï≤¥Í∑∏ & Ìï†Îãπ\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ncuda\n\n\n\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\nmodel = model.to(device)\n\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0"
  },
  {
    "objectID": "posts/CV_classification_0.html#Î°úÏä§-Ìï®ÏàòÏôÄ-ÏòµÌã∞ÎßàÏù¥Ï†Ä-Ï†ïÏùò",
    "href": "posts/CV_classification_0.html#Î°úÏä§-Ìï®ÏàòÏôÄ-ÏòµÌã∞ÎßàÏù¥Ï†Ä-Ï†ïÏùò",
    "title": "CV_classification_0",
    "section": "- Î°úÏä§ Ìï®ÏàòÏôÄ ÏòµÌã∞ÎßàÏù¥Ï†Ä Ï†ïÏùò",
    "text": "- Î°úÏä§ Ìï®ÏàòÏôÄ ÏòµÌã∞ÎßàÏù¥Ï†Ä Ï†ïÏùò\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "posts/CV_classification_0.html#ÌïôÏäµ-Í≤ÄÏ¶ù-ÌÖåÏä§Ìä∏-Î©îÏÜåÎìú-Ï†ïÏùò",
    "href": "posts/CV_classification_0.html#ÌïôÏäµ-Í≤ÄÏ¶ù-ÌÖåÏä§Ìä∏-Î©îÏÜåÎìú-Ï†ïÏùò",
    "title": "CV_classification_0",
    "section": "- ÌïôÏäµ, Í≤ÄÏ¶ù, ÌÖåÏä§Ìä∏ Î©îÏÜåÎìú Ï†ïÏùò",
    "text": "- ÌïôÏäµ, Í≤ÄÏ¶ù, ÌÖåÏä§Ìä∏ Î©îÏÜåÎìú Ï†ïÏùò\n\ndef train(epoch):\n    train_loss = 0.0\n    model.train()\n    for i, data in enumerate(tqdm.tqdm(train_loader), 0):\n        # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞ data: [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # parameter gradientsÎ•º Ï†úÎ°úÌôî\n        optimizer.zero_grad()\n\n        # ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú Ï∂úÎ†• ÏÉùÏÑ±\n        outputs = model(inputs)\n\n        # ÏÜêÏã§Ìï®Ïàò Í≥ÑÏÇ∞ Î∞è ÏóÖÎç∞Ïù¥Ìä∏\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    return train_loss\n\n\ndef val():\n    val_loss = 0.0\n    val_accuracy = 0.0\n    with torch.no_grad():\n        # Î™®Îç∏ ÌèâÍ∞Ä Î™®Îìú ÏÑ§Ï†ï\n        model.eval()\n        for i, data in enumerate(tqdm.tqdm(val_loader), 0):\n            # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞ data: [inputs, labels]\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú Ï∂úÎ†• ÏÉùÏÑ±\n            outputs = model(inputs)\n\n            # ÏÜêÏã§Ìï®Ïàò Í≥ÑÏÇ∞\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # ÏòàÏ∏° ÎùºÎ≤®\n            _, predicted = torch.max(outputs, 1)\n\n            # accuracy Í≥ÑÏÇ∞\n            val_accuracy += (predicted == labels).sum().item()\n\n    return val_loss, val_accuracy\n\n\ndef test(test_loader):\n    correct = 0\n    total = 0\n    correct_class = {classname:0 for classname in classes}\n    total_class = {classname: 0 for classname in classes}\n    model.eval()\n    with torch.no_grad():\n        for data in test_loader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            # ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú Ï∂úÎ†• ÏÉùÏÑ±\n            outputs = model(inputs)\n\n            # ÏòàÏ∏° ÎùºÎ≤®\n            _, predicted = torch.max(outputs.data, 1)\n\n            # Ï†ÑÏ≤¥ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # ÌÅ¥ÎûòÏä§ Î≥Ñ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n            for label, prediction in zip(labels, predicted):\n                if label == prediction:\n                    correct_class[classes[label]] += 1\n                total_class[classes[label]] += 1\n\n    # Ï†ÑÏ≤¥ Ï†ïÌôïÎèÑ Ï∂úÎ†•\n    print(f'Accuracy of the network on the 10000 test images: {100*correct//total}%')\n    # ÌÅ¥ÎûòÏä§ Î≥Ñ Ï†ïÌôïÎèÑ Ï∂úÎ†•\n    for classname, correct_count in correct_class.items():\n        if total_class[classname] == 0:\n            continue\n        accuracy = 100*float(correct_count) / total_class[classname]\n        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f}%')"
  },
  {
    "objectID": "posts/CV_classification_0.html#ÌïôÏäµ",
    "href": "posts/CV_classification_0.html#ÌïôÏäµ",
    "title": "CV_classification_0",
    "section": "- ÌïôÏäµ",
    "text": "- ÌïôÏäµ\n\n# ÌïôÏäµ epoch ÏÑ§Ï†ï\ntrain_epochs = 20\nbest_acc = 0.0\n\n# Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú Ï†ïÏùò\nmodel_path = './cifar_resnet.pth'\nfor epoch in range(train_epochs):\n    # ÌïôÏäµ Î©îÏÜåÎìú Ïã§Ìñâ\n    train_loss = train(epoch)\n    print(f'[{epoch+1}] loss: {train_loss / len(train_loader):.3f}')\n    # Í≤ÄÏ¶ù Î©îÏÜåÎìú Ïã§Ìñâ\n    val_loss, val_acc = val()\n    valid_acc = val_acc / (len(val_loader)*batch_size)\n    print(f'[{epoch+1}] loss: {val_loss/len(val_loader):.3f} acc:{valid_acc:.3f}')\n    # Ï†ïÌôïÎèÑÍ∞Ä Í∏∞Ï°¥ Î≤†Ïä§Ìä∏Î•º Í∞±Ïã†Ìï† Í≤ΩÏö∞ Î™®Îç∏ Ï†ÄÏû•\n    if valid_acc &gt;= best_acc:\n        best_acc = valid_acc\n        torch.save(model.state_dict(), model_path)\n\nprint('Done!!')\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:18&lt;00:00,  8.40it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 12.78it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:16&lt;00:00,  9.41it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.88it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:15&lt;00:00, 10.06it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 13.33it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:15&lt;00:00, 10.16it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 12.92it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:15&lt;00:00, 10.40it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.92it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.23it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 13.26it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.37it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 16.28it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.61it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.96it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.89it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.95it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:14&lt;00:00, 10.85it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.73it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.81it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 14.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.24it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 14.87it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:13&lt;00:00, 11.55it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 14.24it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:14&lt;00:00, 11.16it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 13.24it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:16&lt;00:00,  9.61it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.80it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:17&lt;00:00,  8.87it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 13.50it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:15&lt;00:00, 10.11it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:02&lt;00:00, 15.18it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:15&lt;00:00, 10.27it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 13.19it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:16&lt;00:00,  9.69it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 11.73it/s]\n\n\n[1] loss: 0.581\n[1] loss: 0.673 acc:0.754\n[2] loss: 0.452\n[2] loss: 0.662 acc:0.760\n[3] loss: 0.353\n[3] loss: 0.648 acc:0.773\n[4] loss: 0.263\n[4] loss: 0.693 acc:0.773\n[5] loss: 0.204\n[5] loss: 0.745 acc:0.776\n[6] loss: 0.162\n[6] loss: 0.762 acc:0.769\n[7] loss: 0.131\n[7] loss: 0.781 acc:0.776\n[8] loss: 0.093\n[8] loss: 0.875 acc:0.777\n[9] loss: 0.080\n[9] loss: 0.899 acc:0.777\n[10] loss: 0.064\n[10] loss: 0.884 acc:0.781\n[11] loss: 0.053\n[11] loss: 0.950 acc:0.776\n[12] loss: 0.047\n[12] loss: 0.960 acc:0.781\n[13] loss: 0.039\n[13] loss: 0.964 acc:0.781\n[14] loss: 0.032\n[14] loss: 0.955 acc:0.785\n[15] loss: 0.026\n[15] loss: 1.009 acc:0.780\n[16] loss: 0.028\n[16] loss: 1.056 acc:0.782\n[17] loss: 0.028\n[17] loss: 0.996 acc:0.786\n[18] loss: 0.026\n[18] loss: 1.004 acc:0.788\n[19] loss: 0.023\n[19] loss: 1.029 acc:0.785\n[20] loss: 0.019\n[20] loss: 1.042 acc:0.785\nDone!!"
  },
  {
    "objectID": "posts/CV_classification_0.html#Î™®Îç∏-ÏÑ±Îä•-ÌèâÍ∞Ä",
    "href": "posts/CV_classification_0.html#Î™®Îç∏-ÏÑ±Îä•-ÌèâÍ∞Ä",
    "title": "CV_classification_0",
    "section": "- Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä",
    "text": "- Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä\n\nÏª§Ïä§ÌÖÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÖåÏä§Ìä∏\n\nmodel_path = 'cifar_resnet.pth'\n# Î™®Îç∏ Í∞ÄÏ§ëÏπò ÏóÖÎ°úÎìú\nmodel.load_state_dict(torch.load(model_path))\n# ÌÖåÏä§Ìä∏\ntest(custom_loader)\n\nAccuracy of the network on the 10000 test images: 68%\nAccuracy for class: plane is 100.0%\nAccuracy for class: car   is 60.0%\nAccuracy for class: bird  is 40.0%\nAccuracy for class: cat   is 80.0%\nAccuracy for class: dog   is 60.0%\n\n\n\n\nÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÖåÏä§Ìä∏\n\nmodel_path = 'cifar_resnet.pth'\n# Î™®Îç∏ Í∞ÄÏ§ëÏπò ÏóÖÎ°úÎìú\nmodel.load_state_dict(torch.load(model_path))\ntest(test_loader)\n\nAccuracy of the network on the 10000 test images: 80%\nAccuracy for class: plane is 85.0%\nAccuracy for class: car   is 88.0%\nAccuracy for class: bird  is 75.9%\nAccuracy for class: cat   is 62.5%\nAccuracy for class: deer  is 77.1%\nAccuracy for class: dog   is 71.7%\nAccuracy for class: frog  is 85.6%\nAccuracy for class: horse is 84.0%\nAccuracy for class: ship  is 84.6%\nAccuracy for class: truck is 85.7%"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DL_tutorial",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 9, 2024\n\n\nCV_ObjectDetection_0\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nFeb 5, 2024\n\n\nCV_classification_0\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nJan 31, 2024\n\n\nLMTM\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nJan 30, 2024\n\n\nVGGNet\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nJan 29, 2024\n\n\nAlexNet\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nJan 28, 2024\n\n\nANN_2\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\nJan 27, 2024\n\n\nANN_1\n\n\nÏù¥Ï†ïÏû¨ \n\n\n\n\n\nNo matching items"
  }
]