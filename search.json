[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LMTM.html",
    "href": "posts/LMTM.html",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\n영화 리뷰 텍스트(many)를 입력으로 받아 긍정 또는 부정(one)을 출력하는 구조\nEmbedding: 영화 리뷰(text)를 벡터로 변환하는 연산\nLSTM: 시계열 데이터를 처리하기 위한 구조\nLinear: 결과 출력"
  },
  {
    "objectID": "posts/LMTM.html#sentimental-analysis",
    "href": "posts/LMTM.html#sentimental-analysis",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\n영화 리뷰 텍스트(many)를 입력으로 받아 긍정 또는 부정(one)을 출력하는 구조\nEmbedding: 영화 리뷰(text)를 벡터로 변환하는 연산\nLSTM: 시계열 데이터를 처리하기 위한 구조\nLinear: 결과 출력"
  },
  {
    "objectID": "posts/LMTM.html#step-1-load-libraries-datsets",
    "href": "posts/LMTM.html#step-1-load-libraries-datsets",
    "title": "LMTM",
    "section": "Step 1 : Load libraries & Datsets",
    "text": "Step 1 : Load libraries & Datsets\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport os\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom collections import Counter\n\n\ndata = pd.read_csv('exercise4.csv')\ndata.head()\n\n\n\n\n\n\n\n\nprocessed\nlabel\n\n\n\n\n0\nOne reviewer mentioned watching Oz episode hoo...\n1\n\n\n1\nA wonderful little production . The filming te...\n1\n\n\n2\nI thought wonderful way spend time hot summer ...\n1\n\n\n3\nBasically family little boy Jake think zombie ...\n0\n\n\n4\nPetter Mattei Love Time Money visually stunnin...\n1\n\n\n\n\n\n\n\n\ndata['processed'][0]\n\n'One reviewer mentioned watching Oz episode hooked . They right , exactly happened . The first thing struck Oz brutality unflinching scene violence , set right word GO . Trust , show faint hearted timid . This show pull punch regard drug , sex violence . Its hardcore , classic use word . It called OZ nickname given Oswald Maximum Security State Penitentary . It focus mainly Emerald City , experimental section prison cell glass front face inwards , privacy high agenda . Em City home many . . Aryans , Muslims , gangsta , Latinos , Christians , Italians , Irish . . . . scuffle , death stare , dodgy dealing shady agreement never far away . I would say main appeal show due fact go show dare . Forget pretty picture painted mainstream audience , forget charm , forget romance . . . OZ mess around . The first episode I ever saw struck nasty surreal , I say I ready , I watched , I developed taste Oz , got accustomed high level graphic violence . Not violence , injustice crooked guard sold nickel , inmate kill order get away , well mannered , middle class inmate turned prison bitch due lack street skill prison experience Watching Oz , may become comfortable uncomfortable viewing . . . . thats get touch darker side .'\n\n\n\ndata['processed'] = data['processed'].str.lower().replace(r\"[^a-zA-Z ]\", \"\", regex=True)\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\n- 사전생성\n\n리뷰 문장에 들어있는 단어들을 추출하고, 각각의 단어에 숫자를 부여하는 작업\n[‘one’, ‘reviewer’, ‘mentioned’, ‘watching’, ‘oz’, ‘episode’, ‘hooked’]\n\n\n# 문장에 포함된 단어 토큰화\nreviews = data['processed'].values\nwords = ' '.join(reviews).split()\nwords[:10]\n\n['one',\n 'reviewer',\n 'mentioned',\n 'watching',\n 'oz',\n 'episode',\n 'hooked',\n 'they',\n 'right',\n 'exactly']\n\n\n\ncounter = Counter(words)\nvocab = sorted(counter, key=counter.get, reverse=True)\nint2word = dict(enumerate(vocab, 1))\nint2word[0] = '&lt;PAD&gt;'\nword2int = {word: id for id, word in int2word.items()}\n#word2int\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\n\n- 리뷰 인코딩\n\n리뷰에 포함된 단어를 숫자형태로 변환하는 작업\n{‘i’: 1, ‘movie’: 2, ‘film’: 3, ‘the’: 4, ‘one’: 5, ‘like’: 6, ‘it’: 7, ‘time’: 8, ‘this’: 9, ‘good’: 10, ‘character’: 11,…}\n\n\nreviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n\n100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:02&lt;00:00, 24930.36it/s]\n\n\n\nreviews_enc[0][1:10]\n\n[1095, 972, 74, 2893, 186, 2982, 119, 114, 538]\n\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\nword2int['one'], word2int['reviewer'], word2int['mentioned']\n\n(5, 1095, 972)\n\n\n\ndata['encoded'] = reviews_enc\ndata['encoded']\n\n0        [5, 1095, 972, 74, 2893, 186, 2982, 119, 114, ...\n1        [45, 311, 53, 247, 4, 1270, 1633, 16086, 78, 8...\n2        [1, 97, 311, 28, 1053, 8, 763, 1343, 2345, 112...\n3        [591, 130, 53, 221, 3123, 33, 565, 3653, 608, ...\n4        [57645, 9676, 39, 8, 203, 1993, 1312, 3, 37, 3...\n                               ...                        \n49995    [1, 97, 2, 114, 10, 191, 7, 1413, 128, 26, 913...\n49996    [22, 40, 22, 307, 22, 50, 2847, 869, 545, 1364...\n49997    [1, 3168, 4064, 34678, 7571, 269, 4234, 4064, ...\n49998    [1, 86, 2839, 825, 369, 348, 9292, 5, 9, 208, ...\n49999    [264, 5, 5445, 109, 1941, 2, 213, 328, 123, 44...\nName: encoded, Length: 50000, dtype: object\n\n\n\n\n- 길이 맞춰주기(padding or trim)\n\n신경망의 입력으로 사용하기 위해 일정 길이만큼 맞춰주는 작업\n길이가 긴 문장은 잘라주고(trim), 길이가 짧은 문장은 채워주는(padding) 작업\n\n\ndef pad_features(reviews, pad_id, seq_length=128):\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        features[i, :len(row)] = np.array(row)[:seq_length]\n\n    return features\n\nseq_length = 256\nfeatures = pad_features(reviews_enc, pad_id=word2int['&lt;PAD&gt;'], seq_length=seq_length)\n\nassert len(features) == len(reviews_enc)\nassert len(features[0]) == seq_length\n\n\nnp.full((5,3),2)\n\narray([[2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2]])\n\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\nlabels = data['label'].to_numpy()\nlabels\n\narray([1, 1, 1, ..., 0, 0, 0])\n\n\n\n\n- 데이터 분할\n\n# train test split\ntrain_size = .8\nsplit_id = int(len(features) * train_size)\ntrain_x, test_x, train_y, test_y = features[:split_id], features[split_id:], labels[:split_id], labels[split_id:]\n\nsplit_id = int(len(train_x) * train_size)\ntrain_x, valid_x, train_y, valid_y = train_x[:split_id], train_x[split_id:], train_y[:split_id], train_y[split_id:]\n\nprint('Train X shape: {}, Valid X shape: {}, Test X shape: {}'.format(train_x.shape, valid_x.shape, test_x.shape))\nprint('Train y shape: {}, Valid y shape: {}, Test y shape: {}'.format(train_y.shape, valid_y.shape, test_y.shape))\n\nTrain X shape: (32000, 256), Valid X shape: (8000, 256), Test X shape: (10000, 256)\nTrain y shape: (32000,), Valid y shape: (8000,), Test y shape: (10000,)"
  },
  {
    "objectID": "posts/LMTM.html#step-2-create-dataloader",
    "href": "posts/LMTM.html#step-2-create-dataloader",
    "title": "LMTM",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# set hyperparameter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nlr = 0.001\nbatch_size = 128\nvocab_size = len(word2int)\nembedding_size = 256\ndropout = 0.25\n\nepochs = 8\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'epochs': epochs\n}\n\nes_limit = 5\n\ncuda\n\n\n\ntrainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalidset = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntestset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\ntrainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\nvalloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\ntestloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
  },
  {
    "objectID": "posts/LMTM.html#step-3-set-network-structure",
    "href": "posts/LMTM.html#step-3-set-network-structure",
    "title": "LMTM",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_size=400):\n        super(LSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, 512, 2, dropout=0.25, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(512, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = x.long()\n        x = self.embedding(x)\n        o, _ = self.lstm(x)\n        o = o[:, -1, :]\n        o = self.dropout(o)\n        o = self.fc(o)\n        o = self.sigmoid(o)\n\n        return o"
  },
  {
    "objectID": "posts/LMTM.html#step-4-create-model-instance",
    "href": "posts/LMTM.html#step-4-create-model-instance",
    "title": "LMTM",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\nmodel = LSTMClassifier(vocab_size, embedding_size).to(device)\nprint(model)\n\nLSTMClassifier(\n  (embedding): Embedding(96140, 256)\n  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
  },
  {
    "objectID": "posts/LMTM.html#step-5-model-compile",
    "href": "posts/LMTM.html#step-5-model-compile",
    "title": "LMTM",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\ncriterion = nn.BCELoss()\noptim = Adam(model.parameters(), lr=lr)"
  },
  {
    "objectID": "posts/LMTM.html#step-6-set-train-loop",
    "href": "posts/LMTM.html#step-6-set-train-loop",
    "title": "LMTM",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train(model, trainloader):\n    model.train()\n\n    train_loss = 0\n    train_acc = 0\n\n    for id, (X, y) in enumerate(trainloader):\n        X, y = X.to(device), y.to(device)\n        optim.zero_grad()\n        y_pred = model(X)\n        loss = criterion(y_pred.squeeze(), y.float())\n        loss.backward()\n        optim.step()\n\n        train_loss += loss.item()\n        y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n        equals = y_pred == y\n        acc = torch.mean(equals.type(torch.FloatTensor))\n        train_acc += acc.item()\n\n    history['train_loss'].append(train_loss / len(trainloader))\n    history['train_acc'].append(train_acc / len(trainloader))\n\n    return train_loss, train_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-7-set-test-loop",
    "href": "posts/LMTM.html#step-7-set-test-loop",
    "title": "LMTM",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef validation(model, valloader):\n    model.eval()\n\n    val_loss = 0\n    val_acc = 0\n\n    with torch.no_grad():\n        for id, (X,y) in enumerate(valloader):\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss = criterion(y_pred.squeeze(), y.float())\n\n            val_loss += loss.item()\n\n            y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n            equals = y_pred == y\n            acc = torch.mean(equals.type(torch.FloatTensor))\n            val_acc += acc.item()\n\n        history['val_loss'].append(val_loss / len(valloader))\n        history['val_acc'].append(val_acc / len(valloader))\n\n    return val_loss, val_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-8-run-model",
    "href": "posts/LMTM.html#step-8-run-model",
    "title": "LMTM",
    "section": "Step 8 : Run Model",
    "text": "Step 8 : Run Model\n\n# train loop\nepochloop = tqdm(range(epochs), desc='Training')\n\n# early stop trigger\nes_trigger = 0\nval_loss_min = torch.inf\n\nfor e in epochloop:\n    train_loss, train_acc = train(model, trainloader)\n    val_loss, val_acc = validation(model, valloader)\n    epochloop.write(f'Epoch[{e+1}/{epochs}] Train Loss: {train_loss / len(trainloader):.3f}, Train Acc: {train_acc / len(trainloader):.3f}, Val Loss: {val_loss / len(valloader):.3f}, Val Acc: {val_acc / len(valloader):.3f}')\n\n    # save model if validation loss decrease\n    if val_loss / len(valloader) &lt;= val_loss_min:\n        torch.save(model.state_dict(), './sentiment_lstm.pt')\n        val_loss_min=val_loss / len(valloader)\n        es_trigger = 0\n\n    else:\n        es_trigger += 1\n\n    # early stop\n    if es_trigger &gt;= es_limit:\n        epochloop.write(f'Early stopped at Epoch-{e+1}')\n        history['epochs'] = e+1\n        break\n\nTraining:   0%|                                                                                   | 0/8 [00:12&lt;?, ?it/s]Training:  25%|██████████████████▊                                                        | 2/8 [00:23&lt;01:10, 11.75s/it]Training:  25%|██████████████████▊                                                        | 2/8 [00:32&lt;01:10, 11.75s/it]Training:  38%|████████████████████████████▏                                              | 3/8 [00:42&lt;00:53, 10.67s/it]Training:  50%|█████████████████████████████████████▌                                     | 4/8 [00:51&lt;00:40, 10.24s/it]Training:  62%|██████████████████████████████████████████████▉                            | 5/8 [01:01&lt;00:29,  9.82s/it]Training:  88%|█████████████████████████████████████████████████████████████████▋         | 7/8 [01:11&lt;00:09,  9.95s/it]Training: 100%|███████████████████████████████████████████████████████████████████████████| 8/8 [01:21&lt;00:00, 10.20s/it]\n\n\nEpoch[1/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.496\nEpoch[2/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.514\nEpoch[3/8] Train Loss: 0.669, Train Acc: 0.570, Val Loss: 0.650, Val Acc: 0.706\nEpoch[4/8] Train Loss: 0.478, Train Acc: 0.800, Val Loss: 0.601, Val Acc: 0.772\nEpoch[5/8] Train Loss: 0.448, Train Acc: 0.811, Val Loss: 0.405, Val Acc: 0.818\nEpoch[6/8] Train Loss: 0.314, Train Acc: 0.879, Val Loss: 0.357, Val Acc: 0.847\nEpoch[7/8] Train Loss: 0.237, Train Acc: 0.914, Val Loss: 0.384, Val Acc: 0.842\nEpoch[8/8] Train Loss: 0.203, Train Acc: 0.932, Val Loss: 0.426, Val Acc: 0.856\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_acc'][:history['epochs']], label='Train Acc')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Acc')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_loss'][:history['epochs']], label='Train Loss')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/CV_classification_0.html",
    "href": "posts/CV_classification_0.html",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#import-library",
    "href": "posts/CV_classification_0.html#import-library",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#dataset",
    "href": "posts/CV_classification_0.html#dataset",
    "title": "CV_classification_0",
    "section": "- Dataset",
    "text": "- Dataset\n\n# 데이터 전처리\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        #transforms.RandomChoice([\n        #    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n        #    transforms.RandomResizedCrop(224),\n        #    transforms.RandomAffine(\n        #        degrees=15, translate=(0.2, 0.2),\n        #        scale = (0.8, 1.2), shear=15, resample=Image.BILINEAR)\n        #]),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\nval_transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\n\n# 배치 사이즈와 train:validation 비율 정의\nbatch_size = 256\nval_size = 0.2\n\n# torchvision에서 제공하는 CIFAR10 학습 데이터셋 다운로드\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download=True, transform=train_transform)\nval_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download = True, transform=val_transform)\n\n# train 데이터에서 일정 비율 validation data 분리\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nsplit = int(np.floor(val_size * num_train))\ntrain_idx, val_idx = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n# 데이터로더 정의\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \nsampler=train_sampler, num_workers=2)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n\n\n\n                                          sampler=val_sampler, num_workers=2)\n\n# torchvision에서 제공하는 CIFAR10 테스트 데이터셋 다운로드\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                           download=True, transform=val_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                           shuffle=False, num_workers=2)\n\n# 클래스 정의\nclasses = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n# 데이터셋 확인\ntrain_dataset\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n           )\n\n\n\n# 이미지 데이터 시각화\ndef imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# 학습 이미지 얻기\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n# 이미지 출력\nimshow(torchvision.utils.make_grid(images))\n# 라벨 프린트\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\ncat   plane plane cat   frog  dog   deer  dog   car   bird  plane dog   ship  deer  frog  frog  horse cat   horse truck frog  horse car   plane ship  frog  horse frog  bird  horse plane truck frog  frog  deer  dog   plane car   plane truck car   dog   frog  deer  cat   frog  bird  ship  dog   ship  frog  plane plane ship  bird  car   car   cat   cat   horse truck ship  ship  plane car   truck plane dog   cat   deer  ship  dog   plane frog  horse cat   dog   bird  plane bird  dog   ship  ship  truck bird  horse horse plane truck bird  horse deer  deer  deer  horse horse dog   ship  ship  ship  cat   truck deer  dog   deer  dog   plane cat   cat   bird  car   car   horse deer  plane truck bird  plane ship  deer  ship  truck dog   horse ship  deer  cat   truck deer  dog   ship  frog  dog   car   car   plane deer  dog   plane horse dog   truck bird  dog   deer  bird  bird  bird  dog   car   dog   plane deer  horse plane truck car   plane bird  frog  plane frog  truck truck bird  bird  cat   dog   bird  dog   car   horse cat   car   frog  ship  truck frog  cat   horse plane ship  horse car   cat   cat   horse ship  truck dog   frog  plane horse plane deer  frog  ship  cat   plane cat   frog  car   bird  horse truck dog   plane horse plane frog  truck ship  cat   bird  frog  plane ship  bird  frog  car   frog  truck bird  ship  horse horse ship  bird  plane horse bird  cat   car   car   dog   frog  bird  deer  car   ship  frog  truck ship  cat   truck ship  ship  dog   ship  plane frog  dog   dog   car   plane car  \n\n\n\n# 테스트를 위한 Custom Dataset 다운로드\n!wget https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\n\n--2024-02-05 04:58:20--  https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc [following]\n--2024-02-05 04:58:21--  https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.207.97, 2404:6800:400a:805::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.207.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 77122 (75K) [application/octet-stream]\nSaving to: ‘uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1’\n\nuc?id=1GTES_wxB8b-j 100%[===================&gt;]  75.31K  --.-KB/s    in 0.1s    \n\n2024-02-05 04:58:22 (506 KB/s) - ‘uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1’ saved [77122/77122]\n\n\n\n\n# 파일 압축 해제\nwith zipfile.ZipFile(\"03_classification_custom_dataset.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"03_classification_custom_dataset\")\n\n\nfor folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n    print(folder)\n\nplane\ndog\n.DS_Store\ncat\nbird\ncar\n\n\n\n# 커스텀 데이터셋 클래스\nclass CUSTOMDataset(Dataset):\n    def __init__(self, mode: str = 'test', transforms: transforms = None):\n        self.mode = mode\n        self.transforms = transforms\n        self.images = []\n        self.labels = []\n\n        for folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n            files = os.path.join('03_classification_custom_dataset/custom_dataset',folder)\n            if folder == '.DS_Store':\n                continue\n            files_path = os.listdir(files)\n            for file in files_path:\n                self.images.append(os.path.join(files,file))\n                self.labels.append(classes.index(folder))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index: int) -&gt; Tuple[Tensor]:\n        image = Image.open(self.images[index]).convert('RGB')\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        image = np.array(image)\n        label = self.labels[index]\n        return image, label\n                \n            \n\n\n# 커스텀 데이터셋 & 로더\ncustom_dataset = CUSTOMDataset('test', transforms = val_transform)\ncustom_loader = DataLoader(\n    custom_dataset, batch_size=16, shuffle=False, num_workers=2\n)"
  },
  {
    "objectID": "posts/CV_classification_0.html#모델-불러오기",
    "href": "posts/CV_classification_0.html#모델-불러오기",
    "title": "CV_classification_0",
    "section": "- 모델 불러오기",
    "text": "- 모델 불러오기\n\n# 디바이스 체그 & 할당\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ncuda\n\n\n\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\nmodel = model.to(device)\n\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0"
  },
  {
    "objectID": "posts/CV_classification_0.html#로스-함수와-옵티마이저-정의",
    "href": "posts/CV_classification_0.html#로스-함수와-옵티마이저-정의",
    "title": "CV_classification_0",
    "section": "- 로스 함수와 옵티마이저 정의",
    "text": "- 로스 함수와 옵티마이저 정의\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "posts/CV_classification_0.html#학습-검증-테스트-메소드-정의",
    "href": "posts/CV_classification_0.html#학습-검증-테스트-메소드-정의",
    "title": "CV_classification_0",
    "section": "- 학습, 검증, 테스트 메소드 정의",
    "text": "- 학습, 검증, 테스트 메소드 정의\n\ndef train(epoch):\n    train_loss = 0.0\n    model.train()\n    for i, data in enumerate(tqdm.tqdm(train_loader), 0):\n        # 입력 데이터 가져오기 data: [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # parameter gradients를 제로화\n        optimizer.zero_grad()\n\n        # 입력 이미지에 대한 출력 생성\n        outputs = model(inputs)\n\n        # 손실함수 계산 및 업데이트\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    return train_loss\n\n\ndef val():\n    val_loss = 0.0\n    val_accuracy = 0.0\n    with torch.no_grad():\n        # 모델 평가 모드 설정\n        model.eval()\n        for i, data in enumerate(tqdm.tqdm(val_loader), 0):\n            # 입력 데이터 가져오기 data: [inputs, labels]\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # 입력 이미지에 대한 출력 생성\n            outputs = model(inputs)\n\n            # 손실함수 계산\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # 예측 라벨\n            _, predicted = torch.max(outputs, 1)\n\n            # accuracy 계산\n            val_accuracy += (predicted == labels).sum().item()\n\n    return val_loss, val_accuracy\n\n\ndef test(test_loader):\n    correct = 0\n    total = 0\n    correct_class = {classname:0 for classname in classes}\n    total_class = {classname: 0 for classname in classes}\n    model.eval()\n    with torch.no_grad():\n        for data in test_loader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            # 입력 이미지에 대한 출력 생성\n            outputs = model(inputs)\n\n            # 예측 라벨\n            _, predicted = torch.max(outputs.data, 1)\n\n            # 전체 정확도 계산\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # 클래스 별 정확도 계산\n            for label, prediction in zip(labels, predicted):\n                if label == prediction:\n                    correct_class[classes[label]] += 1\n                total_class[classes[label]] += 1\n\n    # 전체 정확도 출력\n    print(f'Accuracy of the network on the 10000 test images: {100*correct//total}%')\n    # 클래스 별 정확도 출력\n    for classname, correct_count in correct_class.items():\n        if total_class[classname] == 0:\n            continue\n        accuracy = 100*float(correct_count) / total_class[classname]\n        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f}%')"
  },
  {
    "objectID": "posts/CV_classification_0.html#학습",
    "href": "posts/CV_classification_0.html#학습",
    "title": "CV_classification_0",
    "section": "- 학습",
    "text": "- 학습\n\n# 학습 epoch 설정\ntrain_epochs = 20\nbest_acc = 0.0\n\n# 모델 저장 경로 정의\nmodel_path = './cifar_resnet.pth'\nfor epoch in range(train_epochs):\n    # 학습 메소드 실행\n    train_loss = train(epoch)\n    print(f'[{epoch+1}] loss: {train_loss / len(train_loader):.3f}')\n    # 검증 메소드 실행\n    val_loss, val_acc = val()\n    valid_acc = val_acc / (len(val_loader)*batch_size)\n    print(f'[{epoch+1}] loss: {val_loss/len(val_loader):.3f} acc:{valid_acc:.3f}')\n    # 정확도가 기존 베스트를 갱신할 경우 모델 저장\n    if valid_acc &gt;= best_acc:\n        best_acc = valid_acc\n        torch.save(model.state_dict(), model_path)\n\nprint('Done!!')\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:18&lt;00:00,  8.40it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 12.78it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.41it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.88it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.06it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.33it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.16it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 12.92it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.40it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.92it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.23it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.26it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.37it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 16.28it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.61it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.54it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.49it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.96it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.89it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.95it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:14&lt;00:00, 10.85it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.73it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.81it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.84it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.24it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.87it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.55it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.24it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:14&lt;00:00, 11.16it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.24it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.61it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.80it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:17&lt;00:00,  8.87it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.50it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.11it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 15.18it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.27it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.19it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.69it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 11.73it/s]\n\n\n[1] loss: 0.581\n[1] loss: 0.673 acc:0.754\n[2] loss: 0.452\n[2] loss: 0.662 acc:0.760\n[3] loss: 0.353\n[3] loss: 0.648 acc:0.773\n[4] loss: 0.263\n[4] loss: 0.693 acc:0.773\n[5] loss: 0.204\n[5] loss: 0.745 acc:0.776\n[6] loss: 0.162\n[6] loss: 0.762 acc:0.769\n[7] loss: 0.131\n[7] loss: 0.781 acc:0.776\n[8] loss: 0.093\n[8] loss: 0.875 acc:0.777\n[9] loss: 0.080\n[9] loss: 0.899 acc:0.777\n[10] loss: 0.064\n[10] loss: 0.884 acc:0.781\n[11] loss: 0.053\n[11] loss: 0.950 acc:0.776\n[12] loss: 0.047\n[12] loss: 0.960 acc:0.781\n[13] loss: 0.039\n[13] loss: 0.964 acc:0.781\n[14] loss: 0.032\n[14] loss: 0.955 acc:0.785\n[15] loss: 0.026\n[15] loss: 1.009 acc:0.780\n[16] loss: 0.028\n[16] loss: 1.056 acc:0.782\n[17] loss: 0.028\n[17] loss: 0.996 acc:0.786\n[18] loss: 0.026\n[18] loss: 1.004 acc:0.788\n[19] loss: 0.023\n[19] loss: 1.029 acc:0.785\n[20] loss: 0.019\n[20] loss: 1.042 acc:0.785\nDone!!"
  },
  {
    "objectID": "posts/CV_classification_0.html#모델-성능-평가",
    "href": "posts/CV_classification_0.html#모델-성능-평가",
    "title": "CV_classification_0",
    "section": "- 모델 성능 평가",
    "text": "- 모델 성능 평가\n\n커스텀 데이터셋 테스트\n\nmodel_path = 'cifar_resnet.pth'\n# 모델 가중치 업로드\nmodel.load_state_dict(torch.load(model_path))\n# 테스트\ntest(custom_loader)\n\nAccuracy of the network on the 10000 test images: 68%\nAccuracy for class: plane is 100.0%\nAccuracy for class: car   is 60.0%\nAccuracy for class: bird  is 40.0%\nAccuracy for class: cat   is 80.0%\nAccuracy for class: dog   is 60.0%\n\n\n\n\n테스트 데이터셋 테스트\n\nmodel_path = 'cifar_resnet.pth'\n# 모델 가중치 업로드\nmodel.load_state_dict(torch.load(model_path))\ntest(test_loader)\n\nAccuracy of the network on the 10000 test images: 80%\nAccuracy for class: plane is 85.0%\nAccuracy for class: car   is 88.0%\nAccuracy for class: bird  is 75.9%\nAccuracy for class: cat   is 62.5%\nAccuracy for class: deer  is 77.1%\nAccuracy for class: dog   is 71.7%\nAccuracy for class: frog  is 85.6%\nAccuracy for class: horse is 84.0%\nAccuracy for class: ship  is 84.6%\nAccuracy for class: truck is 85.7%"
  },
  {
    "objectID": "posts/AlexNet.html",
    "href": "posts/AlexNet.html",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "href": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-2-data-preprocessing",
    "href": "posts/AlexNet.html#step-2-data-preprocessing",
    "title": "AlexNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\n불러온 이미지의 증강을 통해 학습 정확도를 향상시키도록 합니다.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\nmean = train_img.data.mean(axis=(0,1,2)) / 255\nstd = train_img.data.std(axis=(0,1,2)) / 255\nprint(f'평균: {mean}, 표준편차:{std}')\n\n평균: [0.49139968 0.48215841 0.44653091], 표준편차:[0.24703223 0.24348513 0.26158784]\n\n\n\ntrain_img.data.shape , test_img.data.shape\n\n((50000, 32, 32, 3), (10000, 32, 32, 3))\n\n\n\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n    transforms.RandomCrop(size=train_img.data.shape[1], padding=4),\n    transforms.RandomHorizontalFlip(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\n\ntrain_img2  = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform_train,\n)\n\ntest_img2 = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform_test,\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified"
  },
  {
    "objectID": "posts/AlexNet.html#step-3-set-hyperparameters",
    "href": "posts/AlexNet.html#step-3-set-hyperparameters",
    "title": "AlexNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 128\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/AlexNet.html#step-4-create-dataloader",
    "href": "posts/AlexNet.html#step-4-create-dataloader",
    "title": "AlexNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\n# Create DataLoader\ntrain_loader = DataLoader(train_img2, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img2, batch_size = batch_sizes, shuffle = True)"
  },
  {
    "objectID": "posts/AlexNet.html#eda",
    "href": "posts/AlexNet.html#eda",
    "title": "AlexNet",
    "section": "EDA",
    "text": "EDA\n\nprint(train_img, '\\n-----------------------\\n', test_img)\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n-----------------------\n Dataset CIFAR10\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_loader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([128, 3, 32, 32])\nLabels batch shape: torch.Size([128])\n\n\n\nlabels_map = {\n    0: \"plane\",\n    1: \"car\",\n    2: \"bird\",\n    3: \"cat\",\n    4: \"deer\",\n    5: \"dog\",\n    6: \"frog\",\n    7: \"horse\",\n    8: \"ship\",\n    9: \"truck\",\n}\n\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 5, 5\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(train_img), size=(1,)).item()\n    img, label = train_img[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis('off')\n    plt.imshow(to_pil_image(img))\nplt.show()"
  },
  {
    "objectID": "posts/AlexNet.html#step-5-set-network-structure",
    "href": "posts/AlexNet.html#step-5-set-network-structure",
    "title": "AlexNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 4096),\n            nn.Dropout(0.5),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return(x)"
  },
  {
    "objectID": "posts/AlexNet.html#step-6-create-model-instance",
    "href": "posts/AlexNet.html#step-6-create-model-instance",
    "title": "AlexNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\n# Create Moedl instance\nmodel = AlexNet().to(device)\nprint(model)\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=256, out_features=4096, bias=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/AlexNet.html#step-7-model-compile",
    "href": "posts/AlexNet.html#step-7-model-compile",
    "title": "AlexNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "objectID": "posts/AlexNet.html#step-8-set-train-loop",
    "href": "posts/AlexNet.html#step-8-set-train-loop",
    "title": "AlexNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n        \n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/AlexNet.html#step-9-set-test-loop",
    "href": "posts/AlexNet.html#step-9-set-test-loop",
    "title": "AlexNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss:  {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/AlexNet.html#step-10-run-model",
    "href": "posts/AlexNet.html#step-10-run-model",
    "title": "AlexNet",
    "section": "Step 10 : Run Model",
    "text": "Step 10 : Run Model\n\nfor i in range(epochs):\n    print(f\"epochs {i+1} \\n-----------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\nprint('Done!')\n\nepochs 1 \n-----------------------------\nloss: 2.300106  [    0]/50000\nloss: 1.961412  [12800]/50000\nloss: 1.877241  [25600]/50000\nloss: 1.800212  [38400]/50000\nTest Error: \n Accuracy: 31.4%, Avg loss:  1.762373\n\nepochs 2 \n-----------------------------\nloss: 1.795186  [    0]/50000\nloss: 1.576234  [12800]/50000\nloss: 1.639605  [25600]/50000\nloss: 1.649888  [38400]/50000\nTest Error: \n Accuracy: 44.0%, Avg loss:  1.494407\n\nepochs 3 \n-----------------------------\nloss: 1.534194  [    0]/50000\nloss: 1.496532  [12800]/50000\nloss: 1.524187  [25600]/50000\nloss: 1.440067  [38400]/50000\nTest Error: \n Accuracy: 49.1%, Avg loss:  1.400005\n\nepochs 4 \n-----------------------------\nloss: 1.554884  [    0]/50000\nloss: 1.389567  [12800]/50000\nloss: 1.310677  [25600]/50000\nloss: 1.434475  [38400]/50000\nTest Error: \n Accuracy: 51.9%, Avg loss:  1.308864\n\nepochs 5 \n-----------------------------\nloss: 1.384561  [    0]/50000\nloss: 1.448307  [12800]/50000\nloss: 1.549824  [25600]/50000\nloss: 1.296622  [38400]/50000\nTest Error: \n Accuracy: 54.0%, Avg loss:  1.252349\n\nepochs 6 \n-----------------------------\nloss: 1.339483  [    0]/50000\nloss: 1.216769  [12800]/50000\nloss: 1.371759  [25600]/50000\nloss: 1.374979  [38400]/50000\nTest Error: \n Accuracy: 56.6%, Avg loss:  1.201752\n\nepochs 7 \n-----------------------------\nloss: 1.369484  [    0]/50000\nloss: 1.234365  [12800]/50000\nloss: 1.124013  [25600]/50000\nloss: 1.045216  [38400]/50000\nTest Error: \n Accuracy: 55.2%, Avg loss:  1.240022\n\nepochs 8 \n-----------------------------\nloss: 1.255546  [    0]/50000\nloss: 1.335250  [12800]/50000\nloss: 1.238086  [25600]/50000\nloss: 1.284560  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.171398\n\nepochs 9 \n-----------------------------\nloss: 1.246493  [    0]/50000\nloss: 1.291452  [12800]/50000\nloss: 1.238421  [25600]/50000\nloss: 1.272063  [38400]/50000\nTest Error: \n Accuracy: 59.5%, Avg loss:  1.131185\n\nepochs 10 \n-----------------------------\nloss: 1.220213  [    0]/50000\nloss: 1.313022  [12800]/50000\nloss: 1.362849  [25600]/50000\nloss: 1.224882  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.141315\n\nDone!\n\n\n/root/anaconda3/envs/py/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv2d(input, weight, bias, self.stride,"
  },
  {
    "objectID": "posts/AlexNet.html#step-11-confusion-matrix",
    "href": "posts/AlexNet.html#step-11-confusion-matrix",
    "title": "AlexNet",
    "section": "Step 11 : Confusion Matrix",
    "text": "Step 11 : Confusion Matrix\n\nimport itertools\ndef plot_confusion_matrix(cm, target_names=None, cmap=None, \n                          normalize=True, labels=True, title='Confusion matrix'):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    \n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names)\n        plt.yticks(tick_marks, target_names)\n    \n    if labels:\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n            else:\n                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f};\\\n                         misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n\nfrom sklearn.metrics import confusion_matrix\n\nmodel.eval()\nylabel = []\nypred_label = []\n\nfor batch_idx, (inputs, targets) in enumerate(test_loader):\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    _, predicted = outputs.max(1)\n    ylabel = np.concatenate((ylabel, targets.cpu().numpy()))\n    ypred_label = np.concatenate((ypred_label, predicted.cpu().numpy()))\n\ncnf_matrix = confusion_matrix(ylabel, ypred_label)\n\n\nplot_confusion_matrix(cnf_matrix, \n                      target_names=labels_map.values(), \n                      title='Confusion matrix, trained by AlexNet')"
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html",
    "href": "posts/CV_ObjectDetection_0.html",
    "title": "CV_ObjectDetection_0",
    "section": "",
    "text": "# Pascal VOC 2007 데이터 다운로드\n!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n\n--2024-02-09 07:27:15--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n--2024-02-09 07:27:16--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 460032000 (439M) [application/x-tar]\nSaving to: ‘VOCtrainval_06-Nov-2007.tar.3’\n\nVOCtrainval_06-Nov- 100%[===================&gt;] 438.72M  15.0MB/s    in 30s     \n\n2024-02-09 07:27:47 (14.5 MB/s) - ‘VOCtrainval_06-Nov-2007.tar.3’ saved [460032000/460032000]\n\n--2024-02-09 07:27:47--  http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar [following]\n--2024-02-09 07:27:47--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 451020800 (430M) [application/x-tar]\nSaving to: ‘VOCtest_06-Nov-2007.tar.3’\n\nVOCtest_06-Nov-2007 100%[===================&gt;] 430.13M  16.1MB/s    in 26s     \n\n2024-02-09 07:29:06 (16.3 MB/s) - ‘VOCtest_06-Nov-2007.tar.3’ saved [451020800/451020800]\nfrom pathlib import Path\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval').mkdir(parents=True, exist_ok=True)\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/test').mkdir(parents=True, exist_ok=True)\nroot=Path('/root/2024winter/DL_tutorial/posts/pascal_datasets')\n\nfor path1 in ('images', 'labels'):\n    for path2 in ('train2007', 'val2007', 'test2007'):\n        new_path = root / 'VOC' / path1 / path2\n        new_path.mkdir(parents=True, exist_ok=True)\n# 데이터셋 압축 해제\n!tar -xvf VOCtrainval_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/ &gt; /dev/null 2&gt;&1\n!tar -xvf VOCtest_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/test/ &gt; /dev/null 2&gt;&1\n# XML 형식을 YOLO Format으로 변경해주는 깃허브 클론 \n!git clone https://github.com/ssaru/convert2Yolo.git\n\nfatal: destination path 'convert2Yolo' already exists and is not an empty directory.\n# 필요 라이브러리 설치\n%cd convert2Yolo\n%pip install -qr requirements.txt\n\n[Errno 2] No such file or directory: 'convert2Yolo'\n/root/2024 winter/DL_tutorial/posts/yolov5\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\nNote: you may need to restart the kernel to use updated packages.\n!wget 'https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79'\n\n--2024-02-08 00:13:13--  https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.google.com (drive.google.com)... 142.250.206.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|142.250.206.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79 [following]\n--2024-02-08 00:13:14--  https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.76.129, 2404:6800:400a:80e::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.76.129|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 134 [application/octet-stream]\nSaving to: ‘uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79’\n\nuc?id=1WCodEuV9giZ9 100%[===================&gt;]     134  --.-KB/s    in 0s      \n\n2024-02-08 00:13:15 (13.2 MB/s) - ‘uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79’ saved [134/134]\n# trainval 데이터 yolo format 변환\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n# test 데이터 yolo format 변환\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/test2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n\nVOC Parsing:   |████████████████████████████████████████| 100.0% (5011/5011)  Complete\n\n\nYOLO Generating:|████████████████████████████████████████| 100.0% (5011/5011)  Completeerating:|█████████-------------------------------| 22.7% (1136/5011)  Complete\n\n\nYOLO Saving:   |████████████████████████████████████████| 100.0% (5011/5011)  Complete\n\n\nVOC Parsing:   |████████████████████████████████████████| 100.0% (4952/4952)  Complete\n\n\nYOLO Generating:|████████████████████████████████████████| 100.0% (4952/4952)  Complete\n\n\nYOLO Saving:   |████████████████████████████████████████| 100.0% (4952/4952)  Complete\n# Pasal VOC 제공 파일로 train, val 라벨 분할\nimport shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\nwith open(path) as f:\n    image_ids = f.read().strip().split()\n    for id in image_ids:\n        ori_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007'\n        mv_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/val2007'\n        shutil.move(f\"{ori_path}/{id}.txt\", f\"{mv_path}/{id}.txt\")\n# train / val / test 이미지 파일 복사\nimport os, shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets'\nfor folder, subset in ('trainval', 'train2007'), ('trainval', 'val2007'), ('test', 'test2007'):\n    ex_imgs_path = f'{path}/{folder}/VOCdevkit/VOC2007/JPEGImages'\n    label_path = f'{path}/VOC/labels/{subset}'\n    img_path = f'{path}/VOC/images/{subset}'\n    print(subset,\": \", len(os.listdir(label_path)))\n    for lbs_list in os.listdir(label_path):\n        shutil.move(os.path.join(ex_imgs_path,lbs_list.split('.')[0]+'.jpg'), os.path.join(img_path,lbs_list.split('.')[0]+'.jpg'))\n    \n\ntrain2007 :  2501\nval2007 :  2510\ntest2007 :  4952"
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html#커스텀-데이터-준비",
    "href": "posts/CV_ObjectDetection_0.html#커스텀-데이터-준비",
    "title": "CV_ObjectDetection_0",
    "section": "커스텀 데이터 준비",
    "text": "커스텀 데이터 준비\n\n# 라벨링한 커스텀 데이터셋 다운로드\n!wget 'https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ'\n\n--2024-02-08 09:16:46--  https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ [following]\n--2024-02-08 09:16:46--  https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.222.33, 2404:6800:4004:818::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.222.33|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 443876 (433K) [application/octet-stream]\nSaving to: ‘uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ’\n\nuc?id=1avRwo9y3M1Op 100%[===================&gt;] 433.47K  1.34MB/s    in 0.3s    \n\n2024-02-08 09:16:48 (1.34 MB/s) - ‘uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ’ saved [443876/443876]\n\n\n\n\n# 파일 압축 해제\nimport zipfile\nwith zipfile.ZipFile(\"04_detection_custom_datasets.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"04_detection_custom_datasets\")\n\n\n# Yolo 6.2 release version 클론\n%cd /root/2024\\ winter/DL_tutorial/posts\n! git clone -b v6.2 https://github.com/ultralytics/yolov5.git\n%cd yolov5\n# 필수 라이브러리 다운로드\n!pip install numpy==1.23.0\n%pip install -qr requirements.txt\nimport torch\nimport utils\ndisplay = utils.notebook_init()\n\nYOLOv5 🚀 v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\n\nSetup complete ✅ (128 CPUs, 503.7 GB RAM, 1001.9/1757.9 GB disk)\n\n\n\n# Weights & Biases 셋팅\n%pip install -q wandb\nimport wandb\nwandb.login()\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nNote: you may need to restart the kernel to use updated packages.\n\n\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n# 재현을 위한 랜덤 시드 고정\nimport random\nimport numpy as np\nimport torch\n\nseed = 2024\ndeterministic = True\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nif deterministic:\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n%cd /root/2024winter/DL_tutorial/posts/yolov5\n# Pasal VOC 데이터로 YOLO v5 훈련\n!python train.py --img 640 --batch 32 --epochs 30 --data custom_voc.yaml --weights '' --cfg yolov5s.yaml --seed 2024\n\n/root/2024winter/DL_tutorial/posts/yolov5\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\ntrain: weights=, cfg=yolov5s.yaml, data=custom_voc.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=2024, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\ngithub: skipping check (offline), for updates see https://github.com/ultralytics/yolov5\nrequirements: Pillow==7.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install Pillow==7.2.0' skipped (offline)\nrequirements: cycler==0.10.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install cycler==0.10.0' skipped (offline)\nrequirements: kiwisolver==1.0.1 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install kiwisolver==1.0.1' skipped (offline)\nrequirements: matplotlib==2.2.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install matplotlib==2.2.2' skipped (offline)\nrequirements: numpy==1.14.3 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install numpy==1.14.3' skipped (offline)\nrequirements: pyparsing==2.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pyparsing==2.2.0' skipped (offline)\nrequirements: python-dateutil==2.7.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install python-dateutil==2.7.2' skipped (offline)\nrequirements: pytz==2018.4 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pytz==2018.4' skipped (offline)\nrequirements: six==1.11.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install six==1.11.0' skipped (offline)\nYOLOv5 🚀 v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nwandb: Tracking run with wandb version 0.16.3\nwandb: Run data is saved locally in /root/2024winter/DL_tutorial/posts/yolov5/wandb/run-20240209_073845-bf46v602\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run exalted-thunder-9\nwandb: ⭐️ View project at https://wandb.ai/dlwjdwo1109/YOLOv5\nwandb: 🚀 View run at https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\n\nDataset not found ⚠️, missing paths ['/content/pascal_datasets/VOC/images/test2007']\nTraceback (most recent call last):\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 632, in &lt;module&gt;\n    main(opt)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 528, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 90, in train\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/__init__.py\", line 95, in __init__\n    self.wandb = WandbLogger(self.opt, run_id)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 187, in __init__\n    self.data_dict = check_wandb_dataset(opt.data)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 59, in check_wandb_dataset\n    return check_dataset(data_file)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/general.py\", line 505, in check_dataset\n    raise Exception('Dataset not found ❌')\nException: Dataset not found ❌\nwandb: 🚀 View run exalted-thunder-9 at: https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\nwandb: ️⚡ View job at https://wandb.ai/dlwjdwo1109/YOLOv5/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODM4NjQzMg==/version_details/v0\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240209_073845-bf46v602/logs"
  },
  {
    "objectID": "posts/ANN_2.html",
    "href": "posts/ANN_2.html",
    "title": "ANN_2",
    "section": "",
    "text": "Step 1 : Load libraries & Datasts\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n\n# FashionMNIST 데이터 불러오기\ntraining_data = datasets.FashionMNIST(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = ToTensor()\n)\n\n\ntest_data = datasets.FashionMNIST(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = ToTensor()\n)\n\n\n\nStep 2 : Create DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\n\ndevice = cuda\n\n\n\n\nEDA\n\nprint(training_data, '\\n--------------------------\\n', test_data)\n\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n--------------------------\n Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f'Feature batch shape: {train_features.size()}')\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\n\n\n\nlen(training_data)\n\n60000\n\n\n\nimg, label = training_data[0]\nplt.imshow(img.squeeze(), cmap='gray')\nprint(f'label={label}')\n\nlabel=9\n\n\n\n\n\n\n\n\n\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\n\nfigure = plt.figure(figsize = (20, 8))\ncols, rows = 5, 2\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    print(labels_map[label])\n    plt.axis('off')\n    plt.imshow(img.squeeze(), cmap='gray')\nplt.show()\n\nBag\nBag\nTrouser\nAnkle Boot\nSneaker\nCoat\nCoat\nBag\nAnkle Boot\nShirt\n\n\n\n\n\n\n\n\n\n\n\nStep 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.classifier = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 10)\n        )\n    def forward(self, x):\n        x = self.flatten(x)\n        output = self.classifier(x)\n        return output\n\n\n\nStep 4 : Create Model instacne\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\n\nModel test\n\nX = torch.rand(1, 28, 28, device=device)\noutput = model(X)\nprint(f'모델 출력 결과: {output}\\n')\npred_probab = nn.Softmax(dim=1)(output)\nprint(f'Softmax 결과: {pred_probab}\\n')\ny_pred = pred_probab.argmax()\nprint(y_pred)\n\n모델 출력 결과: tensor([[ 0.2122, -0.0533,  0.4609, -0.1348, -0.2897,  0.0426,  0.2330, -0.0539,\n         -0.2371,  0.3135]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\nSoftmax 결과: tensor([[0.1144, 0.0878, 0.1468, 0.0809, 0.0693, 0.0966, 0.1169, 0.0877, 0.0730,\n         0.1267]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)\n\ntensor(2, device='cuda:0')\n\n\n\n\nStep 5 : Model compile\n\n# Loss\nloss = nn.CrossEntropyLoss()\n# Optimizer\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\nStep 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:7&gt;f} [{current:&gt;5d}]/{size:5d}')\n\n\n\nStep 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')\n\n\n\nStep 8 : Run model\n\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n--------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\nprint(\"Done\")\n\nEpoch 1 \n--------------------------\nloss: 0.431373 [    0]/60000\nloss: 0.587412 [ 6400]/60000\nloss: 0.583656 [12800]/60000\nloss: 0.526607 [19200]/60000\nloss: 0.425904 [25600]/60000\nloss: 0.559104 [32000]/60000\nloss: 0.255318 [38400]/60000\nloss: 0.401609 [44800]/60000\nloss: 0.359597 [51200]/60000\nloss: 0.415519 [57600]/60000\nTest Error: \n Accuracy: 84.0%, Avg loss: 0.441000\n\nEpoch 2 \n--------------------------\nloss: 0.396597 [    0]/60000\nloss: 0.282736 [ 6400]/60000\nloss: 0.348952 [12800]/60000\nloss: 0.394269 [19200]/60000\nloss: 0.309174 [25600]/60000\nloss: 0.406731 [32000]/60000\nloss: 0.255115 [38400]/60000\nloss: 0.464652 [44800]/60000\nloss: 0.266001 [51200]/60000\nloss: 0.316354 [57600]/60000\nTest Error: \n Accuracy: 86.0%, Avg loss: 0.396377\n\nEpoch 3 \n--------------------------\nloss: 0.343163 [    0]/60000\nloss: 0.417957 [ 6400]/60000\nloss: 0.361057 [12800]/60000\nloss: 0.290214 [19200]/60000\nloss: 0.439867 [25600]/60000\nloss: 0.573623 [32000]/60000\nloss: 0.358405 [38400]/60000\nloss: 0.280711 [44800]/60000\nloss: 0.403463 [51200]/60000\nloss: 0.364402 [57600]/60000\nTest Error: \n Accuracy: 85.9%, Avg loss: 0.389022\n\nEpoch 4 \n--------------------------\nloss: 0.269444 [    0]/60000\nloss: 0.317166 [ 6400]/60000\nloss: 0.371522 [12800]/60000\nloss: 0.389202 [19200]/60000\nloss: 0.290166 [25600]/60000\nloss: 0.557429 [32000]/60000\nloss: 0.463130 [38400]/60000\nloss: 0.320192 [44800]/60000\nloss: 0.230014 [51200]/60000\nloss: 0.221253 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.380155\n\nEpoch 5 \n--------------------------\nloss: 0.378960 [    0]/60000\nloss: 0.463637 [ 6400]/60000\nloss: 0.203711 [12800]/60000\nloss: 0.376729 [19200]/60000\nloss: 0.376216 [25600]/60000\nloss: 0.302987 [32000]/60000\nloss: 0.401179 [38400]/60000\nloss: 0.283290 [44800]/60000\nloss: 0.314023 [51200]/60000\nloss: 0.380436 [57600]/60000\nTest Error: \n Accuracy: 86.1%, Avg loss: 0.391101\n\nEpoch 6 \n--------------------------\nloss: 0.324956 [    0]/60000\nloss: 0.376954 [ 6400]/60000\nloss: 0.286267 [12800]/60000\nloss: 0.302243 [19200]/60000\nloss: 0.253790 [25600]/60000\nloss: 0.259367 [32000]/60000\nloss: 0.489078 [38400]/60000\nloss: 0.244414 [44800]/60000\nloss: 0.335011 [51200]/60000\nloss: 0.284465 [57600]/60000\nTest Error: \n Accuracy: 86.4%, Avg loss: 0.392578\n\nEpoch 7 \n--------------------------\nloss: 0.470740 [    0]/60000\nloss: 0.352301 [ 6400]/60000\nloss: 0.215639 [12800]/60000\nloss: 0.214226 [19200]/60000\nloss: 0.198628 [25600]/60000\nloss: 0.290468 [32000]/60000\nloss: 0.396786 [38400]/60000\nloss: 0.337734 [44800]/60000\nloss: 0.209183 [51200]/60000\nloss: 0.446850 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.375429\n\nEpoch 8 \n--------------------------\nloss: 0.325714 [    0]/60000\nloss: 0.252096 [ 6400]/60000\nloss: 0.266274 [12800]/60000\nloss: 0.404169 [19200]/60000\nloss: 0.398270 [25600]/60000\nloss: 0.240059 [32000]/60000\nloss: 0.345725 [38400]/60000\nloss: 0.339798 [44800]/60000\nloss: 0.232539 [51200]/60000\nloss: 0.371980 [57600]/60000\nTest Error: \n Accuracy: 87.3%, Avg loss: 0.363998\n\nEpoch 9 \n--------------------------\nloss: 0.275302 [    0]/60000\nloss: 0.459145 [ 6400]/60000\nloss: 0.477752 [12800]/60000\nloss: 0.368110 [19200]/60000\nloss: 0.166666 [25600]/60000\nloss: 0.340989 [32000]/60000\nloss: 0.412024 [38400]/60000\nloss: 0.255185 [44800]/60000\nloss: 0.370468 [51200]/60000\nloss: 0.383511 [57600]/60000\nTest Error: \n Accuracy: 87.2%, Avg loss: 0.362548\n\nEpoch 10 \n--------------------------\nloss: 0.163738 [    0]/60000\nloss: 0.234758 [ 6400]/60000\nloss: 0.498840 [12800]/60000\nloss: 0.502605 [19200]/60000\nloss: 0.460932 [25600]/60000\nloss: 0.459421 [32000]/60000\nloss: 0.243460 [38400]/60000\nloss: 0.231028 [44800]/60000\nloss: 0.239964 [51200]/60000\nloss: 0.273228 [57600]/60000\nTest Error: \n Accuracy: 87.0%, Avg loss: 0.363198\n\nDone\n\n\n\n\nStep 9 : Save & load model\n\nparameter만 저장하고 불러오기\n\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n\nmodel2 = NeuralNetwork().to(device)\nprint(model2)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\nmodel2.load_state_dict(torch.load('model_weights.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel2.eval()\ntest_loop(test_dataloader, model2, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797\n\n\n\n\n\n\nModel 전체를 저장하고 불러오기\n\ntorch.save(model, 'model.pth')\n\n\nmodel3 = torch.load('model.pth')\n\n\nmodel3.eval()\ntest_loop(test_dataloader, model3, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797"
  },
  {
    "objectID": "posts/ANN_1.html",
    "href": "posts/ANN_1.html",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-1-import",
    "href": "posts/ANN_1.html#step-1-import",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-2-create-dataloader",
    "href": "posts/ANN_1.html#step-2-create-dataloader",
    "title": "ANN_1",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# 데이터 불러오기\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['label'] = iris.target\n\n# 데이터 분할\ny = df['label']\nX = df.drop(['label'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values,\n                                                   random_state=42, stratify=y)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.int64)\ny_test = torch.tensor(y_test, dtype=torch.int64)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
  },
  {
    "objectID": "posts/ANN_1.html#step-3-set-network-structure",
    "href": "posts/ANN_1.html#step-3-set-network-structure",
    "title": "ANN_1",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.input_layer = nn.Linear(4, 16)\n        self.hidden_layer1 = nn.Linear(16, 32)\n        self.output_layer = nn.Linear(32, 3)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.relu(self.input_layer(x))\n        out = self.relu(self.hidden_layer1(out))\n        out = self.output_layer(out)\n        return out"
  },
  {
    "objectID": "posts/ANN_1.html#step-4-create-model-instance",
    "href": "posts/ANN_1.html#step-4-create-model-instance",
    "title": "ANN_1",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\nmodel = NeuralNetwork().to(device)\n\ndevice = cuda"
  },
  {
    "objectID": "posts/ANN_1.html#step-5-model-compile",
    "href": "posts/ANN_1.html#step-5-model-compile",
    "title": "ANN_1",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\n# 모델 컴파일\nlearning_rate = 0.001\nloss = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
  },
  {
    "objectID": "posts/ANN_1.html#step-6-set-train-loop",
    "href": "posts/ANN_1.html#step-6-set-train-loop",
    "title": "ANN_1",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss, current = loss.item(), batch * len(X)\n        print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/ANN_1.html#step-7-set-test-loop",
    "href": "posts/ANN_1.html#step-7-set-test-loop",
    "title": "ANN_1",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error : \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')"
  },
  {
    "objectID": "posts/ANN_1.html#step-8-run-model",
    "href": "posts/ANN_1.html#step-8-run-model",
    "title": "ANN_1",
    "section": "Step 8 : Run model",
    "text": "Step 8 : Run model\n\n# 모델 실행\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n------------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n------------------------------\nloss: 1.104175  [    0]/  112\nloss: 1.063668  [   10]/  112\nloss: 1.001417  [   20]/  112\nloss: 0.974181  [   30]/  112\nloss: 1.099196  [   40]/  112\nloss: 1.131157  [   50]/  112\nloss: 1.029493  [   60]/  112\nloss: 1.046669  [   70]/  112\nloss: 0.991781  [   80]/  112\nloss: 0.980483  [   90]/  112\nloss: 0.995154  [  100]/  112\nloss: 1.178336  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 1.013189\n\nEpoch 2 \n------------------------------\nloss: 0.998811  [    0]/  112\nloss: 1.072228  [   10]/  112\nloss: 0.861224  [   20]/  112\nloss: 1.145309  [   30]/  112\nloss: 0.923511  [   40]/  112\nloss: 1.029394  [   50]/  112\nloss: 1.038562  [   60]/  112\nloss: 0.923659  [   70]/  112\nloss: 1.022684  [   80]/  112\nloss: 0.942879  [   90]/  112\nloss: 1.011915  [  100]/  112\nloss: 0.944836  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 0.978484\n\nEpoch 3 \n------------------------------\nloss: 0.973278  [    0]/  112\nloss: 0.857192  [   10]/  112\nloss: 1.025384  [   20]/  112\nloss: 0.974613  [   30]/  112\nloss: 0.966374  [   40]/  112\nloss: 0.901531  [   50]/  112\nloss: 1.019788  [   60]/  112\nloss: 0.999571  [   70]/  112\nloss: 0.854874  [   80]/  112\nloss: 1.000906  [   90]/  112\nloss: 0.900930  [  100]/  112\nloss: 1.044610  [   22]/  112\nTest Error : \n Accuracy: 39.5%, Avg loss: 0.925748\n\nEpoch 4 \n------------------------------\nloss: 1.016448  [    0]/  112\nloss: 0.965601  [   10]/  112\nloss: 0.949101  [   20]/  112\nloss: 0.891714  [   30]/  112\nloss: 0.936245  [   40]/  112\nloss: 0.809627  [   50]/  112\nloss: 0.898013  [   60]/  112\nloss: 0.915918  [   70]/  112\nloss: 0.908495  [   80]/  112\nloss: 0.837218  [   90]/  112\nloss: 0.851940  [  100]/  112\nloss: 1.014478  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.887178\n\nEpoch 5 \n------------------------------\nloss: 0.929125  [    0]/  112\nloss: 0.860350  [   10]/  112\nloss: 0.894982  [   20]/  112\nloss: 0.848951  [   30]/  112\nloss: 0.874831  [   40]/  112\nloss: 0.867019  [   50]/  112\nloss: 0.865059  [   60]/  112\nloss: 0.850359  [   70]/  112\nloss: 0.778505  [   80]/  112\nloss: 0.894542  [   90]/  112\nloss: 0.853558  [  100]/  112\nloss: 0.779387  [   22]/  112\nTest Error : \n Accuracy: 68.4%, Avg loss: 0.844504\n\nEpoch 6 \n------------------------------\nloss: 0.901229  [    0]/  112\nloss: 0.800752  [   10]/  112\nloss: 0.798276  [   20]/  112\nloss: 0.743961  [   30]/  112\nloss: 0.844832  [   40]/  112\nloss: 0.806112  [   50]/  112\nloss: 0.788561  [   60]/  112\nloss: 0.879695  [   70]/  112\nloss: 0.851877  [   80]/  112\nloss: 0.764796  [   90]/  112\nloss: 0.764707  [  100]/  112\nloss: 0.804097  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.798580\n\nEpoch 7 \n------------------------------\nloss: 0.797247  [    0]/  112\nloss: 0.836456  [   10]/  112\nloss: 0.829977  [   20]/  112\nloss: 0.725152  [   30]/  112\nloss: 0.715440  [   40]/  112\nloss: 0.670871  [   50]/  112\nloss: 0.751514  [   60]/  112\nloss: 0.811848  [   70]/  112\nloss: 0.754924  [   80]/  112\nloss: 0.847931  [   90]/  112\nloss: 0.647542  [  100]/  112\nloss: 0.773911  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.743698\n\nEpoch 8 \n------------------------------\nloss: 0.745911  [    0]/  112\nloss: 0.756747  [   10]/  112\nloss: 0.697434  [   20]/  112\nloss: 0.770861  [   30]/  112\nloss: 0.675138  [   40]/  112\nloss: 0.756205  [   50]/  112\nloss: 0.699248  [   60]/  112\nloss: 0.632746  [   70]/  112\nloss: 0.680969  [   80]/  112\nloss: 0.731905  [   90]/  112\nloss: 0.697679  [  100]/  112\nloss: 0.612848  [   22]/  112\nTest Error : \n Accuracy: 73.7%, Avg loss: 0.692100\n\nEpoch 9 \n------------------------------\nloss: 0.709632  [    0]/  112\nloss: 0.619151  [   10]/  112\nloss: 0.675694  [   20]/  112\nloss: 0.595196  [   30]/  112\nloss: 0.690987  [   40]/  112\nloss: 0.679168  [   50]/  112\nloss: 0.648694  [   60]/  112\nloss: 0.642780  [   70]/  112\nloss: 0.661847  [   80]/  112\nloss: 0.611258  [   90]/  112\nloss: 0.658153  [  100]/  112\nloss: 0.781440  [   22]/  112\nTest Error : \n Accuracy: 71.1%, Avg loss: 0.642173\n\nEpoch 10 \n------------------------------\nloss: 0.675161  [    0]/  112\nloss: 0.570343  [   10]/  112\nloss: 0.511772  [   20]/  112\nloss: 0.692174  [   30]/  112\nloss: 0.523068  [   40]/  112\nloss: 0.557191  [   50]/  112\nloss: 0.715832  [   60]/  112\nloss: 0.556040  [   70]/  112\nloss: 0.620830  [   80]/  112\nloss: 0.581826  [   90]/  112\nloss: 0.604469  [  100]/  112\nloss: 0.707001  [   22]/  112\nTest Error : \n Accuracy: 81.6%, Avg loss: 0.595252\n\nDone!"
  },
  {
    "objectID": "posts/CV_Instance_Segmentation.html",
    "href": "posts/CV_Instance_Segmentation.html",
    "title": "CV_Instance_Segmentation_0",
    "section": "",
    "text": "#!pip install git+https://github.com/facebookresearch/fvcore.git\n#!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n#!pip install -e detectron2_repo\n\n\n\n\n\n!rm -r coco\n\nrm: cannot remove 'coco': No such file or directory\n\n\n\n!wget http://images.cocodataset.org/zips/train2017.zip\n!wget http://images.cocodataset.org/zips/val2017.zip\n!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n\n--2024-02-13 02:07:06--  http://images.cocodataset.org/zips/train2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 16.182.33.73, 16.182.70.25, 52.217.104.196, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|16.182.33.73|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 19336861798 (18G) [application/zip]\nSaving to: ‘train2017.zip’\n\ntrain2017.zip       100%[===================&gt;]  18.01G  14.6MB/s    in 21m 15s \n\n2024-02-13 02:28:21 (14.5 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n\n--2024-02-13 02:28:21--  http://images.cocodataset.org/zips/val2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 52.217.134.73, 54.231.199.1, 54.231.235.145, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|52.217.134.73|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 815585330 (778M) [application/zip]\nSaving to: ‘val2017.zip’\n\nval2017.zip         100%[===================&gt;] 777.80M  8.26MB/s    in 89s     \n\n2024-02-13 02:29:51 (8.70 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n\n--2024-02-13 02:29:51--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 52.216.51.129, 52.216.213.169, 3.5.10.168, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|52.216.51.129|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 252907541 (241M) [application/zip]\nSaving to: ‘annotations_trainval2017.zip’\n\nannotations_trainva 100%[===================&gt;] 241.19M  15.6MB/s    in 17s     \n\n2024-02-13 02:30:09 (14.3 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n\n\n\n\n# Unzip\n!unzip -q train2017.zip -d ./coco\n!unzip -q val2017.zip -d ./coco\n!unzip -q annotations_trainval2017.zip -d ./coco\n\n\n# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n\n\nimport torch\n# 재현성을 위한 시드넘버 고정\nrandom_seed = 2024\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\n\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"coco_train_data\",{},'/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json', '/root/2024winter/DL_tutorial/posts/coco/val2017')\nregister_coco_instances(\"coco_val_data\",{},'/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json', '/root/2024winter/DL_tutorial/posts/coco/val2017')\n\n\n# MetadataCatalog\ntrain_metadata = MetadataCatalog.get(\"coco_train_data\")\nval_metadata = MetadataCatalog.get(\"coco_val_data\")\ntrain_metadata\n\nnamespace(name='coco_train_data',\n          json_file='/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json',\n          image_root='/root/2024winter/DL_tutorial/posts/coco/val2017',\n          evaluator_type='coco')\n\n\n\n# DatasetCatalog\ntrain_dataset_dicts = DatasetCatalog.get(\"coco_train_data\")\nval_dataset_dicts = DatasetCatalog.get(\"coco_val_data\")\n\n[02/13 04:42:15 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:42:16 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.03 seconds.\n[02/13 04:42:17 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n\n\n\n\n\nfor d in random.sample(train_dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.figure(figsize=(5, 5))\n    plt.imshow(vis.get_image()[:, :, ::-1])\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(\n    \"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n)\ncfg.DATASETS.TRAIN = (\"coco_train_data\",)\ncfg.DATASETS.TEST = (\"coco_val_data\",)  # no metrics implemented for this dataset\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.002\ncfg.SOLVER.MAX_ITER = 1000\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\ncfg.OUTPUT_DIR = \"/root/2024winter/DL_tutorial/posts/outputs\"\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()\n\n[02/13 04:44:51 d2.engine.defaults]: Model:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n[02/13 04:44:52 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.07 seconds.\n[02/13 04:44:52 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:44:52 d2.data.build]: Removed 48 images with no usable annotations. 4952 images left.\n[02/13 04:44:52 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n[02/13 04:44:52 d2.data.build]: Using training sampler TrainingSampler\n[02/13 04:44:52 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:44:52 d2.data.common]: Serializing 4952 elements to byte tensors and concatenating them all ...\n[02/13 04:44:52 d2.data.common]: Serialized dataset takes 19.21 MiB\n[02/13 04:44:52 d2.data.build]: Making batched data loader with batch_size=2\nWARNING [02/13 04:44:52 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n[02/13 04:44:53 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n[02/13 04:44:53 d2.engine.train_loop]: Starting training from iteration 0\n[02/13 04:44:55 d2.utils.events]:  eta: 0:01:51  iter: 19  total_loss: 0.9135  loss_cls: 0.2255  loss_box_reg: 0.3434  loss_mask: 0.1967  loss_rpn_cls: 0.01907  loss_rpn_loc: 0.02518    time: 0.1183  last_time: 0.1094  data_time: 0.0207  last_data_time: 0.0048   lr: 3.9962e-05  max_mem: 2468M\n[02/13 04:44:58 d2.utils.events]:  eta: 0:01:53  iter: 39  total_loss: 0.9642  loss_cls: 0.2753  loss_box_reg: 0.3455  loss_mask: 0.2147  loss_rpn_cls: 0.02717  loss_rpn_loc: 0.04455    time: 0.1210  last_time: 0.1222  data_time: 0.0081  last_data_time: 0.0082   lr: 7.9922e-05  max_mem: 2468M\n[02/13 04:45:00 d2.utils.events]:  eta: 0:01:50  iter: 59  total_loss: 1.091  loss_cls: 0.2995  loss_box_reg: 0.4305  loss_mask: 0.2721  loss_rpn_cls: 0.0325  loss_rpn_loc: 0.05139    time: 0.1211  last_time: 0.1111  data_time: 0.0063  last_data_time: 0.0075   lr: 0.00011988  max_mem: 2468M\n[02/13 04:45:03 d2.utils.events]:  eta: 0:01:48  iter: 79  total_loss: 0.9919  loss_cls: 0.2939  loss_box_reg: 0.3489  loss_mask: 0.2043  loss_rpn_cls: 0.03127  loss_rpn_loc: 0.04819    time: 0.1205  last_time: 0.1033  data_time: 0.0104  last_data_time: 0.0054   lr: 0.00015984  max_mem: 2468M\n[02/13 04:45:05 d2.utils.events]:  eta: 0:01:45  iter: 99  total_loss: 1.055  loss_cls: 0.2229  loss_box_reg: 0.4061  loss_mask: 0.2197  loss_rpn_cls: 0.04041  loss_rpn_loc: 0.06296    time: 0.1200  last_time: 0.1120  data_time: 0.0085  last_data_time: 0.0115   lr: 0.0001998  max_mem: 2468M\n[02/13 04:45:08 d2.utils.events]:  eta: 0:01:45  iter: 119  total_loss: 1.126  loss_cls: 0.29  loss_box_reg: 0.3477  loss_mask: 0.2278  loss_rpn_cls: 0.04413  loss_rpn_loc: 0.08736    time: 0.1218  last_time: 0.1353  data_time: 0.0086  last_data_time: 0.0079   lr: 0.00023976  max_mem: 2468M\n[02/13 04:45:10 d2.utils.events]:  eta: 0:01:44  iter: 139  total_loss: 1.052  loss_cls: 0.3016  loss_box_reg: 0.4075  loss_mask: 0.2272  loss_rpn_cls: 0.03207  loss_rpn_loc: 0.04996    time: 0.1235  last_time: 0.1336  data_time: 0.0109  last_data_time: 0.0096   lr: 0.00027972  max_mem: 2468M\n[02/13 04:45:13 d2.utils.events]:  eta: 0:01:41  iter: 159  total_loss: 1.12  loss_cls: 0.295  loss_box_reg: 0.401  loss_mask: 0.2615  loss_rpn_cls: 0.02364  loss_rpn_loc: 0.04561    time: 0.1234  last_time: 0.1250  data_time: 0.0067  last_data_time: 0.0081   lr: 0.00031968  max_mem: 2473M\n[02/13 04:45:16 d2.utils.events]:  eta: 0:01:39  iter: 179  total_loss: 0.9365  loss_cls: 0.2408  loss_box_reg: 0.3592  loss_mask: 0.233  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.03028    time: 0.1244  last_time: 0.1110  data_time: 0.0151  last_data_time: 0.0066   lr: 0.00035964  max_mem: 2473M\n[02/13 04:45:18 d2.utils.events]:  eta: 0:01:37  iter: 199  total_loss: 0.9998  loss_cls: 0.2416  loss_box_reg: 0.4081  loss_mask: 0.2409  loss_rpn_cls: 0.02947  loss_rpn_loc: 0.03085    time: 0.1246  last_time: 0.1183  data_time: 0.0094  last_data_time: 0.0052   lr: 0.0003996  max_mem: 2473M\n[02/13 04:45:20 d2.utils.events]:  eta: 0:01:34  iter: 219  total_loss: 0.9961  loss_cls: 0.2573  loss_box_reg: 0.3871  loss_mask: 0.236  loss_rpn_cls: 0.03193  loss_rpn_loc: 0.03565    time: 0.1242  last_time: 0.1278  data_time: 0.0062  last_data_time: 0.0088   lr: 0.00043956  max_mem: 2473M\n[02/13 04:45:23 d2.utils.events]:  eta: 0:01:32  iter: 239  total_loss: 0.9405  loss_cls: 0.2386  loss_box_reg: 0.3411  loss_mask: 0.2369  loss_rpn_cls: 0.02037  loss_rpn_loc: 0.0328    time: 0.1240  last_time: 0.1462  data_time: 0.0074  last_data_time: 0.0097   lr: 0.00047952  max_mem: 2473M\n[02/13 04:45:26 d2.utils.events]:  eta: 0:01:29  iter: 259  total_loss: 1.075  loss_cls: 0.3166  loss_box_reg: 0.3828  loss_mask: 0.2512  loss_rpn_cls: 0.0336  loss_rpn_loc: 0.04556    time: 0.1245  last_time: 0.1265  data_time: 0.0094  last_data_time: 0.0056   lr: 0.00051948  max_mem: 2473M\n[02/13 04:45:28 d2.utils.events]:  eta: 0:01:26  iter: 279  total_loss: 0.9532  loss_cls: 0.2711  loss_box_reg: 0.3281  loss_mask: 0.2328  loss_rpn_cls: 0.02803  loss_rpn_loc: 0.03686    time: 0.1239  last_time: 0.1217  data_time: 0.0073  last_data_time: 0.0062   lr: 0.00055944  max_mem: 2473M\n[02/13 04:45:30 d2.utils.events]:  eta: 0:01:24  iter: 299  total_loss: 0.8519  loss_cls: 0.236  loss_box_reg: 0.3498  loss_mask: 0.2202  loss_rpn_cls: 0.01544  loss_rpn_loc: 0.02504    time: 0.1239  last_time: 0.1432  data_time: 0.0085  last_data_time: 0.0068   lr: 0.0005994  max_mem: 2473M\n[02/13 04:45:33 d2.utils.events]:  eta: 0:01:22  iter: 319  total_loss: 0.9913  loss_cls: 0.2939  loss_box_reg: 0.3954  loss_mask: 0.2473  loss_rpn_cls: 0.02953  loss_rpn_loc: 0.05801    time: 0.1243  last_time: 0.1396  data_time: 0.0099  last_data_time: 0.0036   lr: 0.00063936  max_mem: 2473M\n[02/13 04:45:35 d2.utils.events]:  eta: 0:01:19  iter: 339  total_loss: 1.084  loss_cls: 0.2824  loss_box_reg: 0.3847  loss_mask: 0.2359  loss_rpn_cls: 0.03157  loss_rpn_loc: 0.03688    time: 0.1241  last_time: 0.1225  data_time: 0.0076  last_data_time: 0.0057   lr: 0.00067932  max_mem: 2473M\n[02/13 04:45:38 d2.utils.events]:  eta: 0:01:16  iter: 359  total_loss: 0.9851  loss_cls: 0.2923  loss_box_reg: 0.3409  loss_mask: 0.2503  loss_rpn_cls: 0.01778  loss_rpn_loc: 0.03838    time: 0.1236  last_time: 0.1043  data_time: 0.0061  last_data_time: 0.0077   lr: 0.00071928  max_mem: 2473M\n[02/13 04:45:40 d2.utils.events]:  eta: 0:01:14  iter: 379  total_loss: 0.9603  loss_cls: 0.243  loss_box_reg: 0.3565  loss_mask: 0.2018  loss_rpn_cls: 0.02237  loss_rpn_loc: 0.04897    time: 0.1233  last_time: 0.1007  data_time: 0.0075  last_data_time: 0.0057   lr: 0.00075924  max_mem: 2473M\n[02/13 04:45:43 d2.utils.events]:  eta: 0:01:11  iter: 399  total_loss: 0.8493  loss_cls: 0.193  loss_box_reg: 0.2683  loss_mask: 0.2049  loss_rpn_cls: 0.04498  loss_rpn_loc: 0.06218    time: 0.1231  last_time: 0.1183  data_time: 0.0095  last_data_time: 0.0048   lr: 0.0007992  max_mem: 2473M\n[02/13 04:45:45 d2.utils.events]:  eta: 0:01:09  iter: 419  total_loss: 0.9737  loss_cls: 0.2657  loss_box_reg: 0.3631  loss_mask: 0.2675  loss_rpn_cls: 0.0237  loss_rpn_loc: 0.02856    time: 0.1228  last_time: 0.1337  data_time: 0.0061  last_data_time: 0.0055   lr: 0.00083916  max_mem: 2473M\n[02/13 04:45:47 d2.utils.events]:  eta: 0:01:06  iter: 439  total_loss: 0.9801  loss_cls: 0.2758  loss_box_reg: 0.3691  loss_mask: 0.2352  loss_rpn_cls: 0.03374  loss_rpn_loc: 0.0679    time: 0.1228  last_time: 0.1154  data_time: 0.0095  last_data_time: 0.0057   lr: 0.00087912  max_mem: 2473M\n[02/13 04:45:50 d2.utils.events]:  eta: 0:01:04  iter: 459  total_loss: 0.9442  loss_cls: 0.2619  loss_box_reg: 0.3198  loss_mask: 0.2345  loss_rpn_cls: 0.03548  loss_rpn_loc: 0.05815    time: 0.1226  last_time: 0.1406  data_time: 0.0054  last_data_time: 0.0030   lr: 0.00091908  max_mem: 2473M\n[02/13 04:45:52 d2.utils.events]:  eta: 0:01:01  iter: 479  total_loss: 1.086  loss_cls: 0.2395  loss_box_reg: 0.4103  loss_mask: 0.2464  loss_rpn_cls: 0.04081  loss_rpn_loc: 0.06109    time: 0.1225  last_time: 0.1251  data_time: 0.0082  last_data_time: 0.0061   lr: 0.00095904  max_mem: 2473M\n[02/13 04:45:55 d2.utils.events]:  eta: 0:00:59  iter: 499  total_loss: 1.179  loss_cls: 0.31  loss_box_reg: 0.3986  loss_mask: 0.2535  loss_rpn_cls: 0.04035  loss_rpn_loc: 0.1044    time: 0.1225  last_time: 0.1086  data_time: 0.0073  last_data_time: 0.0070   lr: 0.000999  max_mem: 2473M\n[02/13 04:45:57 d2.utils.events]:  eta: 0:00:57  iter: 519  total_loss: 0.9325  loss_cls: 0.2939  loss_box_reg: 0.3721  loss_mask: 0.2286  loss_rpn_cls: 0.02657  loss_rpn_loc: 0.02841    time: 0.1229  last_time: 0.1582  data_time: 0.0096  last_data_time: 0.0217   lr: 0.001039  max_mem: 2473M\n[02/13 04:46:00 d2.utils.events]:  eta: 0:00:55  iter: 539  total_loss: 0.9261  loss_cls: 0.2287  loss_box_reg: 0.3299  loss_mask: 0.2194  loss_rpn_cls: 0.02425  loss_rpn_loc: 0.05447    time: 0.1230  last_time: 0.1237  data_time: 0.0062  last_data_time: 0.0050   lr: 0.0010789  max_mem: 2473M\n[02/13 04:46:02 d2.utils.events]:  eta: 0:00:52  iter: 559  total_loss: 1.173  loss_cls: 0.3525  loss_box_reg: 0.3845  loss_mask: 0.26  loss_rpn_cls: 0.03674  loss_rpn_loc: 0.0619    time: 0.1229  last_time: 0.1308  data_time: 0.0065  last_data_time: 0.0054   lr: 0.0011189  max_mem: 2473M\n[02/13 04:46:05 d2.utils.events]:  eta: 0:00:50  iter: 579  total_loss: 1.017  loss_cls: 0.3499  loss_box_reg: 0.3281  loss_mask: 0.2034  loss_rpn_cls: 0.03605  loss_rpn_loc: 0.04706    time: 0.1231  last_time: 0.1141  data_time: 0.0105  last_data_time: 0.0081   lr: 0.0011588  max_mem: 2473M\n[02/13 04:46:07 d2.utils.events]:  eta: 0:00:48  iter: 599  total_loss: 1.047  loss_cls: 0.3254  loss_box_reg: 0.3606  loss_mask: 0.2615  loss_rpn_cls: 0.03869  loss_rpn_loc: 0.03192    time: 0.1235  last_time: 0.1356  data_time: 0.0088  last_data_time: 0.0192   lr: 0.0011988  max_mem: 2473M\n[02/13 04:46:10 d2.utils.events]:  eta: 0:00:45  iter: 619  total_loss: 0.7698  loss_cls: 0.2613  loss_box_reg: 0.2978  loss_mask: 0.2185  loss_rpn_cls: 0.02539  loss_rpn_loc: 0.03508    time: 0.1241  last_time: 0.1243  data_time: 0.0130  last_data_time: 0.0107   lr: 0.0012388  max_mem: 2473M\n[02/13 04:46:13 d2.utils.events]:  eta: 0:00:43  iter: 639  total_loss: 1.112  loss_cls: 0.353  loss_box_reg: 0.3621  loss_mask: 0.2437  loss_rpn_cls: 0.02337  loss_rpn_loc: 0.05272    time: 0.1247  last_time: 0.1451  data_time: 0.0151  last_data_time: 0.0145   lr: 0.0012787  max_mem: 2473M\n[02/13 04:46:16 d2.utils.events]:  eta: 0:00:41  iter: 659  total_loss: 1.291  loss_cls: 0.3047  loss_box_reg: 0.3817  loss_mask: 0.2762  loss_rpn_cls: 0.03335  loss_rpn_loc: 0.05792    time: 0.1257  last_time: 0.1471  data_time: 0.0157  last_data_time: 0.0076   lr: 0.0013187  max_mem: 2473M\n[02/13 04:46:20 d2.utils.events]:  eta: 0:00:39  iter: 679  total_loss: 1.081  loss_cls: 0.3433  loss_box_reg: 0.3318  loss_mask: 0.2444  loss_rpn_cls: 0.05075  loss_rpn_loc: 0.04307    time: 0.1268  last_time: 0.1964  data_time: 0.0254  last_data_time: 0.0112   lr: 0.0013586  max_mem: 2473M\n[02/13 04:46:23 d2.utils.events]:  eta: 0:00:36  iter: 699  total_loss: 0.9815  loss_cls: 0.2452  loss_box_reg: 0.3903  loss_mask: 0.2241  loss_rpn_cls: 0.02741  loss_rpn_loc: 0.04214    time: 0.1278  last_time: 0.1553  data_time: 0.0106  last_data_time: 0.0135   lr: 0.0013986  max_mem: 2473M\n[02/13 04:46:26 d2.utils.events]:  eta: 0:00:34  iter: 719  total_loss: 1.041  loss_cls: 0.3232  loss_box_reg: 0.3421  loss_mask: 0.2496  loss_rpn_cls: 0.0315  loss_rpn_loc: 0.03683    time: 0.1282  last_time: 0.1179  data_time: 0.0103  last_data_time: 0.0061   lr: 0.0014386  max_mem: 2473M\n[02/13 04:46:28 d2.utils.events]:  eta: 0:00:31  iter: 739  total_loss: 1.335  loss_cls: 0.4221  loss_box_reg: 0.433  loss_mask: 0.2776  loss_rpn_cls: 0.03746  loss_rpn_loc: 0.06529    time: 0.1283  last_time: 0.1676  data_time: 0.0087  last_data_time: 0.0276   lr: 0.0014785  max_mem: 2473M\n[02/13 04:46:31 d2.utils.events]:  eta: 0:00:29  iter: 759  total_loss: 1.054  loss_cls: 0.3465  loss_box_reg: 0.3453  loss_mask: 0.2371  loss_rpn_cls: 0.03524  loss_rpn_loc: 0.06073    time: 0.1286  last_time: 0.1249  data_time: 0.0068  last_data_time: 0.0056   lr: 0.0015185  max_mem: 2473M\n[02/13 04:46:34 d2.utils.events]:  eta: 0:00:27  iter: 779  total_loss: 1.288  loss_cls: 0.4118  loss_box_reg: 0.4728  loss_mask: 0.2628  loss_rpn_cls: 0.03728  loss_rpn_loc: 0.06528    time: 0.1285  last_time: 0.1223  data_time: 0.0067  last_data_time: 0.0134   lr: 0.0015584  max_mem: 2473M\n[02/13 04:46:36 d2.utils.events]:  eta: 0:00:24  iter: 799  total_loss: 1.276  loss_cls: 0.3934  loss_box_reg: 0.4404  loss_mask: 0.2486  loss_rpn_cls: 0.03664  loss_rpn_loc: 0.06733    time: 0.1285  last_time: 0.1452  data_time: 0.0106  last_data_time: 0.0204   lr: 0.0015984  max_mem: 2473M\n[02/13 04:46:39 d2.utils.events]:  eta: 0:00:22  iter: 819  total_loss: 1.046  loss_cls: 0.3259  loss_box_reg: 0.3935  loss_mask: 0.2525  loss_rpn_cls: 0.04915  loss_rpn_loc: 0.04426    time: 0.1283  last_time: 0.1370  data_time: 0.0093  last_data_time: 0.0082   lr: 0.0016384  max_mem: 2473M\n[02/13 04:46:41 d2.utils.events]:  eta: 0:00:19  iter: 839  total_loss: 1.168  loss_cls: 0.3326  loss_box_reg: 0.4265  loss_mask: 0.2727  loss_rpn_cls: 0.0369  loss_rpn_loc: 0.09471    time: 0.1282  last_time: 0.1198  data_time: 0.0080  last_data_time: 0.0050   lr: 0.0016783  max_mem: 2473M\n[02/13 04:46:44 d2.utils.events]:  eta: 0:00:17  iter: 859  total_loss: 1.234  loss_cls: 0.3414  loss_box_reg: 0.4458  loss_mask: 0.2656  loss_rpn_cls: 0.06422  loss_rpn_loc: 0.08606    time: 0.1280  last_time: 0.1110  data_time: 0.0063  last_data_time: 0.0099   lr: 0.0017183  max_mem: 2473M\n[02/13 04:46:46 d2.utils.events]:  eta: 0:00:14  iter: 879  total_loss: 1.186  loss_cls: 0.3201  loss_box_reg: 0.402  loss_mask: 0.2859  loss_rpn_cls: 0.03309  loss_rpn_loc: 0.03605    time: 0.1279  last_time: 0.1252  data_time: 0.0071  last_data_time: 0.0053   lr: 0.0017582  max_mem: 2473M\n[02/13 04:46:49 d2.utils.events]:  eta: 0:00:12  iter: 899  total_loss: 1.131  loss_cls: 0.3438  loss_box_reg: 0.3714  loss_mask: 0.2574  loss_rpn_cls: 0.02271  loss_rpn_loc: 0.04514    time: 0.1278  last_time: 0.1091  data_time: 0.0055  last_data_time: 0.0058   lr: 0.0017982  max_mem: 2473M\n[02/13 04:46:51 d2.utils.events]:  eta: 0:00:09  iter: 919  total_loss: 1.369  loss_cls: 0.3949  loss_box_reg: 0.4476  loss_mask: 0.2914  loss_rpn_cls: 0.04681  loss_rpn_loc: 0.09672    time: 0.1275  last_time: 0.1167  data_time: 0.0074  last_data_time: 0.0101   lr: 0.0018382  max_mem: 2473M\n[02/13 04:46:53 d2.utils.events]:  eta: 0:00:07  iter: 939  total_loss: 1.247  loss_cls: 0.3892  loss_box_reg: 0.4228  loss_mask: 0.2499  loss_rpn_cls: 0.05528  loss_rpn_loc: 0.04001    time: 0.1272  last_time: 0.0998  data_time: 0.0078  last_data_time: 0.0034   lr: 0.0018781  max_mem: 2473M\n[02/13 04:46:56 d2.utils.events]:  eta: 0:00:04  iter: 959  total_loss: 1.189  loss_cls: 0.4043  loss_box_reg: 0.4079  loss_mask: 0.2503  loss_rpn_cls: 0.03474  loss_rpn_loc: 0.06123    time: 0.1271  last_time: 0.1097  data_time: 0.0067  last_data_time: 0.0051   lr: 0.0019181  max_mem: 2473M\n[02/13 04:46:58 d2.utils.events]:  eta: 0:00:02  iter: 979  total_loss: 1.276  loss_cls: 0.3649  loss_box_reg: 0.4229  loss_mask: 0.2671  loss_rpn_cls: 0.0651  loss_rpn_loc: 0.1141    time: 0.1269  last_time: 0.1131  data_time: 0.0070  last_data_time: 0.0059   lr: 0.001958  max_mem: 2473M\n[02/13 04:47:01 d2.utils.events]:  eta: 0:00:00  iter: 999  total_loss: 1.171  loss_cls: 0.4161  loss_box_reg: 0.4521  loss_mask: 0.2544  loss_rpn_cls: 0.04434  loss_rpn_loc: 0.03685    time: 0.1266  last_time: 0.1066  data_time: 0.0076  last_data_time: 0.0030   lr: 0.001998  max_mem: 2473M\n[02/13 04:47:01 d2.engine.hooks]: Overall training speed: 998 iterations in 0:02:06 (0.1266 s / it)\n[02/13 04:47:01 d2.engine.hooks]: Total training time: 0:02:08 (0:00:01 on hooks)\n[02/13 04:47:02 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.06 seconds.\n[02/13 04:47:03 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:47:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n[02/13 04:47:03 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:47:03 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...\n[02/13 04:47:04 d2.data.common]: Serialized dataset takes 19.22 MiB\nWARNING [02/13 04:47:04 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n\n\n\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\npredictor = DefaultPredictor(cfg)\n\nevaluator = COCOEvaluator(\"coco_val_data\", output_dir = \"./output\")\nval_loader = build_detection_test_loader(cfg, \"coco_val_data\")\ninference_on_dataset(trainer.model, val_loader, evaluator)\n\n[02/13 04:51:01 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /root/2024winter/DL_tutorial/posts/outputs/model_final.pth ...\n[02/13 04:51:03 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:51:04 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n[02/13 04:51:04 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:51:04 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...\n[02/13 04:51:04 d2.data.common]: Serialized dataset takes 19.22 MiB\n[02/13 04:51:04 d2.evaluation.evaluator]: Start inference on 5000 batches\n[02/13 04:51:05 d2.evaluation.evaluator]: Inference done 11/5000. Dataloading: 0.0014 s/iter. Inference: 0.0588 s/iter. Eval: 0.0347 s/iter. Total: 0.0949 s/iter. ETA=0:07:53\n[02/13 04:51:10 d2.evaluation.evaluator]: Inference done 69/5000. Dataloading: 0.0018 s/iter. Inference: 0.0511 s/iter. Eval: 0.0343 s/iter. Total: 0.0874 s/iter. ETA=0:07:10\n[02/13 04:51:15 d2.evaluation.evaluator]: Inference done 124/5000. Dataloading: 0.0019 s/iter. Inference: 0.0505 s/iter. Eval: 0.0371 s/iter. Total: 0.0896 s/iter. ETA=0:07:16\n[02/13 04:51:20 d2.evaluation.evaluator]: Inference done 184/5000. Dataloading: 0.0019 s/iter. Inference: 0.0480 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:07:01\n[02/13 04:51:25 d2.evaluation.evaluator]: Inference done 247/5000. Dataloading: 0.0018 s/iter. Inference: 0.0473 s/iter. Eval: 0.0362 s/iter. Total: 0.0854 s/iter. ETA=0:06:45\n[02/13 04:51:31 d2.evaluation.evaluator]: Inference done 312/5000. Dataloading: 0.0018 s/iter. Inference: 0.0457 s/iter. Eval: 0.0363 s/iter. Total: 0.0838 s/iter. ETA=0:06:32\n[02/13 04:51:36 d2.evaluation.evaluator]: Inference done 371/5000. Dataloading: 0.0019 s/iter. Inference: 0.0460 s/iter. Eval: 0.0361 s/iter. Total: 0.0841 s/iter. ETA=0:06:29\n[02/13 04:51:41 d2.evaluation.evaluator]: Inference done 431/5000. Dataloading: 0.0019 s/iter. Inference: 0.0464 s/iter. Eval: 0.0356 s/iter. Total: 0.0840 s/iter. ETA=0:06:23\n[02/13 04:51:46 d2.evaluation.evaluator]: Inference done 489/5000. Dataloading: 0.0019 s/iter. Inference: 0.0471 s/iter. Eval: 0.0352 s/iter. Total: 0.0843 s/iter. ETA=0:06:20\n[02/13 04:51:51 d2.evaluation.evaluator]: Inference done 539/5000. Dataloading: 0.0019 s/iter. Inference: 0.0476 s/iter. Eval: 0.0363 s/iter. Total: 0.0860 s/iter. ETA=0:06:23\n[02/13 04:51:56 d2.evaluation.evaluator]: Inference done 596/5000. Dataloading: 0.0020 s/iter. Inference: 0.0476 s/iter. Eval: 0.0365 s/iter. Total: 0.0862 s/iter. ETA=0:06:19\n[02/13 04:52:01 d2.evaluation.evaluator]: Inference done 653/5000. Dataloading: 0.0020 s/iter. Inference: 0.0475 s/iter. Eval: 0.0368 s/iter. Total: 0.0863 s/iter. ETA=0:06:15\n[02/13 04:52:06 d2.evaluation.evaluator]: Inference done 713/5000. Dataloading: 0.0020 s/iter. Inference: 0.0474 s/iter. Eval: 0.0367 s/iter. Total: 0.0862 s/iter. ETA=0:06:09\n[02/13 04:52:11 d2.evaluation.evaluator]: Inference done 777/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0365 s/iter. Total: 0.0856 s/iter. ETA=0:06:01\n[02/13 04:52:16 d2.evaluation.evaluator]: Inference done 835/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0365 s/iter. Total: 0.0856 s/iter. ETA=0:05:56\n[02/13 04:52:21 d2.evaluation.evaluator]: Inference done 895/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0364 s/iter. Total: 0.0856 s/iter. ETA=0:05:51\n[02/13 04:52:26 d2.evaluation.evaluator]: Inference done 953/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0364 s/iter. Total: 0.0856 s/iter. ETA=0:05:46\n[02/13 04:52:31 d2.evaluation.evaluator]: Inference done 1015/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0362 s/iter. Total: 0.0853 s/iter. ETA=0:05:40\n[02/13 04:52:36 d2.evaluation.evaluator]: Inference done 1074/5000. Dataloading: 0.0020 s/iter. Inference: 0.0469 s/iter. Eval: 0.0363 s/iter. Total: 0.0853 s/iter. ETA=0:05:35\n[02/13 04:52:41 d2.evaluation.evaluator]: Inference done 1133/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0362 s/iter. Total: 0.0853 s/iter. ETA=0:05:29\n[02/13 04:52:46 d2.evaluation.evaluator]: Inference done 1193/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0361 s/iter. Total: 0.0853 s/iter. ETA=0:05:24\n[02/13 04:52:51 d2.evaluation.evaluator]: Inference done 1253/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0363 s/iter. Total: 0.0852 s/iter. ETA=0:05:19\n[02/13 04:52:56 d2.evaluation.evaluator]: Inference done 1313/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0362 s/iter. Total: 0.0852 s/iter. ETA=0:05:14\n[02/13 04:53:01 d2.evaluation.evaluator]: Inference done 1365/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0367 s/iter. Total: 0.0856 s/iter. ETA=0:05:11\n[02/13 04:53:06 d2.evaluation.evaluator]: Inference done 1427/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:05:05\n[02/13 04:53:11 d2.evaluation.evaluator]: Inference done 1487/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0366 s/iter. Total: 0.0854 s/iter. ETA=0:04:59\n[02/13 04:53:16 d2.evaluation.evaluator]: Inference done 1547/5000. Dataloading: 0.0020 s/iter. Inference: 0.0466 s/iter. Eval: 0.0366 s/iter. Total: 0.0854 s/iter. ETA=0:04:54\n[02/13 04:53:21 d2.evaluation.evaluator]: Inference done 1605/5000. Dataloading: 0.0020 s/iter. Inference: 0.0466 s/iter. Eval: 0.0367 s/iter. Total: 0.0854 s/iter. ETA=0:04:49\n[02/13 04:53:26 d2.evaluation.evaluator]: Inference done 1662/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:04:45\n[02/13 04:53:32 d2.evaluation.evaluator]: Inference done 1719/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0367 s/iter. Total: 0.0856 s/iter. ETA=0:04:40\n[02/13 04:53:37 d2.evaluation.evaluator]: Inference done 1781/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:04:35\n[02/13 04:53:42 d2.evaluation.evaluator]: Inference done 1844/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0365 s/iter. Total: 0.0853 s/iter. ETA=0:04:29\n[02/13 04:53:47 d2.evaluation.evaluator]: Inference done 1906/5000. Dataloading: 0.0020 s/iter. Inference: 0.0465 s/iter. Eval: 0.0364 s/iter. Total: 0.0851 s/iter. ETA=0:04:23\n[02/13 04:53:52 d2.evaluation.evaluator]: Inference done 1969/5000. Dataloading: 0.0020 s/iter. Inference: 0.0465 s/iter. Eval: 0.0363 s/iter. Total: 0.0850 s/iter. ETA=0:04:17\n[02/13 04:53:57 d2.evaluation.evaluator]: Inference done 2031/5000. Dataloading: 0.0020 s/iter. Inference: 0.0463 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:11\n[02/13 04:54:02 d2.evaluation.evaluator]: Inference done 2090/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:07\n[02/13 04:54:07 d2.evaluation.evaluator]: Inference done 2149/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:02\n[02/13 04:54:12 d2.evaluation.evaluator]: Inference done 2207/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0365 s/iter. Total: 0.0850 s/iter. ETA=0:03:57\n[02/13 04:54:17 d2.evaluation.evaluator]: Inference done 2265/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0365 s/iter. Total: 0.0850 s/iter. ETA=0:03:52\n[02/13 04:54:22 d2.evaluation.evaluator]: Inference done 2326/5000. Dataloading: 0.0021 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:03:47\n[02/13 04:54:27 d2.evaluation.evaluator]: Inference done 2382/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0366 s/iter. Total: 0.0851 s/iter. ETA=0:03:42\n[02/13 04:54:32 d2.evaluation.evaluator]: Inference done 2440/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0367 s/iter. Total: 0.0851 s/iter. ETA=0:03:37\n[02/13 04:54:37 d2.evaluation.evaluator]: Inference done 2498/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0367 s/iter. Total: 0.0851 s/iter. ETA=0:03:33\n[02/13 04:54:42 d2.evaluation.evaluator]: Inference done 2546/5000. Dataloading: 0.0021 s/iter. Inference: 0.0465 s/iter. Eval: 0.0368 s/iter. Total: 0.0855 s/iter. ETA=0:03:29\n[02/13 04:54:47 d2.evaluation.evaluator]: Inference done 2589/5000. Dataloading: 0.0021 s/iter. Inference: 0.0467 s/iter. Eval: 0.0371 s/iter. Total: 0.0861 s/iter. ETA=0:03:27\n[02/13 04:54:52 d2.evaluation.evaluator]: Inference done 2628/5000. Dataloading: 0.0021 s/iter. Inference: 0.0471 s/iter. Eval: 0.0374 s/iter. Total: 0.0867 s/iter. ETA=0:03:25\n[02/13 04:54:57 d2.evaluation.evaluator]: Inference done 2666/5000. Dataloading: 0.0021 s/iter. Inference: 0.0475 s/iter. Eval: 0.0377 s/iter. Total: 0.0874 s/iter. ETA=0:03:24\n[02/13 04:55:03 d2.evaluation.evaluator]: Inference done 2720/5000. Dataloading: 0.0022 s/iter. Inference: 0.0477 s/iter. Eval: 0.0376 s/iter. Total: 0.0876 s/iter. ETA=0:03:19\n[02/13 04:55:08 d2.evaluation.evaluator]: Inference done 2778/5000. Dataloading: 0.0021 s/iter. Inference: 0.0478 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:03:14\n[02/13 04:55:13 d2.evaluation.evaluator]: Inference done 2836/5000. Dataloading: 0.0021 s/iter. Inference: 0.0478 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:03:09\n[02/13 04:55:18 d2.evaluation.evaluator]: Inference done 2892/5000. Dataloading: 0.0021 s/iter. Inference: 0.0480 s/iter. Eval: 0.0373 s/iter. Total: 0.0876 s/iter. ETA=0:03:04\n[02/13 04:55:23 d2.evaluation.evaluator]: Inference done 2948/5000. Dataloading: 0.0021 s/iter. Inference: 0.0481 s/iter. Eval: 0.0373 s/iter. Total: 0.0876 s/iter. ETA=0:02:59\n[02/13 04:55:28 d2.evaluation.evaluator]: Inference done 3007/5000. Dataloading: 0.0021 s/iter. Inference: 0.0482 s/iter. Eval: 0.0371 s/iter. Total: 0.0876 s/iter. ETA=0:02:54\n[02/13 04:55:33 d2.evaluation.evaluator]: Inference done 3065/5000. Dataloading: 0.0021 s/iter. Inference: 0.0483 s/iter. Eval: 0.0370 s/iter. Total: 0.0876 s/iter. ETA=0:02:49\n[02/13 04:55:38 d2.evaluation.evaluator]: Inference done 3125/5000. Dataloading: 0.0021 s/iter. Inference: 0.0484 s/iter. Eval: 0.0369 s/iter. Total: 0.0875 s/iter. ETA=0:02:44\n[02/13 04:55:43 d2.evaluation.evaluator]: Inference done 3185/5000. Dataloading: 0.0021 s/iter. Inference: 0.0484 s/iter. Eval: 0.0368 s/iter. Total: 0.0875 s/iter. ETA=0:02:38\n[02/13 04:55:48 d2.evaluation.evaluator]: Inference done 3242/5000. Dataloading: 0.0021 s/iter. Inference: 0.0485 s/iter. Eval: 0.0368 s/iter. Total: 0.0875 s/iter. ETA=0:02:33\n[02/13 04:55:53 d2.evaluation.evaluator]: Inference done 3299/5000. Dataloading: 0.0021 s/iter. Inference: 0.0485 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:28\n[02/13 04:55:58 d2.evaluation.evaluator]: Inference done 3355/5000. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:23\n[02/13 04:56:03 d2.evaluation.evaluator]: Inference done 3413/5000. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:18\n[02/13 04:56:08 d2.evaluation.evaluator]: Inference done 3474/5000. Dataloading: 0.0021 s/iter. Inference: 0.0487 s/iter. Eval: 0.0366 s/iter. Total: 0.0874 s/iter. ETA=0:02:13\n[02/13 04:56:13 d2.evaluation.evaluator]: Inference done 3535/5000. Dataloading: 0.0021 s/iter. Inference: 0.0487 s/iter. Eval: 0.0365 s/iter. Total: 0.0874 s/iter. ETA=0:02:07\n[02/13 04:56:18 d2.evaluation.evaluator]: Inference done 3582/5000. Dataloading: 0.0021 s/iter. Inference: 0.0488 s/iter. Eval: 0.0366 s/iter. Total: 0.0876 s/iter. ETA=0:02:04\n[02/13 04:56:23 d2.evaluation.evaluator]: Inference done 3642/5000. Dataloading: 0.0021 s/iter. Inference: 0.0488 s/iter. Eval: 0.0366 s/iter. Total: 0.0876 s/iter. ETA=0:01:58\n[02/13 04:56:28 d2.evaluation.evaluator]: Inference done 3699/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0365 s/iter. Total: 0.0876 s/iter. ETA=0:01:53\n[02/13 04:56:33 d2.evaluation.evaluator]: Inference done 3760/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0364 s/iter. Total: 0.0875 s/iter. ETA=0:01:48\n[02/13 04:56:38 d2.evaluation.evaluator]: Inference done 3820/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0364 s/iter. Total: 0.0875 s/iter. ETA=0:01:43\n[02/13 04:56:43 d2.evaluation.evaluator]: Inference done 3885/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0362 s/iter. Total: 0.0873 s/iter. ETA=0:01:37\n[02/13 04:56:49 d2.evaluation.evaluator]: Inference done 3947/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:31\n[02/13 04:56:54 d2.evaluation.evaluator]: Inference done 4003/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:26\n[02/13 04:56:59 d2.evaluation.evaluator]: Inference done 4058/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0362 s/iter. Total: 0.0873 s/iter. ETA=0:01:22\n[02/13 04:57:04 d2.evaluation.evaluator]: Inference done 4121/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:16\n[02/13 04:57:09 d2.evaluation.evaluator]: Inference done 4178/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:11\n[02/13 04:57:14 d2.evaluation.evaluator]: Inference done 4238/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0360 s/iter. Total: 0.0872 s/iter. ETA=0:01:06\n[02/13 04:57:19 d2.evaluation.evaluator]: Inference done 4299/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0360 s/iter. Total: 0.0871 s/iter. ETA=0:01:01\n[02/13 04:57:24 d2.evaluation.evaluator]: Inference done 4357/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0359 s/iter. Total: 0.0871 s/iter. ETA=0:00:56\n[02/13 04:57:29 d2.evaluation.evaluator]: Inference done 4417/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0359 s/iter. Total: 0.0871 s/iter. ETA=0:00:50\n[02/13 04:57:34 d2.evaluation.evaluator]: Inference done 4475/5000. Dataloading: 0.0021 s/iter. Inference: 0.0491 s/iter. Eval: 0.0358 s/iter. Total: 0.0871 s/iter. ETA=0:00:45\n[02/13 04:57:39 d2.evaluation.evaluator]: Inference done 4535/5000. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0357 s/iter. Total: 0.0870 s/iter. ETA=0:00:40\n[02/13 04:57:44 d2.evaluation.evaluator]: Inference done 4596/5000. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0357 s/iter. Total: 0.0870 s/iter. ETA=0:00:35\n[02/13 04:57:49 d2.evaluation.evaluator]: Inference done 4647/5000. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0358 s/iter. Total: 0.0871 s/iter. ETA=0:00:30\n[02/13 04:57:54 d2.evaluation.evaluator]: Inference done 4681/5000. Dataloading: 0.0021 s/iter. Inference: 0.0494 s/iter. Eval: 0.0360 s/iter. Total: 0.0875 s/iter. ETA=0:00:27\n[02/13 04:57:59 d2.evaluation.evaluator]: Inference done 4720/5000. Dataloading: 0.0021 s/iter. Inference: 0.0496 s/iter. Eval: 0.0361 s/iter. Total: 0.0879 s/iter. ETA=0:00:24\n[02/13 04:58:04 d2.evaluation.evaluator]: Inference done 4767/5000. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0362 s/iter. Total: 0.0881 s/iter. ETA=0:00:20\n[02/13 04:58:09 d2.evaluation.evaluator]: Inference done 4824/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0362 s/iter. Total: 0.0881 s/iter. ETA=0:00:15\n[02/13 04:58:14 d2.evaluation.evaluator]: Inference done 4881/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0361 s/iter. Total: 0.0881 s/iter. ETA=0:00:10\n[02/13 04:58:19 d2.evaluation.evaluator]: Inference done 4935/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0362 s/iter. Total: 0.0882 s/iter. ETA=0:00:05\n[02/13 04:58:25 d2.evaluation.evaluator]: Inference done 4992/5000. Dataloading: 0.0021 s/iter. Inference: 0.0499 s/iter. Eval: 0.0361 s/iter. Total: 0.0882 s/iter. ETA=0:00:00\n[02/13 04:58:25 d2.evaluation.evaluator]: Total inference time: 0:07:20.479637 (0.088184 s / iter per device, on 1 devices)\n[02/13 04:58:25 d2.evaluation.evaluator]: Total inference pure compute time: 0:04:09 (0.049895 s / iter per device, on 1 devices)\n[02/13 04:58:30 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n[02/13 04:58:30 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n[02/13 04:58:33 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\nLoading and preparing results...\nDONE (t=0.40s)\ncreating index...\nindex created!\n[02/13 04:58:34 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n[02/13 04:58:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 11.37 seconds.\n[02/13 04:58:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n[02/13 04:58:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 1.54 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.498\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.318\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.360\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.273\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.306\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.499\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.569\n[02/13 04:58:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 29.897 | 49.818 | 31.757 | 19.091 | 33.007 | 35.992 |\n[02/13 04:58:47 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n| category      | AP     | category     | AP     | category       | AP     |\n|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n| person        | 47.838 | bicycle      | 23.478 | car            | 35.332 |\n| motorcycle    | 32.072 | airplane     | 41.206 | bus            | 51.420 |\n| train         | 49.491 | truck        | 26.024 | boat           | 18.960 |\n| traffic light | 19.952 | fire hydrant | 59.531 | stop sign      | 54.634 |\n| parking meter | 34.804 | bench        | 17.211 | bird           | 24.288 |\n| cat           | 40.017 | dog          | 36.215 | horse          | 41.811 |\n| sheep         | 24.404 | cow          | 38.517 | elephant       | 47.882 |\n| bear          | 43.047 | zebra        | 55.295 | giraffe        | 53.434 |\n| backpack      | 4.107  | umbrella     | 25.139 | handbag        | 9.306  |\n| tie           | 27.322 | suitcase     | 21.393 | frisbee        | 55.385 |\n| skis          | 18.424 | snowboard    | 27.931 | sports ball    | 40.706 |\n| kite          | 30.145 | baseball bat | 21.415 | baseball glove | 24.885 |\n| skateboard    | 35.433 | surfboard    | 24.567 | tennis racket  | 35.924 |\n| bottle        | 32.441 | wine glass   | 29.303 | cup            | 34.689 |\n| fork          | 17.648 | knife        | 6.489  | spoon          | 11.327 |\n| bowl          | 32.606 | banana       | 14.851 | apple          | 14.518 |\n| sandwich      | 21.648 | orange       | 23.322 | broccoli       | 17.270 |\n| carrot        | 17.806 | hot dog      | 21.116 | pizza          | 41.106 |\n| donut         | 37.720 | cake         | 22.721 | chair          | 19.240 |\n| couch         | 28.865 | potted plant | 22.025 | bed            | 26.000 |\n| dining table  | 19.767 | toilet       | 47.664 | tv             | 39.008 |\n| laptop        | 42.352 | mouse        | 53.223 | remote         | 16.222 |\n| keyboard      | 36.314 | cell phone   | 20.834 | microwave      | 42.930 |\n| oven          | 22.920 | toaster      | 32.372 | sink           | 29.007 |\n| refrigerator  | 39.891 | book         | 8.055  | clock          | 40.063 |\n| vase          | 31.166 | scissors     | 11.895 | teddy bear     | 29.199 |\n| hair drier    | 2.024  | toothbrush   | 15.190 |                |        |\nLoading and preparing results...\nDONE (t=4.31s)\ncreating index...\nindex created!\n[02/13 04:59:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n[02/13 04:59:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 14.30 seconds.\n[02/13 04:59:16 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n[02/13 04:59:17 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 1.34 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.471\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.303\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.144\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.312\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.267\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.481\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n[02/13 04:59:18 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 28.662 | 47.121 | 30.279 | 14.394 | 31.241 | 39.792 |\n[02/13 04:59:18 d2.evaluation.coco_evaluation]: Per-category segm AP: \n| category      | AP     | category     | AP     | category       | AP     |\n|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n| person        | 41.252 | bicycle      | 11.980 | car            | 33.768 |\n| motorcycle    | 25.730 | airplane     | 35.403 | bus            | 53.283 |\n| train         | 52.598 | truck        | 27.219 | boat           | 17.898 |\n| traffic light | 20.063 | fire hydrant | 58.757 | stop sign      | 56.504 |\n| parking meter | 39.879 | bench        | 13.094 | bird           | 18.633 |\n| cat           | 44.937 | dog          | 39.134 | horse          | 32.426 |\n| sheep         | 22.079 | cow          | 34.962 | elephant       | 43.845 |\n| bear          | 43.428 | zebra        | 50.255 | giraffe        | 42.791 |\n| backpack      | 3.070  | umbrella     | 34.073 | handbag        | 8.804  |\n| tie           | 25.973 | suitcase     | 23.217 | frisbee        | 55.818 |\n| skis          | 1.953  | snowboard    | 16.213 | sports ball    | 41.947 |\n| kite          | 22.435 | baseball bat | 21.437 | baseball glove | 29.435 |\n| skateboard    | 20.624 | surfboard    | 20.775 | tennis racket  | 47.517 |\n| bottle        | 33.193 | wine glass   | 27.668 | cup            | 35.272 |\n| fork          | 5.703  | knife        | 2.179  | spoon          | 8.625  |\n| bowl          | 32.855 | banana       | 11.942 | apple          | 15.228 |\n| sandwich      | 25.858 | orange       | 24.107 | broccoli       | 17.538 |\n| carrot        | 16.124 | hot dog      | 21.730 | pizza          | 41.892 |\n| donut         | 43.238 | cake         | 24.126 | chair          | 11.464 |\n| couch         | 25.759 | potted plant | 20.276 | bed            | 20.477 |\n| dining table  | 11.458 | toilet       | 49.441 | tv             | 43.849 |\n| laptop        | 43.364 | mouse        | 53.676 | remote         | 17.722 |\n| keyboard      | 39.225 | cell phone   | 26.123 | microwave      | 47.567 |\n| oven          | 23.584 | toaster      | 37.491 | sink           | 29.526 |\n| refrigerator  | 40.803 | book         | 6.693  | clock          | 43.155 |\n| vase          | 30.156 | scissors     | 10.523 | teddy bear     | 30.139 |\n| hair drier    | 0.187  | toothbrush   | 9.801  |                |        |\n\n\nOrderedDict([('bbox',\n              {'AP': 29.89689017794718,\n               'AP50': 49.81768645224712,\n               'AP75': 31.75684033977177,\n               'APs': 19.091146863259823,\n               'APm': 33.00671209183804,\n               'APl': 35.99166344401009,\n               'AP-person': 47.837560337143984,\n               'AP-bicycle': 23.47814560773066,\n               'AP-car': 35.332145066511,\n               'AP-motorcycle': 32.07172272587435,\n               'AP-airplane': 41.20588239061185,\n               'AP-bus': 51.420009438029666,\n               'AP-train': 49.49114357900364,\n               'AP-truck': 26.023794935388324,\n               'AP-boat': 18.9602461116214,\n               'AP-traffic light': 19.9521983797877,\n               'AP-fire hydrant': 59.53117091427524,\n               'AP-stop sign': 54.63431579194609,\n               'AP-parking meter': 34.803858333145854,\n               'AP-bench': 17.210808241735307,\n               'AP-bird': 24.287728734734667,\n               'AP-cat': 40.01669713689599,\n               'AP-dog': 36.21485337000998,\n               'AP-horse': 41.811047625435855,\n               'AP-sheep': 24.40385172034866,\n               'AP-cow': 38.51727158084108,\n               'AP-elephant': 47.882326630736515,\n               'AP-bear': 43.046572215754935,\n               'AP-zebra': 55.29518540456856,\n               'AP-giraffe': 53.43429768524197,\n               'AP-backpack': 4.10653831430483,\n               'AP-umbrella': 25.13944697585527,\n               'AP-handbag': 9.306254822354724,\n               'AP-tie': 27.321944146697817,\n               'AP-suitcase': 21.393099020425794,\n               'AP-frisbee': 55.38519299029526,\n               'AP-skis': 18.424028799910637,\n               'AP-snowboard': 27.93062121315892,\n               'AP-sports ball': 40.70572939438888,\n               'AP-kite': 30.1448409925177,\n               'AP-baseball bat': 21.41505682963772,\n               'AP-baseball glove': 24.88477652822228,\n               'AP-skateboard': 35.43250731045371,\n               'AP-surfboard': 24.56681960929748,\n               'AP-tennis racket': 35.924037712245145,\n               'AP-bottle': 32.440957179199856,\n               'AP-wine glass': 29.30315839934765,\n               'AP-cup': 34.688662676350305,\n               'AP-fork': 17.648460196785454,\n               'AP-knife': 6.489014035767637,\n               'AP-spoon': 11.326758827226104,\n               'AP-bowl': 32.60595516623553,\n               'AP-banana': 14.850983715257158,\n               'AP-apple': 14.518068381401013,\n               'AP-sandwich': 21.648061518774306,\n               'AP-orange': 23.321788766204204,\n               'AP-broccoli': 17.27036935565236,\n               'AP-carrot': 17.805870075670093,\n               'AP-hot dog': 21.11634657823014,\n               'AP-pizza': 41.10561040833102,\n               'AP-donut': 37.71982162773581,\n               'AP-cake': 22.721214765344666,\n               'AP-chair': 19.24040698001655,\n               'AP-couch': 28.864502321656886,\n               'AP-potted plant': 22.02453853016209,\n               'AP-bed': 25.999786248222506,\n               'AP-dining table': 19.766840271606974,\n               'AP-toilet': 47.663609421333156,\n               'AP-tv': 39.00821630716829,\n               'AP-laptop': 42.351954633553596,\n               'AP-mouse': 53.2230527938069,\n               'AP-remote': 16.222228003183716,\n               'AP-keyboard': 36.31434769943729,\n               'AP-cell phone': 20.834159106237465,\n               'AP-microwave': 42.93029819699271,\n               'AP-oven': 22.920311025094627,\n               'AP-toaster': 32.37201100567437,\n               'AP-sink': 29.00693619350771,\n               'AP-refrigerator': 39.89130619477842,\n               'AP-book': 8.054941266635423,\n               'AP-clock': 40.06277695071099,\n               'AP-vase': 31.16619906385666,\n               'AP-scissors': 11.895045305984278,\n               'AP-teddy bear': 29.199265484243536,\n               'AP-hair drier': 2.023946580704582,\n               'AP-toothbrush': 15.189704360553186}),\n             ('segm',\n              {'AP': 28.661839929847766,\n               'AP50': 47.12099506785067,\n               'AP75': 30.27852102510463,\n               'APs': 14.393543998149433,\n               'APm': 31.240826986233245,\n               'APl': 39.79242278943046,\n               'AP-person': 41.25247349406813,\n               'AP-bicycle': 11.980056444623962,\n               'AP-car': 33.767649311682966,\n               'AP-motorcycle': 25.72961219232462,\n               'AP-airplane': 35.40335889956266,\n               'AP-bus': 53.28296316844562,\n               'AP-train': 52.59815704465161,\n               'AP-truck': 27.219112196412286,\n               'AP-boat': 17.898113630853008,\n               'AP-traffic light': 20.063050134999397,\n               'AP-fire hydrant': 58.75691495361581,\n               'AP-stop sign': 56.50424204130755,\n               'AP-parking meter': 39.87929817341212,\n               'AP-bench': 13.093799678463267,\n               'AP-bird': 18.633126014944484,\n               'AP-cat': 44.93671365428673,\n               'AP-dog': 39.134465086431405,\n               'AP-horse': 32.42593406670286,\n               'AP-sheep': 22.07929100248326,\n               'AP-cow': 34.96219015157876,\n               'AP-elephant': 43.84481472494,\n               'AP-bear': 43.42779378594565,\n               'AP-zebra': 50.254983961962374,\n               'AP-giraffe': 42.791213855527516,\n               'AP-backpack': 3.069830116972104,\n               'AP-umbrella': 34.07250440438077,\n               'AP-handbag': 8.8037834537288,\n               'AP-tie': 25.972833559829827,\n               'AP-suitcase': 23.21712969553343,\n               'AP-frisbee': 55.81774651489321,\n               'AP-skis': 1.9534711443298516,\n               'AP-snowboard': 16.212898925517894,\n               'AP-sports ball': 41.946609283399354,\n               'AP-kite': 22.434833850648612,\n               'AP-baseball bat': 21.437251665137875,\n               'AP-baseball glove': 29.434721623759575,\n               'AP-skateboard': 20.62420030530276,\n               'AP-surfboard': 20.7746697482192,\n               'AP-tennis racket': 47.5171436088261,\n               'AP-bottle': 33.193296640795076,\n               'AP-wine glass': 27.66844317937192,\n               'AP-cup': 35.27153103017214,\n               'AP-fork': 5.702777294880277,\n               'AP-knife': 2.1788346592089,\n               'AP-spoon': 8.624995301616934,\n               'AP-bowl': 32.85509874871778,\n               'AP-banana': 11.94177658970979,\n               'AP-apple': 15.22818936776687,\n               'AP-sandwich': 25.858064250410823,\n               'AP-orange': 24.107266334795867,\n               'AP-broccoli': 17.53776752525587,\n               'AP-carrot': 16.12362682209143,\n               'AP-hot dog': 21.730336517936593,\n               'AP-pizza': 41.89164234108817,\n               'AP-donut': 43.23758089806856,\n               'AP-cake': 24.12553215716846,\n               'AP-chair': 11.464367279360923,\n               'AP-couch': 25.75895204314586,\n               'AP-potted plant': 20.276217596489406,\n               'AP-bed': 20.476727012227187,\n               'AP-dining table': 11.45829001940977,\n               'AP-toilet': 49.44101117760728,\n               'AP-tv': 43.84883647370952,\n               'AP-laptop': 43.36437965344922,\n               'AP-mouse': 53.67643707607608,\n               'AP-remote': 17.722464443723158,\n               'AP-keyboard': 39.22521398632689,\n               'AP-cell phone': 26.122823527455523,\n               'AP-microwave': 47.56700008587836,\n               'AP-oven': 23.584145282510224,\n               'AP-toaster': 37.49139238248149,\n               'AP-sink': 29.526361068986912,\n               'AP-refrigerator': 40.80288598047604,\n               'AP-book': 6.693159472760312,\n               'AP-clock': 43.15466937561561,\n               'AP-vase': 30.156106300249856,\n               'AP-scissors': 10.522703248492874,\n               'AP-teddy bear': 30.139171881692267,\n               'AP-hair drier': 0.1872745414076291,\n               'AP-toothbrush': 9.80088924952792})])\n\n\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set the testing threshold for this model\ncfg.DATASETS.TEST = (\"coco_val_data\",)\npredictor = DefaultPredictor(cfg)\n\n[02/13 05:02:50 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /root/2024winter/DL_tutorial/posts/outputs/model_final.pth ...\n\n\n\nfrom detectron2.utils.visualizer import ColorMode\n\nfor d in random.sample(val_dataset_dicts, 3):\n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                  metadata=val_metadata,\n                  scale=0.8,\n                  instance_mode=ColorMode.IMAGE_BW) # remove the colors of unsegmented pixels\nv = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.figure(figsize=(5, 5))\nplt.imshow(vis.get_image()[:, :, ::-1])\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\n\n!unzip /root/2024winter/DL_tutorial/posts/instance_custom_annotation.zip -d /root/2024winter/DL_tutorial/posts/coco\n\nArchive:  /root/2024winter/DL_tutorial/posts/instance_custom_annotation.zip\n  inflating: /root/2024winter/DL_tutorial/posts/coco/__MACOSX/._annotations  \n  inflating: /root/2024winter/DL_tutorial/posts/coco/annotations/instances_custom.json  \n  inflating: /root/2024winter/DL_tutorial/posts/coco/__MACOSX/annotations/._instances_custom.json"
  },
  {
    "objectID": "posts/CV_Instance_Segmentation.html#instance-segmentation",
    "href": "posts/CV_Instance_Segmentation.html#instance-segmentation",
    "title": "CV_Instance_Segmentation_0",
    "section": "",
    "text": "#!pip install git+https://github.com/facebookresearch/fvcore.git\n#!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n#!pip install -e detectron2_repo\n\n\n\n\n\n!rm -r coco\n\nrm: cannot remove 'coco': No such file or directory\n\n\n\n!wget http://images.cocodataset.org/zips/train2017.zip\n!wget http://images.cocodataset.org/zips/val2017.zip\n!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n\n--2024-02-13 02:07:06--  http://images.cocodataset.org/zips/train2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 16.182.33.73, 16.182.70.25, 52.217.104.196, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|16.182.33.73|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 19336861798 (18G) [application/zip]\nSaving to: ‘train2017.zip’\n\ntrain2017.zip       100%[===================&gt;]  18.01G  14.6MB/s    in 21m 15s \n\n2024-02-13 02:28:21 (14.5 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n\n--2024-02-13 02:28:21--  http://images.cocodataset.org/zips/val2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 52.217.134.73, 54.231.199.1, 54.231.235.145, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|52.217.134.73|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 815585330 (778M) [application/zip]\nSaving to: ‘val2017.zip’\n\nval2017.zip         100%[===================&gt;] 777.80M  8.26MB/s    in 89s     \n\n2024-02-13 02:29:51 (8.70 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n\n--2024-02-13 02:29:51--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\nResolving images.cocodataset.org (images.cocodataset.org)... 52.216.51.129, 52.216.213.169, 3.5.10.168, ...\nConnecting to images.cocodataset.org (images.cocodataset.org)|52.216.51.129|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 252907541 (241M) [application/zip]\nSaving to: ‘annotations_trainval2017.zip’\n\nannotations_trainva 100%[===================&gt;] 241.19M  15.6MB/s    in 17s     \n\n2024-02-13 02:30:09 (14.3 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n\n\n\n\n# Unzip\n!unzip -q train2017.zip -d ./coco\n!unzip -q val2017.zip -d ./coco\n!unzip -q annotations_trainval2017.zip -d ./coco\n\n\n# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n\n\nimport torch\n# 재현성을 위한 시드넘버 고정\nrandom_seed = 2024\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\n\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"coco_train_data\",{},'/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json', '/root/2024winter/DL_tutorial/posts/coco/val2017')\nregister_coco_instances(\"coco_val_data\",{},'/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json', '/root/2024winter/DL_tutorial/posts/coco/val2017')\n\n\n# MetadataCatalog\ntrain_metadata = MetadataCatalog.get(\"coco_train_data\")\nval_metadata = MetadataCatalog.get(\"coco_val_data\")\ntrain_metadata\n\nnamespace(name='coco_train_data',\n          json_file='/root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json',\n          image_root='/root/2024winter/DL_tutorial/posts/coco/val2017',\n          evaluator_type='coco')\n\n\n\n# DatasetCatalog\ntrain_dataset_dicts = DatasetCatalog.get(\"coco_train_data\")\nval_dataset_dicts = DatasetCatalog.get(\"coco_val_data\")\n\n[02/13 04:42:15 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:42:16 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.03 seconds.\n[02/13 04:42:17 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n\n\n\n\n\nfor d in random.sample(train_dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.figure(figsize=(5, 5))\n    plt.imshow(vis.get_image()[:, :, ::-1])\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(\n    \"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n)\ncfg.DATASETS.TRAIN = (\"coco_train_data\",)\ncfg.DATASETS.TEST = (\"coco_val_data\",)  # no metrics implemented for this dataset\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.002\ncfg.SOLVER.MAX_ITER = 1000\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\ncfg.OUTPUT_DIR = \"/root/2024winter/DL_tutorial/posts/outputs\"\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()\n\n[02/13 04:44:51 d2.engine.defaults]: Model:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n[02/13 04:44:52 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.07 seconds.\n[02/13 04:44:52 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:44:52 d2.data.build]: Removed 48 images with no usable annotations. 4952 images left.\n[02/13 04:44:52 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n[02/13 04:44:52 d2.data.build]: Using training sampler TrainingSampler\n[02/13 04:44:52 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:44:52 d2.data.common]: Serializing 4952 elements to byte tensors and concatenating them all ...\n[02/13 04:44:52 d2.data.common]: Serialized dataset takes 19.21 MiB\n[02/13 04:44:52 d2.data.build]: Making batched data loader with batch_size=2\nWARNING [02/13 04:44:52 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n[02/13 04:44:53 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n[02/13 04:44:53 d2.engine.train_loop]: Starting training from iteration 0\n[02/13 04:44:55 d2.utils.events]:  eta: 0:01:51  iter: 19  total_loss: 0.9135  loss_cls: 0.2255  loss_box_reg: 0.3434  loss_mask: 0.1967  loss_rpn_cls: 0.01907  loss_rpn_loc: 0.02518    time: 0.1183  last_time: 0.1094  data_time: 0.0207  last_data_time: 0.0048   lr: 3.9962e-05  max_mem: 2468M\n[02/13 04:44:58 d2.utils.events]:  eta: 0:01:53  iter: 39  total_loss: 0.9642  loss_cls: 0.2753  loss_box_reg: 0.3455  loss_mask: 0.2147  loss_rpn_cls: 0.02717  loss_rpn_loc: 0.04455    time: 0.1210  last_time: 0.1222  data_time: 0.0081  last_data_time: 0.0082   lr: 7.9922e-05  max_mem: 2468M\n[02/13 04:45:00 d2.utils.events]:  eta: 0:01:50  iter: 59  total_loss: 1.091  loss_cls: 0.2995  loss_box_reg: 0.4305  loss_mask: 0.2721  loss_rpn_cls: 0.0325  loss_rpn_loc: 0.05139    time: 0.1211  last_time: 0.1111  data_time: 0.0063  last_data_time: 0.0075   lr: 0.00011988  max_mem: 2468M\n[02/13 04:45:03 d2.utils.events]:  eta: 0:01:48  iter: 79  total_loss: 0.9919  loss_cls: 0.2939  loss_box_reg: 0.3489  loss_mask: 0.2043  loss_rpn_cls: 0.03127  loss_rpn_loc: 0.04819    time: 0.1205  last_time: 0.1033  data_time: 0.0104  last_data_time: 0.0054   lr: 0.00015984  max_mem: 2468M\n[02/13 04:45:05 d2.utils.events]:  eta: 0:01:45  iter: 99  total_loss: 1.055  loss_cls: 0.2229  loss_box_reg: 0.4061  loss_mask: 0.2197  loss_rpn_cls: 0.04041  loss_rpn_loc: 0.06296    time: 0.1200  last_time: 0.1120  data_time: 0.0085  last_data_time: 0.0115   lr: 0.0001998  max_mem: 2468M\n[02/13 04:45:08 d2.utils.events]:  eta: 0:01:45  iter: 119  total_loss: 1.126  loss_cls: 0.29  loss_box_reg: 0.3477  loss_mask: 0.2278  loss_rpn_cls: 0.04413  loss_rpn_loc: 0.08736    time: 0.1218  last_time: 0.1353  data_time: 0.0086  last_data_time: 0.0079   lr: 0.00023976  max_mem: 2468M\n[02/13 04:45:10 d2.utils.events]:  eta: 0:01:44  iter: 139  total_loss: 1.052  loss_cls: 0.3016  loss_box_reg: 0.4075  loss_mask: 0.2272  loss_rpn_cls: 0.03207  loss_rpn_loc: 0.04996    time: 0.1235  last_time: 0.1336  data_time: 0.0109  last_data_time: 0.0096   lr: 0.00027972  max_mem: 2468M\n[02/13 04:45:13 d2.utils.events]:  eta: 0:01:41  iter: 159  total_loss: 1.12  loss_cls: 0.295  loss_box_reg: 0.401  loss_mask: 0.2615  loss_rpn_cls: 0.02364  loss_rpn_loc: 0.04561    time: 0.1234  last_time: 0.1250  data_time: 0.0067  last_data_time: 0.0081   lr: 0.00031968  max_mem: 2473M\n[02/13 04:45:16 d2.utils.events]:  eta: 0:01:39  iter: 179  total_loss: 0.9365  loss_cls: 0.2408  loss_box_reg: 0.3592  loss_mask: 0.233  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.03028    time: 0.1244  last_time: 0.1110  data_time: 0.0151  last_data_time: 0.0066   lr: 0.00035964  max_mem: 2473M\n[02/13 04:45:18 d2.utils.events]:  eta: 0:01:37  iter: 199  total_loss: 0.9998  loss_cls: 0.2416  loss_box_reg: 0.4081  loss_mask: 0.2409  loss_rpn_cls: 0.02947  loss_rpn_loc: 0.03085    time: 0.1246  last_time: 0.1183  data_time: 0.0094  last_data_time: 0.0052   lr: 0.0003996  max_mem: 2473M\n[02/13 04:45:20 d2.utils.events]:  eta: 0:01:34  iter: 219  total_loss: 0.9961  loss_cls: 0.2573  loss_box_reg: 0.3871  loss_mask: 0.236  loss_rpn_cls: 0.03193  loss_rpn_loc: 0.03565    time: 0.1242  last_time: 0.1278  data_time: 0.0062  last_data_time: 0.0088   lr: 0.00043956  max_mem: 2473M\n[02/13 04:45:23 d2.utils.events]:  eta: 0:01:32  iter: 239  total_loss: 0.9405  loss_cls: 0.2386  loss_box_reg: 0.3411  loss_mask: 0.2369  loss_rpn_cls: 0.02037  loss_rpn_loc: 0.0328    time: 0.1240  last_time: 0.1462  data_time: 0.0074  last_data_time: 0.0097   lr: 0.00047952  max_mem: 2473M\n[02/13 04:45:26 d2.utils.events]:  eta: 0:01:29  iter: 259  total_loss: 1.075  loss_cls: 0.3166  loss_box_reg: 0.3828  loss_mask: 0.2512  loss_rpn_cls: 0.0336  loss_rpn_loc: 0.04556    time: 0.1245  last_time: 0.1265  data_time: 0.0094  last_data_time: 0.0056   lr: 0.00051948  max_mem: 2473M\n[02/13 04:45:28 d2.utils.events]:  eta: 0:01:26  iter: 279  total_loss: 0.9532  loss_cls: 0.2711  loss_box_reg: 0.3281  loss_mask: 0.2328  loss_rpn_cls: 0.02803  loss_rpn_loc: 0.03686    time: 0.1239  last_time: 0.1217  data_time: 0.0073  last_data_time: 0.0062   lr: 0.00055944  max_mem: 2473M\n[02/13 04:45:30 d2.utils.events]:  eta: 0:01:24  iter: 299  total_loss: 0.8519  loss_cls: 0.236  loss_box_reg: 0.3498  loss_mask: 0.2202  loss_rpn_cls: 0.01544  loss_rpn_loc: 0.02504    time: 0.1239  last_time: 0.1432  data_time: 0.0085  last_data_time: 0.0068   lr: 0.0005994  max_mem: 2473M\n[02/13 04:45:33 d2.utils.events]:  eta: 0:01:22  iter: 319  total_loss: 0.9913  loss_cls: 0.2939  loss_box_reg: 0.3954  loss_mask: 0.2473  loss_rpn_cls: 0.02953  loss_rpn_loc: 0.05801    time: 0.1243  last_time: 0.1396  data_time: 0.0099  last_data_time: 0.0036   lr: 0.00063936  max_mem: 2473M\n[02/13 04:45:35 d2.utils.events]:  eta: 0:01:19  iter: 339  total_loss: 1.084  loss_cls: 0.2824  loss_box_reg: 0.3847  loss_mask: 0.2359  loss_rpn_cls: 0.03157  loss_rpn_loc: 0.03688    time: 0.1241  last_time: 0.1225  data_time: 0.0076  last_data_time: 0.0057   lr: 0.00067932  max_mem: 2473M\n[02/13 04:45:38 d2.utils.events]:  eta: 0:01:16  iter: 359  total_loss: 0.9851  loss_cls: 0.2923  loss_box_reg: 0.3409  loss_mask: 0.2503  loss_rpn_cls: 0.01778  loss_rpn_loc: 0.03838    time: 0.1236  last_time: 0.1043  data_time: 0.0061  last_data_time: 0.0077   lr: 0.00071928  max_mem: 2473M\n[02/13 04:45:40 d2.utils.events]:  eta: 0:01:14  iter: 379  total_loss: 0.9603  loss_cls: 0.243  loss_box_reg: 0.3565  loss_mask: 0.2018  loss_rpn_cls: 0.02237  loss_rpn_loc: 0.04897    time: 0.1233  last_time: 0.1007  data_time: 0.0075  last_data_time: 0.0057   lr: 0.00075924  max_mem: 2473M\n[02/13 04:45:43 d2.utils.events]:  eta: 0:01:11  iter: 399  total_loss: 0.8493  loss_cls: 0.193  loss_box_reg: 0.2683  loss_mask: 0.2049  loss_rpn_cls: 0.04498  loss_rpn_loc: 0.06218    time: 0.1231  last_time: 0.1183  data_time: 0.0095  last_data_time: 0.0048   lr: 0.0007992  max_mem: 2473M\n[02/13 04:45:45 d2.utils.events]:  eta: 0:01:09  iter: 419  total_loss: 0.9737  loss_cls: 0.2657  loss_box_reg: 0.3631  loss_mask: 0.2675  loss_rpn_cls: 0.0237  loss_rpn_loc: 0.02856    time: 0.1228  last_time: 0.1337  data_time: 0.0061  last_data_time: 0.0055   lr: 0.00083916  max_mem: 2473M\n[02/13 04:45:47 d2.utils.events]:  eta: 0:01:06  iter: 439  total_loss: 0.9801  loss_cls: 0.2758  loss_box_reg: 0.3691  loss_mask: 0.2352  loss_rpn_cls: 0.03374  loss_rpn_loc: 0.0679    time: 0.1228  last_time: 0.1154  data_time: 0.0095  last_data_time: 0.0057   lr: 0.00087912  max_mem: 2473M\n[02/13 04:45:50 d2.utils.events]:  eta: 0:01:04  iter: 459  total_loss: 0.9442  loss_cls: 0.2619  loss_box_reg: 0.3198  loss_mask: 0.2345  loss_rpn_cls: 0.03548  loss_rpn_loc: 0.05815    time: 0.1226  last_time: 0.1406  data_time: 0.0054  last_data_time: 0.0030   lr: 0.00091908  max_mem: 2473M\n[02/13 04:45:52 d2.utils.events]:  eta: 0:01:01  iter: 479  total_loss: 1.086  loss_cls: 0.2395  loss_box_reg: 0.4103  loss_mask: 0.2464  loss_rpn_cls: 0.04081  loss_rpn_loc: 0.06109    time: 0.1225  last_time: 0.1251  data_time: 0.0082  last_data_time: 0.0061   lr: 0.00095904  max_mem: 2473M\n[02/13 04:45:55 d2.utils.events]:  eta: 0:00:59  iter: 499  total_loss: 1.179  loss_cls: 0.31  loss_box_reg: 0.3986  loss_mask: 0.2535  loss_rpn_cls: 0.04035  loss_rpn_loc: 0.1044    time: 0.1225  last_time: 0.1086  data_time: 0.0073  last_data_time: 0.0070   lr: 0.000999  max_mem: 2473M\n[02/13 04:45:57 d2.utils.events]:  eta: 0:00:57  iter: 519  total_loss: 0.9325  loss_cls: 0.2939  loss_box_reg: 0.3721  loss_mask: 0.2286  loss_rpn_cls: 0.02657  loss_rpn_loc: 0.02841    time: 0.1229  last_time: 0.1582  data_time: 0.0096  last_data_time: 0.0217   lr: 0.001039  max_mem: 2473M\n[02/13 04:46:00 d2.utils.events]:  eta: 0:00:55  iter: 539  total_loss: 0.9261  loss_cls: 0.2287  loss_box_reg: 0.3299  loss_mask: 0.2194  loss_rpn_cls: 0.02425  loss_rpn_loc: 0.05447    time: 0.1230  last_time: 0.1237  data_time: 0.0062  last_data_time: 0.0050   lr: 0.0010789  max_mem: 2473M\n[02/13 04:46:02 d2.utils.events]:  eta: 0:00:52  iter: 559  total_loss: 1.173  loss_cls: 0.3525  loss_box_reg: 0.3845  loss_mask: 0.26  loss_rpn_cls: 0.03674  loss_rpn_loc: 0.0619    time: 0.1229  last_time: 0.1308  data_time: 0.0065  last_data_time: 0.0054   lr: 0.0011189  max_mem: 2473M\n[02/13 04:46:05 d2.utils.events]:  eta: 0:00:50  iter: 579  total_loss: 1.017  loss_cls: 0.3499  loss_box_reg: 0.3281  loss_mask: 0.2034  loss_rpn_cls: 0.03605  loss_rpn_loc: 0.04706    time: 0.1231  last_time: 0.1141  data_time: 0.0105  last_data_time: 0.0081   lr: 0.0011588  max_mem: 2473M\n[02/13 04:46:07 d2.utils.events]:  eta: 0:00:48  iter: 599  total_loss: 1.047  loss_cls: 0.3254  loss_box_reg: 0.3606  loss_mask: 0.2615  loss_rpn_cls: 0.03869  loss_rpn_loc: 0.03192    time: 0.1235  last_time: 0.1356  data_time: 0.0088  last_data_time: 0.0192   lr: 0.0011988  max_mem: 2473M\n[02/13 04:46:10 d2.utils.events]:  eta: 0:00:45  iter: 619  total_loss: 0.7698  loss_cls: 0.2613  loss_box_reg: 0.2978  loss_mask: 0.2185  loss_rpn_cls: 0.02539  loss_rpn_loc: 0.03508    time: 0.1241  last_time: 0.1243  data_time: 0.0130  last_data_time: 0.0107   lr: 0.0012388  max_mem: 2473M\n[02/13 04:46:13 d2.utils.events]:  eta: 0:00:43  iter: 639  total_loss: 1.112  loss_cls: 0.353  loss_box_reg: 0.3621  loss_mask: 0.2437  loss_rpn_cls: 0.02337  loss_rpn_loc: 0.05272    time: 0.1247  last_time: 0.1451  data_time: 0.0151  last_data_time: 0.0145   lr: 0.0012787  max_mem: 2473M\n[02/13 04:46:16 d2.utils.events]:  eta: 0:00:41  iter: 659  total_loss: 1.291  loss_cls: 0.3047  loss_box_reg: 0.3817  loss_mask: 0.2762  loss_rpn_cls: 0.03335  loss_rpn_loc: 0.05792    time: 0.1257  last_time: 0.1471  data_time: 0.0157  last_data_time: 0.0076   lr: 0.0013187  max_mem: 2473M\n[02/13 04:46:20 d2.utils.events]:  eta: 0:00:39  iter: 679  total_loss: 1.081  loss_cls: 0.3433  loss_box_reg: 0.3318  loss_mask: 0.2444  loss_rpn_cls: 0.05075  loss_rpn_loc: 0.04307    time: 0.1268  last_time: 0.1964  data_time: 0.0254  last_data_time: 0.0112   lr: 0.0013586  max_mem: 2473M\n[02/13 04:46:23 d2.utils.events]:  eta: 0:00:36  iter: 699  total_loss: 0.9815  loss_cls: 0.2452  loss_box_reg: 0.3903  loss_mask: 0.2241  loss_rpn_cls: 0.02741  loss_rpn_loc: 0.04214    time: 0.1278  last_time: 0.1553  data_time: 0.0106  last_data_time: 0.0135   lr: 0.0013986  max_mem: 2473M\n[02/13 04:46:26 d2.utils.events]:  eta: 0:00:34  iter: 719  total_loss: 1.041  loss_cls: 0.3232  loss_box_reg: 0.3421  loss_mask: 0.2496  loss_rpn_cls: 0.0315  loss_rpn_loc: 0.03683    time: 0.1282  last_time: 0.1179  data_time: 0.0103  last_data_time: 0.0061   lr: 0.0014386  max_mem: 2473M\n[02/13 04:46:28 d2.utils.events]:  eta: 0:00:31  iter: 739  total_loss: 1.335  loss_cls: 0.4221  loss_box_reg: 0.433  loss_mask: 0.2776  loss_rpn_cls: 0.03746  loss_rpn_loc: 0.06529    time: 0.1283  last_time: 0.1676  data_time: 0.0087  last_data_time: 0.0276   lr: 0.0014785  max_mem: 2473M\n[02/13 04:46:31 d2.utils.events]:  eta: 0:00:29  iter: 759  total_loss: 1.054  loss_cls: 0.3465  loss_box_reg: 0.3453  loss_mask: 0.2371  loss_rpn_cls: 0.03524  loss_rpn_loc: 0.06073    time: 0.1286  last_time: 0.1249  data_time: 0.0068  last_data_time: 0.0056   lr: 0.0015185  max_mem: 2473M\n[02/13 04:46:34 d2.utils.events]:  eta: 0:00:27  iter: 779  total_loss: 1.288  loss_cls: 0.4118  loss_box_reg: 0.4728  loss_mask: 0.2628  loss_rpn_cls: 0.03728  loss_rpn_loc: 0.06528    time: 0.1285  last_time: 0.1223  data_time: 0.0067  last_data_time: 0.0134   lr: 0.0015584  max_mem: 2473M\n[02/13 04:46:36 d2.utils.events]:  eta: 0:00:24  iter: 799  total_loss: 1.276  loss_cls: 0.3934  loss_box_reg: 0.4404  loss_mask: 0.2486  loss_rpn_cls: 0.03664  loss_rpn_loc: 0.06733    time: 0.1285  last_time: 0.1452  data_time: 0.0106  last_data_time: 0.0204   lr: 0.0015984  max_mem: 2473M\n[02/13 04:46:39 d2.utils.events]:  eta: 0:00:22  iter: 819  total_loss: 1.046  loss_cls: 0.3259  loss_box_reg: 0.3935  loss_mask: 0.2525  loss_rpn_cls: 0.04915  loss_rpn_loc: 0.04426    time: 0.1283  last_time: 0.1370  data_time: 0.0093  last_data_time: 0.0082   lr: 0.0016384  max_mem: 2473M\n[02/13 04:46:41 d2.utils.events]:  eta: 0:00:19  iter: 839  total_loss: 1.168  loss_cls: 0.3326  loss_box_reg: 0.4265  loss_mask: 0.2727  loss_rpn_cls: 0.0369  loss_rpn_loc: 0.09471    time: 0.1282  last_time: 0.1198  data_time: 0.0080  last_data_time: 0.0050   lr: 0.0016783  max_mem: 2473M\n[02/13 04:46:44 d2.utils.events]:  eta: 0:00:17  iter: 859  total_loss: 1.234  loss_cls: 0.3414  loss_box_reg: 0.4458  loss_mask: 0.2656  loss_rpn_cls: 0.06422  loss_rpn_loc: 0.08606    time: 0.1280  last_time: 0.1110  data_time: 0.0063  last_data_time: 0.0099   lr: 0.0017183  max_mem: 2473M\n[02/13 04:46:46 d2.utils.events]:  eta: 0:00:14  iter: 879  total_loss: 1.186  loss_cls: 0.3201  loss_box_reg: 0.402  loss_mask: 0.2859  loss_rpn_cls: 0.03309  loss_rpn_loc: 0.03605    time: 0.1279  last_time: 0.1252  data_time: 0.0071  last_data_time: 0.0053   lr: 0.0017582  max_mem: 2473M\n[02/13 04:46:49 d2.utils.events]:  eta: 0:00:12  iter: 899  total_loss: 1.131  loss_cls: 0.3438  loss_box_reg: 0.3714  loss_mask: 0.2574  loss_rpn_cls: 0.02271  loss_rpn_loc: 0.04514    time: 0.1278  last_time: 0.1091  data_time: 0.0055  last_data_time: 0.0058   lr: 0.0017982  max_mem: 2473M\n[02/13 04:46:51 d2.utils.events]:  eta: 0:00:09  iter: 919  total_loss: 1.369  loss_cls: 0.3949  loss_box_reg: 0.4476  loss_mask: 0.2914  loss_rpn_cls: 0.04681  loss_rpn_loc: 0.09672    time: 0.1275  last_time: 0.1167  data_time: 0.0074  last_data_time: 0.0101   lr: 0.0018382  max_mem: 2473M\n[02/13 04:46:53 d2.utils.events]:  eta: 0:00:07  iter: 939  total_loss: 1.247  loss_cls: 0.3892  loss_box_reg: 0.4228  loss_mask: 0.2499  loss_rpn_cls: 0.05528  loss_rpn_loc: 0.04001    time: 0.1272  last_time: 0.0998  data_time: 0.0078  last_data_time: 0.0034   lr: 0.0018781  max_mem: 2473M\n[02/13 04:46:56 d2.utils.events]:  eta: 0:00:04  iter: 959  total_loss: 1.189  loss_cls: 0.4043  loss_box_reg: 0.4079  loss_mask: 0.2503  loss_rpn_cls: 0.03474  loss_rpn_loc: 0.06123    time: 0.1271  last_time: 0.1097  data_time: 0.0067  last_data_time: 0.0051   lr: 0.0019181  max_mem: 2473M\n[02/13 04:46:58 d2.utils.events]:  eta: 0:00:02  iter: 979  total_loss: 1.276  loss_cls: 0.3649  loss_box_reg: 0.4229  loss_mask: 0.2671  loss_rpn_cls: 0.0651  loss_rpn_loc: 0.1141    time: 0.1269  last_time: 0.1131  data_time: 0.0070  last_data_time: 0.0059   lr: 0.001958  max_mem: 2473M\n[02/13 04:47:01 d2.utils.events]:  eta: 0:00:00  iter: 999  total_loss: 1.171  loss_cls: 0.4161  loss_box_reg: 0.4521  loss_mask: 0.2544  loss_rpn_cls: 0.04434  loss_rpn_loc: 0.03685    time: 0.1266  last_time: 0.1066  data_time: 0.0076  last_data_time: 0.0030   lr: 0.001998  max_mem: 2473M\n[02/13 04:47:01 d2.engine.hooks]: Overall training speed: 998 iterations in 0:02:06 (0.1266 s / it)\n[02/13 04:47:01 d2.engine.hooks]: Total training time: 0:02:08 (0:00:01 on hooks)\n[02/13 04:47:02 d2.data.datasets.coco]: Loading /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json takes 1.06 seconds.\n[02/13 04:47:03 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:47:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n[02/13 04:47:03 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:47:03 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...\n[02/13 04:47:04 d2.data.common]: Serialized dataset takes 19.22 MiB\nWARNING [02/13 04:47:04 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n\n\n\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\npredictor = DefaultPredictor(cfg)\n\nevaluator = COCOEvaluator(\"coco_val_data\", output_dir = \"./output\")\nval_loader = build_detection_test_loader(cfg, \"coco_val_data\")\ninference_on_dataset(trainer.model, val_loader, evaluator)\n\n[02/13 04:51:01 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /root/2024winter/DL_tutorial/posts/outputs/model_final.pth ...\n[02/13 04:51:03 d2.data.datasets.coco]: Loaded 5000 images in COCO format from /root/2024winter/DL_tutorial/posts/coco/annotations/instances_val2017.json\n[02/13 04:51:04 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n[02/13 04:51:04 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[02/13 04:51:04 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...\n[02/13 04:51:04 d2.data.common]: Serialized dataset takes 19.22 MiB\n[02/13 04:51:04 d2.evaluation.evaluator]: Start inference on 5000 batches\n[02/13 04:51:05 d2.evaluation.evaluator]: Inference done 11/5000. Dataloading: 0.0014 s/iter. Inference: 0.0588 s/iter. Eval: 0.0347 s/iter. Total: 0.0949 s/iter. ETA=0:07:53\n[02/13 04:51:10 d2.evaluation.evaluator]: Inference done 69/5000. Dataloading: 0.0018 s/iter. Inference: 0.0511 s/iter. Eval: 0.0343 s/iter. Total: 0.0874 s/iter. ETA=0:07:10\n[02/13 04:51:15 d2.evaluation.evaluator]: Inference done 124/5000. Dataloading: 0.0019 s/iter. Inference: 0.0505 s/iter. Eval: 0.0371 s/iter. Total: 0.0896 s/iter. ETA=0:07:16\n[02/13 04:51:20 d2.evaluation.evaluator]: Inference done 184/5000. Dataloading: 0.0019 s/iter. Inference: 0.0480 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:07:01\n[02/13 04:51:25 d2.evaluation.evaluator]: Inference done 247/5000. Dataloading: 0.0018 s/iter. Inference: 0.0473 s/iter. Eval: 0.0362 s/iter. Total: 0.0854 s/iter. ETA=0:06:45\n[02/13 04:51:31 d2.evaluation.evaluator]: Inference done 312/5000. Dataloading: 0.0018 s/iter. Inference: 0.0457 s/iter. Eval: 0.0363 s/iter. Total: 0.0838 s/iter. ETA=0:06:32\n[02/13 04:51:36 d2.evaluation.evaluator]: Inference done 371/5000. Dataloading: 0.0019 s/iter. Inference: 0.0460 s/iter. Eval: 0.0361 s/iter. Total: 0.0841 s/iter. ETA=0:06:29\n[02/13 04:51:41 d2.evaluation.evaluator]: Inference done 431/5000. Dataloading: 0.0019 s/iter. Inference: 0.0464 s/iter. Eval: 0.0356 s/iter. Total: 0.0840 s/iter. ETA=0:06:23\n[02/13 04:51:46 d2.evaluation.evaluator]: Inference done 489/5000. Dataloading: 0.0019 s/iter. Inference: 0.0471 s/iter. Eval: 0.0352 s/iter. Total: 0.0843 s/iter. ETA=0:06:20\n[02/13 04:51:51 d2.evaluation.evaluator]: Inference done 539/5000. Dataloading: 0.0019 s/iter. Inference: 0.0476 s/iter. Eval: 0.0363 s/iter. Total: 0.0860 s/iter. ETA=0:06:23\n[02/13 04:51:56 d2.evaluation.evaluator]: Inference done 596/5000. Dataloading: 0.0020 s/iter. Inference: 0.0476 s/iter. Eval: 0.0365 s/iter. Total: 0.0862 s/iter. ETA=0:06:19\n[02/13 04:52:01 d2.evaluation.evaluator]: Inference done 653/5000. Dataloading: 0.0020 s/iter. Inference: 0.0475 s/iter. Eval: 0.0368 s/iter. Total: 0.0863 s/iter. ETA=0:06:15\n[02/13 04:52:06 d2.evaluation.evaluator]: Inference done 713/5000. Dataloading: 0.0020 s/iter. Inference: 0.0474 s/iter. Eval: 0.0367 s/iter. Total: 0.0862 s/iter. ETA=0:06:09\n[02/13 04:52:11 d2.evaluation.evaluator]: Inference done 777/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0365 s/iter. Total: 0.0856 s/iter. ETA=0:06:01\n[02/13 04:52:16 d2.evaluation.evaluator]: Inference done 835/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0365 s/iter. Total: 0.0856 s/iter. ETA=0:05:56\n[02/13 04:52:21 d2.evaluation.evaluator]: Inference done 895/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0364 s/iter. Total: 0.0856 s/iter. ETA=0:05:51\n[02/13 04:52:26 d2.evaluation.evaluator]: Inference done 953/5000. Dataloading: 0.0020 s/iter. Inference: 0.0471 s/iter. Eval: 0.0364 s/iter. Total: 0.0856 s/iter. ETA=0:05:46\n[02/13 04:52:31 d2.evaluation.evaluator]: Inference done 1015/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0362 s/iter. Total: 0.0853 s/iter. ETA=0:05:40\n[02/13 04:52:36 d2.evaluation.evaluator]: Inference done 1074/5000. Dataloading: 0.0020 s/iter. Inference: 0.0469 s/iter. Eval: 0.0363 s/iter. Total: 0.0853 s/iter. ETA=0:05:35\n[02/13 04:52:41 d2.evaluation.evaluator]: Inference done 1133/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0362 s/iter. Total: 0.0853 s/iter. ETA=0:05:29\n[02/13 04:52:46 d2.evaluation.evaluator]: Inference done 1193/5000. Dataloading: 0.0020 s/iter. Inference: 0.0470 s/iter. Eval: 0.0361 s/iter. Total: 0.0853 s/iter. ETA=0:05:24\n[02/13 04:52:51 d2.evaluation.evaluator]: Inference done 1253/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0363 s/iter. Total: 0.0852 s/iter. ETA=0:05:19\n[02/13 04:52:56 d2.evaluation.evaluator]: Inference done 1313/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0362 s/iter. Total: 0.0852 s/iter. ETA=0:05:14\n[02/13 04:53:01 d2.evaluation.evaluator]: Inference done 1365/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0367 s/iter. Total: 0.0856 s/iter. ETA=0:05:11\n[02/13 04:53:06 d2.evaluation.evaluator]: Inference done 1427/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:05:05\n[02/13 04:53:11 d2.evaluation.evaluator]: Inference done 1487/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0366 s/iter. Total: 0.0854 s/iter. ETA=0:04:59\n[02/13 04:53:16 d2.evaluation.evaluator]: Inference done 1547/5000. Dataloading: 0.0020 s/iter. Inference: 0.0466 s/iter. Eval: 0.0366 s/iter. Total: 0.0854 s/iter. ETA=0:04:54\n[02/13 04:53:21 d2.evaluation.evaluator]: Inference done 1605/5000. Dataloading: 0.0020 s/iter. Inference: 0.0466 s/iter. Eval: 0.0367 s/iter. Total: 0.0854 s/iter. ETA=0:04:49\n[02/13 04:53:26 d2.evaluation.evaluator]: Inference done 1662/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:04:45\n[02/13 04:53:32 d2.evaluation.evaluator]: Inference done 1719/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0367 s/iter. Total: 0.0856 s/iter. ETA=0:04:40\n[02/13 04:53:37 d2.evaluation.evaluator]: Inference done 1781/5000. Dataloading: 0.0020 s/iter. Inference: 0.0468 s/iter. Eval: 0.0366 s/iter. Total: 0.0855 s/iter. ETA=0:04:35\n[02/13 04:53:42 d2.evaluation.evaluator]: Inference done 1844/5000. Dataloading: 0.0020 s/iter. Inference: 0.0467 s/iter. Eval: 0.0365 s/iter. Total: 0.0853 s/iter. ETA=0:04:29\n[02/13 04:53:47 d2.evaluation.evaluator]: Inference done 1906/5000. Dataloading: 0.0020 s/iter. Inference: 0.0465 s/iter. Eval: 0.0364 s/iter. Total: 0.0851 s/iter. ETA=0:04:23\n[02/13 04:53:52 d2.evaluation.evaluator]: Inference done 1969/5000. Dataloading: 0.0020 s/iter. Inference: 0.0465 s/iter. Eval: 0.0363 s/iter. Total: 0.0850 s/iter. ETA=0:04:17\n[02/13 04:53:57 d2.evaluation.evaluator]: Inference done 2031/5000. Dataloading: 0.0020 s/iter. Inference: 0.0463 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:11\n[02/13 04:54:02 d2.evaluation.evaluator]: Inference done 2090/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:07\n[02/13 04:54:07 d2.evaluation.evaluator]: Inference done 2149/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:04:02\n[02/13 04:54:12 d2.evaluation.evaluator]: Inference done 2207/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0365 s/iter. Total: 0.0850 s/iter. ETA=0:03:57\n[02/13 04:54:17 d2.evaluation.evaluator]: Inference done 2265/5000. Dataloading: 0.0020 s/iter. Inference: 0.0464 s/iter. Eval: 0.0365 s/iter. Total: 0.0850 s/iter. ETA=0:03:52\n[02/13 04:54:22 d2.evaluation.evaluator]: Inference done 2326/5000. Dataloading: 0.0021 s/iter. Inference: 0.0464 s/iter. Eval: 0.0364 s/iter. Total: 0.0849 s/iter. ETA=0:03:47\n[02/13 04:54:27 d2.evaluation.evaluator]: Inference done 2382/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0366 s/iter. Total: 0.0851 s/iter. ETA=0:03:42\n[02/13 04:54:32 d2.evaluation.evaluator]: Inference done 2440/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0367 s/iter. Total: 0.0851 s/iter. ETA=0:03:37\n[02/13 04:54:37 d2.evaluation.evaluator]: Inference done 2498/5000. Dataloading: 0.0021 s/iter. Inference: 0.0463 s/iter. Eval: 0.0367 s/iter. Total: 0.0851 s/iter. ETA=0:03:33\n[02/13 04:54:42 d2.evaluation.evaluator]: Inference done 2546/5000. Dataloading: 0.0021 s/iter. Inference: 0.0465 s/iter. Eval: 0.0368 s/iter. Total: 0.0855 s/iter. ETA=0:03:29\n[02/13 04:54:47 d2.evaluation.evaluator]: Inference done 2589/5000. Dataloading: 0.0021 s/iter. Inference: 0.0467 s/iter. Eval: 0.0371 s/iter. Total: 0.0861 s/iter. ETA=0:03:27\n[02/13 04:54:52 d2.evaluation.evaluator]: Inference done 2628/5000. Dataloading: 0.0021 s/iter. Inference: 0.0471 s/iter. Eval: 0.0374 s/iter. Total: 0.0867 s/iter. ETA=0:03:25\n[02/13 04:54:57 d2.evaluation.evaluator]: Inference done 2666/5000. Dataloading: 0.0021 s/iter. Inference: 0.0475 s/iter. Eval: 0.0377 s/iter. Total: 0.0874 s/iter. ETA=0:03:24\n[02/13 04:55:03 d2.evaluation.evaluator]: Inference done 2720/5000. Dataloading: 0.0022 s/iter. Inference: 0.0477 s/iter. Eval: 0.0376 s/iter. Total: 0.0876 s/iter. ETA=0:03:19\n[02/13 04:55:08 d2.evaluation.evaluator]: Inference done 2778/5000. Dataloading: 0.0021 s/iter. Inference: 0.0478 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:03:14\n[02/13 04:55:13 d2.evaluation.evaluator]: Inference done 2836/5000. Dataloading: 0.0021 s/iter. Inference: 0.0478 s/iter. Eval: 0.0375 s/iter. Total: 0.0875 s/iter. ETA=0:03:09\n[02/13 04:55:18 d2.evaluation.evaluator]: Inference done 2892/5000. Dataloading: 0.0021 s/iter. Inference: 0.0480 s/iter. Eval: 0.0373 s/iter. Total: 0.0876 s/iter. ETA=0:03:04\n[02/13 04:55:23 d2.evaluation.evaluator]: Inference done 2948/5000. Dataloading: 0.0021 s/iter. Inference: 0.0481 s/iter. Eval: 0.0373 s/iter. Total: 0.0876 s/iter. ETA=0:02:59\n[02/13 04:55:28 d2.evaluation.evaluator]: Inference done 3007/5000. Dataloading: 0.0021 s/iter. Inference: 0.0482 s/iter. Eval: 0.0371 s/iter. Total: 0.0876 s/iter. ETA=0:02:54\n[02/13 04:55:33 d2.evaluation.evaluator]: Inference done 3065/5000. Dataloading: 0.0021 s/iter. Inference: 0.0483 s/iter. Eval: 0.0370 s/iter. Total: 0.0876 s/iter. ETA=0:02:49\n[02/13 04:55:38 d2.evaluation.evaluator]: Inference done 3125/5000. Dataloading: 0.0021 s/iter. Inference: 0.0484 s/iter. Eval: 0.0369 s/iter. Total: 0.0875 s/iter. ETA=0:02:44\n[02/13 04:55:43 d2.evaluation.evaluator]: Inference done 3185/5000. Dataloading: 0.0021 s/iter. Inference: 0.0484 s/iter. Eval: 0.0368 s/iter. Total: 0.0875 s/iter. ETA=0:02:38\n[02/13 04:55:48 d2.evaluation.evaluator]: Inference done 3242/5000. Dataloading: 0.0021 s/iter. Inference: 0.0485 s/iter. Eval: 0.0368 s/iter. Total: 0.0875 s/iter. ETA=0:02:33\n[02/13 04:55:53 d2.evaluation.evaluator]: Inference done 3299/5000. Dataloading: 0.0021 s/iter. Inference: 0.0485 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:28\n[02/13 04:55:58 d2.evaluation.evaluator]: Inference done 3355/5000. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:23\n[02/13 04:56:03 d2.evaluation.evaluator]: Inference done 3413/5000. Dataloading: 0.0021 s/iter. Inference: 0.0486 s/iter. Eval: 0.0367 s/iter. Total: 0.0875 s/iter. ETA=0:02:18\n[02/13 04:56:08 d2.evaluation.evaluator]: Inference done 3474/5000. Dataloading: 0.0021 s/iter. Inference: 0.0487 s/iter. Eval: 0.0366 s/iter. Total: 0.0874 s/iter. ETA=0:02:13\n[02/13 04:56:13 d2.evaluation.evaluator]: Inference done 3535/5000. Dataloading: 0.0021 s/iter. Inference: 0.0487 s/iter. Eval: 0.0365 s/iter. Total: 0.0874 s/iter. ETA=0:02:07\n[02/13 04:56:18 d2.evaluation.evaluator]: Inference done 3582/5000. Dataloading: 0.0021 s/iter. Inference: 0.0488 s/iter. Eval: 0.0366 s/iter. Total: 0.0876 s/iter. ETA=0:02:04\n[02/13 04:56:23 d2.evaluation.evaluator]: Inference done 3642/5000. Dataloading: 0.0021 s/iter. Inference: 0.0488 s/iter. Eval: 0.0366 s/iter. Total: 0.0876 s/iter. ETA=0:01:58\n[02/13 04:56:28 d2.evaluation.evaluator]: Inference done 3699/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0365 s/iter. Total: 0.0876 s/iter. ETA=0:01:53\n[02/13 04:56:33 d2.evaluation.evaluator]: Inference done 3760/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0364 s/iter. Total: 0.0875 s/iter. ETA=0:01:48\n[02/13 04:56:38 d2.evaluation.evaluator]: Inference done 3820/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0364 s/iter. Total: 0.0875 s/iter. ETA=0:01:43\n[02/13 04:56:43 d2.evaluation.evaluator]: Inference done 3885/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0362 s/iter. Total: 0.0873 s/iter. ETA=0:01:37\n[02/13 04:56:49 d2.evaluation.evaluator]: Inference done 3947/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:31\n[02/13 04:56:54 d2.evaluation.evaluator]: Inference done 4003/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:26\n[02/13 04:56:59 d2.evaluation.evaluator]: Inference done 4058/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0362 s/iter. Total: 0.0873 s/iter. ETA=0:01:22\n[02/13 04:57:04 d2.evaluation.evaluator]: Inference done 4121/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:16\n[02/13 04:57:09 d2.evaluation.evaluator]: Inference done 4178/5000. Dataloading: 0.0021 s/iter. Inference: 0.0489 s/iter. Eval: 0.0361 s/iter. Total: 0.0872 s/iter. ETA=0:01:11\n[02/13 04:57:14 d2.evaluation.evaluator]: Inference done 4238/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0360 s/iter. Total: 0.0872 s/iter. ETA=0:01:06\n[02/13 04:57:19 d2.evaluation.evaluator]: Inference done 4299/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0360 s/iter. Total: 0.0871 s/iter. ETA=0:01:01\n[02/13 04:57:24 d2.evaluation.evaluator]: Inference done 4357/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0359 s/iter. Total: 0.0871 s/iter. ETA=0:00:56\n[02/13 04:57:29 d2.evaluation.evaluator]: Inference done 4417/5000. Dataloading: 0.0021 s/iter. Inference: 0.0490 s/iter. Eval: 0.0359 s/iter. Total: 0.0871 s/iter. ETA=0:00:50\n[02/13 04:57:34 d2.evaluation.evaluator]: Inference done 4475/5000. Dataloading: 0.0021 s/iter. Inference: 0.0491 s/iter. Eval: 0.0358 s/iter. Total: 0.0871 s/iter. ETA=0:00:45\n[02/13 04:57:39 d2.evaluation.evaluator]: Inference done 4535/5000. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0357 s/iter. Total: 0.0870 s/iter. ETA=0:00:40\n[02/13 04:57:44 d2.evaluation.evaluator]: Inference done 4596/5000. Dataloading: 0.0020 s/iter. Inference: 0.0491 s/iter. Eval: 0.0357 s/iter. Total: 0.0870 s/iter. ETA=0:00:35\n[02/13 04:57:49 d2.evaluation.evaluator]: Inference done 4647/5000. Dataloading: 0.0020 s/iter. Inference: 0.0492 s/iter. Eval: 0.0358 s/iter. Total: 0.0871 s/iter. ETA=0:00:30\n[02/13 04:57:54 d2.evaluation.evaluator]: Inference done 4681/5000. Dataloading: 0.0021 s/iter. Inference: 0.0494 s/iter. Eval: 0.0360 s/iter. Total: 0.0875 s/iter. ETA=0:00:27\n[02/13 04:57:59 d2.evaluation.evaluator]: Inference done 4720/5000. Dataloading: 0.0021 s/iter. Inference: 0.0496 s/iter. Eval: 0.0361 s/iter. Total: 0.0879 s/iter. ETA=0:00:24\n[02/13 04:58:04 d2.evaluation.evaluator]: Inference done 4767/5000. Dataloading: 0.0021 s/iter. Inference: 0.0497 s/iter. Eval: 0.0362 s/iter. Total: 0.0881 s/iter. ETA=0:00:20\n[02/13 04:58:09 d2.evaluation.evaluator]: Inference done 4824/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0362 s/iter. Total: 0.0881 s/iter. ETA=0:00:15\n[02/13 04:58:14 d2.evaluation.evaluator]: Inference done 4881/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0361 s/iter. Total: 0.0881 s/iter. ETA=0:00:10\n[02/13 04:58:19 d2.evaluation.evaluator]: Inference done 4935/5000. Dataloading: 0.0021 s/iter. Inference: 0.0498 s/iter. Eval: 0.0362 s/iter. Total: 0.0882 s/iter. ETA=0:00:05\n[02/13 04:58:25 d2.evaluation.evaluator]: Inference done 4992/5000. Dataloading: 0.0021 s/iter. Inference: 0.0499 s/iter. Eval: 0.0361 s/iter. Total: 0.0882 s/iter. ETA=0:00:00\n[02/13 04:58:25 d2.evaluation.evaluator]: Total inference time: 0:07:20.479637 (0.088184 s / iter per device, on 1 devices)\n[02/13 04:58:25 d2.evaluation.evaluator]: Total inference pure compute time: 0:04:09 (0.049895 s / iter per device, on 1 devices)\n[02/13 04:58:30 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n[02/13 04:58:30 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n[02/13 04:58:33 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\nLoading and preparing results...\nDONE (t=0.40s)\ncreating index...\nindex created!\n[02/13 04:58:34 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n[02/13 04:58:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 11.37 seconds.\n[02/13 04:58:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n[02/13 04:58:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 1.54 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.498\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.318\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.360\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.273\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.448\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.306\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.499\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.569\n[02/13 04:58:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 29.897 | 49.818 | 31.757 | 19.091 | 33.007 | 35.992 |\n[02/13 04:58:47 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n| category      | AP     | category     | AP     | category       | AP     |\n|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n| person        | 47.838 | bicycle      | 23.478 | car            | 35.332 |\n| motorcycle    | 32.072 | airplane     | 41.206 | bus            | 51.420 |\n| train         | 49.491 | truck        | 26.024 | boat           | 18.960 |\n| traffic light | 19.952 | fire hydrant | 59.531 | stop sign      | 54.634 |\n| parking meter | 34.804 | bench        | 17.211 | bird           | 24.288 |\n| cat           | 40.017 | dog          | 36.215 | horse          | 41.811 |\n| sheep         | 24.404 | cow          | 38.517 | elephant       | 47.882 |\n| bear          | 43.047 | zebra        | 55.295 | giraffe        | 53.434 |\n| backpack      | 4.107  | umbrella     | 25.139 | handbag        | 9.306  |\n| tie           | 27.322 | suitcase     | 21.393 | frisbee        | 55.385 |\n| skis          | 18.424 | snowboard    | 27.931 | sports ball    | 40.706 |\n| kite          | 30.145 | baseball bat | 21.415 | baseball glove | 24.885 |\n| skateboard    | 35.433 | surfboard    | 24.567 | tennis racket  | 35.924 |\n| bottle        | 32.441 | wine glass   | 29.303 | cup            | 34.689 |\n| fork          | 17.648 | knife        | 6.489  | spoon          | 11.327 |\n| bowl          | 32.606 | banana       | 14.851 | apple          | 14.518 |\n| sandwich      | 21.648 | orange       | 23.322 | broccoli       | 17.270 |\n| carrot        | 17.806 | hot dog      | 21.116 | pizza          | 41.106 |\n| donut         | 37.720 | cake         | 22.721 | chair          | 19.240 |\n| couch         | 28.865 | potted plant | 22.025 | bed            | 26.000 |\n| dining table  | 19.767 | toilet       | 47.664 | tv             | 39.008 |\n| laptop        | 42.352 | mouse        | 53.223 | remote         | 16.222 |\n| keyboard      | 36.314 | cell phone   | 20.834 | microwave      | 42.930 |\n| oven          | 22.920 | toaster      | 32.372 | sink           | 29.007 |\n| refrigerator  | 39.891 | book         | 8.055  | clock          | 40.063 |\n| vase          | 31.166 | scissors     | 11.895 | teddy bear     | 29.199 |\n| hair drier    | 2.024  | toothbrush   | 15.190 |                |        |\nLoading and preparing results...\nDONE (t=4.31s)\ncreating index...\nindex created!\n[02/13 04:59:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n[02/13 04:59:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 14.30 seconds.\n[02/13 04:59:16 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n[02/13 04:59:17 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 1.34 seconds.\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.471\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.303\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.144\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.312\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.398\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.267\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.426\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.481\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n[02/13 04:59:18 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 28.662 | 47.121 | 30.279 | 14.394 | 31.241 | 39.792 |\n[02/13 04:59:18 d2.evaluation.coco_evaluation]: Per-category segm AP: \n| category      | AP     | category     | AP     | category       | AP     |\n|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n| person        | 41.252 | bicycle      | 11.980 | car            | 33.768 |\n| motorcycle    | 25.730 | airplane     | 35.403 | bus            | 53.283 |\n| train         | 52.598 | truck        | 27.219 | boat           | 17.898 |\n| traffic light | 20.063 | fire hydrant | 58.757 | stop sign      | 56.504 |\n| parking meter | 39.879 | bench        | 13.094 | bird           | 18.633 |\n| cat           | 44.937 | dog          | 39.134 | horse          | 32.426 |\n| sheep         | 22.079 | cow          | 34.962 | elephant       | 43.845 |\n| bear          | 43.428 | zebra        | 50.255 | giraffe        | 42.791 |\n| backpack      | 3.070  | umbrella     | 34.073 | handbag        | 8.804  |\n| tie           | 25.973 | suitcase     | 23.217 | frisbee        | 55.818 |\n| skis          | 1.953  | snowboard    | 16.213 | sports ball    | 41.947 |\n| kite          | 22.435 | baseball bat | 21.437 | baseball glove | 29.435 |\n| skateboard    | 20.624 | surfboard    | 20.775 | tennis racket  | 47.517 |\n| bottle        | 33.193 | wine glass   | 27.668 | cup            | 35.272 |\n| fork          | 5.703  | knife        | 2.179  | spoon          | 8.625  |\n| bowl          | 32.855 | banana       | 11.942 | apple          | 15.228 |\n| sandwich      | 25.858 | orange       | 24.107 | broccoli       | 17.538 |\n| carrot        | 16.124 | hot dog      | 21.730 | pizza          | 41.892 |\n| donut         | 43.238 | cake         | 24.126 | chair          | 11.464 |\n| couch         | 25.759 | potted plant | 20.276 | bed            | 20.477 |\n| dining table  | 11.458 | toilet       | 49.441 | tv             | 43.849 |\n| laptop        | 43.364 | mouse        | 53.676 | remote         | 17.722 |\n| keyboard      | 39.225 | cell phone   | 26.123 | microwave      | 47.567 |\n| oven          | 23.584 | toaster      | 37.491 | sink           | 29.526 |\n| refrigerator  | 40.803 | book         | 6.693  | clock          | 43.155 |\n| vase          | 30.156 | scissors     | 10.523 | teddy bear     | 30.139 |\n| hair drier    | 0.187  | toothbrush   | 9.801  |                |        |\n\n\nOrderedDict([('bbox',\n              {'AP': 29.89689017794718,\n               'AP50': 49.81768645224712,\n               'AP75': 31.75684033977177,\n               'APs': 19.091146863259823,\n               'APm': 33.00671209183804,\n               'APl': 35.99166344401009,\n               'AP-person': 47.837560337143984,\n               'AP-bicycle': 23.47814560773066,\n               'AP-car': 35.332145066511,\n               'AP-motorcycle': 32.07172272587435,\n               'AP-airplane': 41.20588239061185,\n               'AP-bus': 51.420009438029666,\n               'AP-train': 49.49114357900364,\n               'AP-truck': 26.023794935388324,\n               'AP-boat': 18.9602461116214,\n               'AP-traffic light': 19.9521983797877,\n               'AP-fire hydrant': 59.53117091427524,\n               'AP-stop sign': 54.63431579194609,\n               'AP-parking meter': 34.803858333145854,\n               'AP-bench': 17.210808241735307,\n               'AP-bird': 24.287728734734667,\n               'AP-cat': 40.01669713689599,\n               'AP-dog': 36.21485337000998,\n               'AP-horse': 41.811047625435855,\n               'AP-sheep': 24.40385172034866,\n               'AP-cow': 38.51727158084108,\n               'AP-elephant': 47.882326630736515,\n               'AP-bear': 43.046572215754935,\n               'AP-zebra': 55.29518540456856,\n               'AP-giraffe': 53.43429768524197,\n               'AP-backpack': 4.10653831430483,\n               'AP-umbrella': 25.13944697585527,\n               'AP-handbag': 9.306254822354724,\n               'AP-tie': 27.321944146697817,\n               'AP-suitcase': 21.393099020425794,\n               'AP-frisbee': 55.38519299029526,\n               'AP-skis': 18.424028799910637,\n               'AP-snowboard': 27.93062121315892,\n               'AP-sports ball': 40.70572939438888,\n               'AP-kite': 30.1448409925177,\n               'AP-baseball bat': 21.41505682963772,\n               'AP-baseball glove': 24.88477652822228,\n               'AP-skateboard': 35.43250731045371,\n               'AP-surfboard': 24.56681960929748,\n               'AP-tennis racket': 35.924037712245145,\n               'AP-bottle': 32.440957179199856,\n               'AP-wine glass': 29.30315839934765,\n               'AP-cup': 34.688662676350305,\n               'AP-fork': 17.648460196785454,\n               'AP-knife': 6.489014035767637,\n               'AP-spoon': 11.326758827226104,\n               'AP-bowl': 32.60595516623553,\n               'AP-banana': 14.850983715257158,\n               'AP-apple': 14.518068381401013,\n               'AP-sandwich': 21.648061518774306,\n               'AP-orange': 23.321788766204204,\n               'AP-broccoli': 17.27036935565236,\n               'AP-carrot': 17.805870075670093,\n               'AP-hot dog': 21.11634657823014,\n               'AP-pizza': 41.10561040833102,\n               'AP-donut': 37.71982162773581,\n               'AP-cake': 22.721214765344666,\n               'AP-chair': 19.24040698001655,\n               'AP-couch': 28.864502321656886,\n               'AP-potted plant': 22.02453853016209,\n               'AP-bed': 25.999786248222506,\n               'AP-dining table': 19.766840271606974,\n               'AP-toilet': 47.663609421333156,\n               'AP-tv': 39.00821630716829,\n               'AP-laptop': 42.351954633553596,\n               'AP-mouse': 53.2230527938069,\n               'AP-remote': 16.222228003183716,\n               'AP-keyboard': 36.31434769943729,\n               'AP-cell phone': 20.834159106237465,\n               'AP-microwave': 42.93029819699271,\n               'AP-oven': 22.920311025094627,\n               'AP-toaster': 32.37201100567437,\n               'AP-sink': 29.00693619350771,\n               'AP-refrigerator': 39.89130619477842,\n               'AP-book': 8.054941266635423,\n               'AP-clock': 40.06277695071099,\n               'AP-vase': 31.16619906385666,\n               'AP-scissors': 11.895045305984278,\n               'AP-teddy bear': 29.199265484243536,\n               'AP-hair drier': 2.023946580704582,\n               'AP-toothbrush': 15.189704360553186}),\n             ('segm',\n              {'AP': 28.661839929847766,\n               'AP50': 47.12099506785067,\n               'AP75': 30.27852102510463,\n               'APs': 14.393543998149433,\n               'APm': 31.240826986233245,\n               'APl': 39.79242278943046,\n               'AP-person': 41.25247349406813,\n               'AP-bicycle': 11.980056444623962,\n               'AP-car': 33.767649311682966,\n               'AP-motorcycle': 25.72961219232462,\n               'AP-airplane': 35.40335889956266,\n               'AP-bus': 53.28296316844562,\n               'AP-train': 52.59815704465161,\n               'AP-truck': 27.219112196412286,\n               'AP-boat': 17.898113630853008,\n               'AP-traffic light': 20.063050134999397,\n               'AP-fire hydrant': 58.75691495361581,\n               'AP-stop sign': 56.50424204130755,\n               'AP-parking meter': 39.87929817341212,\n               'AP-bench': 13.093799678463267,\n               'AP-bird': 18.633126014944484,\n               'AP-cat': 44.93671365428673,\n               'AP-dog': 39.134465086431405,\n               'AP-horse': 32.42593406670286,\n               'AP-sheep': 22.07929100248326,\n               'AP-cow': 34.96219015157876,\n               'AP-elephant': 43.84481472494,\n               'AP-bear': 43.42779378594565,\n               'AP-zebra': 50.254983961962374,\n               'AP-giraffe': 42.791213855527516,\n               'AP-backpack': 3.069830116972104,\n               'AP-umbrella': 34.07250440438077,\n               'AP-handbag': 8.8037834537288,\n               'AP-tie': 25.972833559829827,\n               'AP-suitcase': 23.21712969553343,\n               'AP-frisbee': 55.81774651489321,\n               'AP-skis': 1.9534711443298516,\n               'AP-snowboard': 16.212898925517894,\n               'AP-sports ball': 41.946609283399354,\n               'AP-kite': 22.434833850648612,\n               'AP-baseball bat': 21.437251665137875,\n               'AP-baseball glove': 29.434721623759575,\n               'AP-skateboard': 20.62420030530276,\n               'AP-surfboard': 20.7746697482192,\n               'AP-tennis racket': 47.5171436088261,\n               'AP-bottle': 33.193296640795076,\n               'AP-wine glass': 27.66844317937192,\n               'AP-cup': 35.27153103017214,\n               'AP-fork': 5.702777294880277,\n               'AP-knife': 2.1788346592089,\n               'AP-spoon': 8.624995301616934,\n               'AP-bowl': 32.85509874871778,\n               'AP-banana': 11.94177658970979,\n               'AP-apple': 15.22818936776687,\n               'AP-sandwich': 25.858064250410823,\n               'AP-orange': 24.107266334795867,\n               'AP-broccoli': 17.53776752525587,\n               'AP-carrot': 16.12362682209143,\n               'AP-hot dog': 21.730336517936593,\n               'AP-pizza': 41.89164234108817,\n               'AP-donut': 43.23758089806856,\n               'AP-cake': 24.12553215716846,\n               'AP-chair': 11.464367279360923,\n               'AP-couch': 25.75895204314586,\n               'AP-potted plant': 20.276217596489406,\n               'AP-bed': 20.476727012227187,\n               'AP-dining table': 11.45829001940977,\n               'AP-toilet': 49.44101117760728,\n               'AP-tv': 43.84883647370952,\n               'AP-laptop': 43.36437965344922,\n               'AP-mouse': 53.67643707607608,\n               'AP-remote': 17.722464443723158,\n               'AP-keyboard': 39.22521398632689,\n               'AP-cell phone': 26.122823527455523,\n               'AP-microwave': 47.56700008587836,\n               'AP-oven': 23.584145282510224,\n               'AP-toaster': 37.49139238248149,\n               'AP-sink': 29.526361068986912,\n               'AP-refrigerator': 40.80288598047604,\n               'AP-book': 6.693159472760312,\n               'AP-clock': 43.15466937561561,\n               'AP-vase': 30.156106300249856,\n               'AP-scissors': 10.522703248492874,\n               'AP-teddy bear': 30.139171881692267,\n               'AP-hair drier': 0.1872745414076291,\n               'AP-toothbrush': 9.80088924952792})])\n\n\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set the testing threshold for this model\ncfg.DATASETS.TEST = (\"coco_val_data\",)\npredictor = DefaultPredictor(cfg)\n\n[02/13 05:02:50 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /root/2024winter/DL_tutorial/posts/outputs/model_final.pth ...\n\n\n\nfrom detectron2.utils.visualizer import ColorMode\n\nfor d in random.sample(val_dataset_dicts, 3):\n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                  metadata=val_metadata,\n                  scale=0.8,\n                  instance_mode=ColorMode.IMAGE_BW) # remove the colors of unsegmented pixels\nv = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.figure(figsize=(5, 5))\nplt.imshow(vis.get_image()[:, :, ::-1])\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\n\n!unzip /root/2024winter/DL_tutorial/posts/instance_custom_annotation.zip -d /root/2024winter/DL_tutorial/posts/coco\n\nArchive:  /root/2024winter/DL_tutorial/posts/instance_custom_annotation.zip\n  inflating: /root/2024winter/DL_tutorial/posts/coco/__MACOSX/._annotations  \n  inflating: /root/2024winter/DL_tutorial/posts/coco/annotations/instances_custom.json  \n  inflating: /root/2024winter/DL_tutorial/posts/coco/__MACOSX/annotations/._instances_custom.json"
  },
  {
    "objectID": "posts/CV_Instance_Segmentation.html#코드-추가-작성-예정-.-.-.",
    "href": "posts/CV_Instance_Segmentation.html#코드-추가-작성-예정-.-.-.",
    "title": "CV_Instance_Segmentation_0",
    "section": "코드 추가 작성 예정 . . .",
    "text": "코드 추가 작성 예정 . . ."
  },
  {
    "objectID": "posts/VGGNet.html",
    "href": "posts/VGGNet.html",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet 구조 살펴보기\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#vggnet",
    "href": "posts/VGGNet.html#vggnet",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet 구조 살펴보기\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "href": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "title": "VGGNet",
    "section": "Step 1 : Load libraries & Datasets",
    "text": "Step 1 : Load libraries & Datasets\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-2-data-preprocessing",
    "href": "posts/VGGNet.html#step-2-data-preprocessing",
    "title": "VGGNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\n불러온 이미지의 증강을 통해 학습 정확도를 향상시키도록 합니다.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform,\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ntrain_img.data.shape\n\n(50000, 32, 32, 3)"
  },
  {
    "objectID": "posts/VGGNet.html#step-3-set-hyperparameters",
    "href": "posts/VGGNet.html#step-3-set-hyperparameters",
    "title": "VGGNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 32\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/VGGNet.html#step-4-create-dataloader",
    "href": "posts/VGGNet.html#step-4-create-dataloader",
    "title": "VGGNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\ntrain_loader = DataLoader(train_img, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img, batch_size = batch_sizes, shuffle = False)"
  },
  {
    "objectID": "posts/VGGNet.html#step-5-set-network-structure",
    "href": "posts/VGGNet.html#step-5-set-network-structure",
    "title": "VGGNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\n# Model\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 360),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(360, 100),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(100, 10),\n        )\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),  # 추가\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n                \n        return nn.Sequential(*layers)"
  },
  {
    "objectID": "posts/VGGNet.html#step-6-create-model-instance",
    "href": "posts/VGGNet.html#step-6-create-model-instance",
    "title": "VGGNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\nmodel = VGG('VGG16').to(device)\nprint(model)\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=360, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=360, out_features=100, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=100, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/VGGNet.html#step-7-model-compile",
    "href": "posts/VGGNet.html#step-7-model-compile",
    "title": "VGGNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
  },
  {
    "objectID": "posts/VGGNet.html#step-8-set-train-loop",
    "href": "posts/VGGNet.html#step-8-set-train-loop",
    "title": "VGGNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}   [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/VGGNet.html#step-9-set-test-loop",
    "href": "posts/VGGNet.html#step-9-set-test-loop",
    "title": "VGGNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y  = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-10-run-model",
    "href": "posts/VGGNet.html#step-10-run-model",
    "title": "VGGNet",
    "section": "Step 10 : Run model",
    "text": "Step 10 : Run model\n\nfor i in range(epochs):\n    print(f\"Epoch {i+1} \\n---------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n---------------------------\nloss: 2.336054   [    0]/50000\nloss: 2.201652   [ 3200]/50000\nloss: 1.913308   [ 6400]/50000\nloss: 1.833156   [ 9600]/50000\nloss: 2.025965   [12800]/50000\nloss: 1.458568   [16000]/50000\nloss: 1.454589   [19200]/50000\nloss: 1.586300   [22400]/50000\nloss: 1.777550   [25600]/50000\nloss: 1.902973   [28800]/50000\nloss: 1.534656   [32000]/50000\nloss: 1.808721   [35200]/50000\nloss: 1.449842   [38400]/50000\nloss: 1.357022   [41600]/50000\nloss: 1.378457   [44800]/50000\nloss: 1.557457   [48000]/50000\nTest Error: \n Accuracy: 50.6%, Avg loss: 1.338251\n\nEpoch 2 \n---------------------------\nloss: 1.564054   [    0]/50000\nloss: 1.571676   [ 3200]/50000\nloss: 1.477239   [ 6400]/50000\nloss: 1.520172   [ 9600]/50000\nloss: 1.256163   [12800]/50000\nloss: 1.066607   [16000]/50000\nloss: 1.460073   [19200]/50000\nloss: 1.077838   [22400]/50000\nloss: 1.215548   [25600]/50000\nloss: 0.884830   [28800]/50000\nloss: 1.028723   [32000]/50000\nloss: 1.288996   [35200]/50000\nloss: 1.299563   [38400]/50000\nloss: 0.939404   [41600]/50000\nloss: 0.955451   [44800]/50000\nloss: 1.098657   [48000]/50000\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.923354\n\nEpoch 3 \n---------------------------\nloss: 0.796024   [    0]/50000\nloss: 0.677388   [ 3200]/50000\nloss: 0.617442   [ 6400]/50000\nloss: 1.173936   [ 9600]/50000\nloss: 0.786573   [12800]/50000\nloss: 0.798586   [16000]/50000\nloss: 1.181702   [19200]/50000\nloss: 0.897227   [22400]/50000\nloss: 0.735924   [25600]/50000\nloss: 1.028793   [28800]/50000\nloss: 0.834691   [32000]/50000\nloss: 1.081767   [35200]/50000\nloss: 0.828031   [38400]/50000\nloss: 1.046338   [41600]/50000\nloss: 0.828228   [44800]/50000\nloss: 1.146716   [48000]/50000\nTest Error: \n Accuracy: 70.4%, Avg loss: 0.880181\n\nEpoch 4 \n---------------------------\nloss: 0.617879   [    0]/50000\nloss: 0.876245   [ 3200]/50000\nloss: 0.673582   [ 6400]/50000\nloss: 0.556679   [ 9600]/50000\nloss: 0.699025   [12800]/50000\nloss: 1.006697   [16000]/50000\nloss: 0.683750   [19200]/50000\nloss: 1.277821   [22400]/50000\nloss: 0.562071   [25600]/50000\nloss: 0.686587   [28800]/50000\nloss: 0.873322   [32000]/50000\nloss: 0.719097   [35200]/50000\nloss: 0.457578   [38400]/50000\nloss: 0.514047   [41600]/50000\nloss: 0.729195   [44800]/50000\nloss: 0.858265   [48000]/50000\nTest Error: \n Accuracy: 75.3%, Avg loss: 0.712957\n\nEpoch 5 \n---------------------------\nloss: 0.855375   [    0]/50000\nloss: 0.854320   [ 3200]/50000\nloss: 0.695397   [ 6400]/50000\nloss: 0.525759   [ 9600]/50000\nloss: 0.381186   [12800]/50000\nloss: 0.587416   [16000]/50000\nloss: 0.511339   [19200]/50000\nloss: 1.319725   [22400]/50000\nloss: 0.649993   [25600]/50000\nloss: 0.508207   [28800]/50000\nloss: 0.585140   [32000]/50000\nloss: 0.794928   [35200]/50000\nloss: 0.799448   [38400]/50000\nloss: 0.417046   [41600]/50000\nloss: 0.498251   [44800]/50000\nloss: 0.779942   [48000]/50000\nTest Error: \n Accuracy: 76.6%, Avg loss: 0.682496\n\nEpoch 6 \n---------------------------\nloss: 0.719160   [    0]/50000\nloss: 0.627115   [ 3200]/50000\nloss: 0.255042   [ 6400]/50000\nloss: 0.400026   [ 9600]/50000\nloss: 0.737379   [12800]/50000\nloss: 0.741243   [16000]/50000\nloss: 0.726986   [19200]/50000\nloss: 0.266388   [22400]/50000\nloss: 0.633677   [25600]/50000\nloss: 0.482972   [28800]/50000\nloss: 0.444857   [32000]/50000\nloss: 0.513320   [35200]/50000\nloss: 0.529961   [38400]/50000\nloss: 0.784853   [41600]/50000\nloss: 0.560646   [44800]/50000\nloss: 0.426722   [48000]/50000\nTest Error: \n Accuracy: 75.4%, Avg loss: 0.737327\n\nEpoch 7 \n---------------------------\nloss: 0.456941   [    0]/50000\nloss: 0.552954   [ 3200]/50000\nloss: 0.588921   [ 6400]/50000\nloss: 0.359172   [ 9600]/50000\nloss: 0.380740   [12800]/50000\nloss: 0.230270   [16000]/50000\nloss: 0.544868   [19200]/50000\nloss: 0.470449   [22400]/50000\nloss: 0.716484   [25600]/50000\nloss: 0.427520   [28800]/50000\nloss: 0.485696   [32000]/50000\nloss: 0.250514   [35200]/50000\nloss: 0.619605   [38400]/50000\nloss: 0.534625   [41600]/50000\nloss: 0.294415   [44800]/50000\nloss: 0.676517   [48000]/50000\nTest Error: \n Accuracy: 81.8%, Avg loss: 0.560385\n\nEpoch 8 \n---------------------------\nloss: 0.584514   [    0]/50000\nloss: 0.433411   [ 3200]/50000\nloss: 0.360651   [ 6400]/50000\nloss: 0.707992   [ 9600]/50000\nloss: 0.449344   [12800]/50000\nloss: 0.380623   [16000]/50000\nloss: 0.333079   [19200]/50000\nloss: 0.420316   [22400]/50000\nloss: 0.414326   [25600]/50000\nloss: 0.539709   [28800]/50000\nloss: 0.425368   [32000]/50000\nloss: 0.610167   [35200]/50000\nloss: 0.427243   [38400]/50000\nloss: 0.787724   [41600]/50000\nloss: 0.561038   [44800]/50000\nloss: 0.456995   [48000]/50000\nTest Error: \n Accuracy: 81.5%, Avg loss: 0.566010\n\nEpoch 9 \n---------------------------\nloss: 0.383341   [    0]/50000\nloss: 0.381802   [ 3200]/50000\nloss: 0.258570   [ 6400]/50000\nloss: 0.421782   [ 9600]/50000\nloss: 0.455991   [12800]/50000\nloss: 0.531303   [16000]/50000\nloss: 0.758386   [19200]/50000\nloss: 0.285010   [22400]/50000\nloss: 0.314713   [25600]/50000\nloss: 0.419822   [28800]/50000\nloss: 0.278820   [32000]/50000\nloss: 0.553399   [35200]/50000\nloss: 0.416081   [38400]/50000\nloss: 0.365547   [41600]/50000\nloss: 0.686296   [44800]/50000\nloss: 0.814931   [48000]/50000\nTest Error: \n Accuracy: 83.5%, Avg loss: 0.502169\n\nEpoch 10 \n---------------------------\nloss: 0.275047   [    0]/50000\nloss: 0.573386   [ 3200]/50000\nloss: 0.737843   [ 6400]/50000\nloss: 0.478916   [ 9600]/50000\nloss: 0.429536   [12800]/50000\nloss: 0.238580   [16000]/50000\nloss: 0.406505   [19200]/50000\nloss: 0.228436   [22400]/50000\nloss: 0.370529   [25600]/50000\nloss: 0.344406   [28800]/50000\nloss: 0.301163   [32000]/50000\nloss: 0.257651   [35200]/50000\nloss: 0.833092   [38400]/50000\nloss: 0.897587   [41600]/50000\nloss: 0.397286   [44800]/50000\nloss: 0.637342   [48000]/50000\nTest Error: \n Accuracy: 84.8%, Avg loss: 0.467348\n\nDone!"
  },
  {
    "objectID": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "href": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "title": "VGGNet",
    "section": "## CIFAR Classifier(Pretrained VGGNet)",
    "text": "## CIFAR Classifier(Pretrained VGGNet)\nImageNet 데이터로 학습한 VGGNet을 사용하여 주어진 데이터 셋에서 사용할 수 있도록 Fine tuning 해봅니다."
  },
  {
    "objectID": "posts/CV_Semantic_Segmentation.html",
    "href": "posts/CV_Semantic_Segmentation.html",
    "title": "CV_Semantic_Segmentaion_0",
    "section": "",
    "text": "# !jar xvf pavement_crack.zip\n\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nimport torchvision.transforms as transforms\n\nimport os, zipfile\nimport tqdm\nimport random\nimport shutil\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport scipy.ndimage as ndimage\nimport matplotlib.pyplot as plt\n\n\n# 재현성을 위한 랜덤시드 고정\nrandom_seed = 2024\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\n\n\n\n# 클래스 정의\nclass CrackDatasets(Dataset):\n    def __init__(self, img_dir, mask_dir, img_transform, mask_transform):\n        self.img_dir = img_dir\n        self.img_transform = img_transform\n        self.img_files = []\n        self.mask_dir = mask_dir\n        self.mask_transform = mask_transform\n        self.mask_files = []\n        self.seed = np.random.randint(2024)\n\n        for img_name in os.listdir(self.img_dir):\n            if img_name.split('.')[-1] in ('png', 'jpg'):\n                self.img_files.append(os.path.join(self.img_dir, img_name))\n                self.mask_files.append(os.path.join(self.mask_dir, img_name))\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_files[i])\n        if self.img_transform is not None:\n            random.seed(self.seed)\n            img = self.img_transform(img)\n\n        mask = Image.open(self.mask_files[i]).convert('L')\n        if self.mask_transform is not None:\n            mask = self.mask_transform(mask)\n\n        return img, mask\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\n\n\ndef dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float=1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\ndef iou(y_true, y_pred, epsilon: float = 1e-6):\n    intersection = (y_true * y_pred).sum()\n    union = y_true.sum() + y_pred.sum() - intersection\n    return (intersection + epsilon) / (union + epsilon)\n\n\n\n\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n\n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                             nn.ReLU(),\n                             nn.BatchNorm2d(num_features=out_channels),\n                             nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                             nn.ReLU(),\n                             nn.BatchNorm2d(num_features=out_channels))\n        return block\n\n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) #[-1, 64, 256, 256]\n        contracting_12_out = self.contracting_12(contracting_11_out) #[-1, 64, 128, 128]\n\n        contracting_21_out = self.contracting_21(contracting_12_out) #[-1, 128, 128, 128]\n        contracting_22_out = self.contracting_22(contracting_21_out) #[-1, 128, 64, 64]\n\n        contracting_31_out = self.contracting_31(contracting_22_out) #[-1, 256, 64, 64]\n        contracting_32_out = self.contracting_32(contracting_31_out) #[-1, 256, 32, 32]\n\n        contracting_41_out = self.contracting_41(contracting_32_out) #[-1, 512, 32, 32]\n        contracting_42_out = self.contracting_42(contracting_41_out) #[-1, 512, 16, 16]\n\n        middle_out = self.middle(contracting_42_out) #[-1, 512, 32, 32]\n\n        expansive_11_out = self.expansive_11(middle_out) #[-1, 512, 32, 32]\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1))\n        # [-1, 1024, 32, 32] -&gt; # [-1, 512, 32, 32]\n        \n        expansive_21_out = self.expansive_21(expansive_12_out) #[-1, 256, 64, 64]\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1))\n        # [-1, 512, 64, 64] -&gt; # [-1, 256, 64, 64]\n        \n        expansive_31_out = self.expansive_31(expansive_22_out) #[-1, 128, 128, 128]\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1))\n        # [-1, 256, 128, 128] -&gt; # [-1, 128, 128, 128]\n        \n        expansive_41_out = self.expansive_41(expansive_32_out)\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out,contracting_11_out), dim=1))\n        #[-1, 128, 256, 256] -&gt; # [1, 64, 256,256]\n        \n        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n        return output_out\n\n\n\n\nclass AverageMeter(object):\n    #Computes and stores the average and current value\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(optimizer, epoch, lr):\n    #Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n    lr = lr * (0.1 **(epoch//30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef find_latest_model_path(dir):\n    model_paths = []\n    epochs = []\n    for path in Path(dir).glob('*.pth'):\n        if 'epoch' not in path.stem:\n            continue\n        model_paths.append(path)\n        parts = path.stem.split('_')\n        epoch = int(parts[-1])\n        epochs.append(epoch)\n\n    if len(epochs) &gt; 0:\n        epochs = np.array(epochs)\n        max_idx = np.argmax(epochs)\n        return model_paths[max_idx]\n    else:\n        return None\n\ndef save_check_point(state, is_best, file_name='checkpoint.pth'):\n    torch.save(state, file_name)\n    if is_best:\n        shutil.copy(file_name, 'model_best.pth')\n\ndef calc_crack_pixel_weight(mask_dir):\n    avg_w = 0.0\n    n_files = 0\n    for path in Path(mask_dir).glob('*.*'):\n        n_files += 1\n        m = ndimage.imread(path)\n        ncrack = np.sum((m&gt;0)[:])\n        w = float(ncrack) / (m.shape[0]*m.shape[1])\n        avg_w = avg_w + (1-w)\n\n    return avg_w / (1.0 - avg_w)\n\n\n\n\n\n\ndef train(train_loader, model, criterion, optimizer, valid_loader, model_dir, n_epoch, batch_size, lr, device):\n\n    latest_model_path = find_latest_model_path(model_dir)\n    best_model_path = os.path.join(*[model_dir, 'model_best.pth'])\n\n    if latest_model_path is not None:\n        state = torch.load(latest_model_path)\n        epoch = state['epoch']\n        model.load_state_dict(state['model'])\n        epoch = epoch\n\n        # if latest model path does exist, best_model_path should exists as well\n        assert Path(best_model_path).exists() == True, f'best model path {best_model_path} doest not exist'\n        # load the min loss so far\n        best_state = torch.load(latest_model_path)\n        min_val_los = best_state['valid_loss']\n\n        print(f'Restored model at epoch {epoch}. Min Validation loss so far is: {min_val_los}')\n        epoch += 1\n        print(f'Started training model from epoch {epoch}')\n    else:\n        print('Started training model from epoch 0')\n        epoch = 0\n        min_val_los = 9999\n\n    valid_losses = []\n    for epoch in range(epoch, n_epoch):\n        adjust_learning_rate(optimizer, epoch, lr)\n\n        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n        tq.set_description(f'Epoch {epoch}')\n\n        losses = AverageMeter()\n        t_iou = 0\n        t_dice = 0\n\n        model.train()\n        for i, (input, target) in enumerate(train_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            masks_pred = model(input_var)\n            pred = F.sigmoid(masks_pred)\n            target_mask = target_var\n\n            pred[pred&gt;0.5] = 1\n            pred[pred&lt;=0.5] = 0\n            target_mask[target_mask&gt;0.5] = 1\n            target_mask[target_mask&lt;=0.5] = 0\n\n            masks_prob_flat = masks_pred.view(-1)\n            true_masks_flat = target_var.view(-1)\n\n            loss = criterion(masks_prob_flat, true_masks_flat)\n            losses.update(loss)\n            tq.set_postfix(loss='{:.5f}'.format(losses.avg))\n            tq.update(batch_size)\n\n            t_dice += dice_coeff(pred, target_mask)\n            t_iou += iou(pred, target_mask)\n\n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f'train miou : {t_iou/len(train_loader):.5f} train dice score : {t_dice/len(train_loader):.5f}')\n        valid_metrics = valid(model, valid_loader, criterion)\n        valid_loss = valid_metrics['valid_loss']\n        valid_dice = valid_metrics['v_dice']\n        valid_iou = valid_metrics['v_iou']\n        valid_losses.append(valid_loss)\n        print(f'valid_loss = {valid_loss:.5f}')\n        print(f'valid miou : {valid_iou/len(valid_loader):.5f} valid dice score : {valid_dice/len(valid_loader):.5f}')\n        tq.close()\n\n        # save the model of the current epoch\n        epoch_model_path = os.path.join(*[model_dir, f'model_epoch_{epoch}.pth'])\n        torch.save({\n            'model': model.state_dict(),\n            'epoch': epoch,\n            'valid_loss': valid_loss,\n            'train_loss': losses.avg\n        }, epoch_model_path)\n\n        if valid_loss &lt; min_val_los:\n            min_val_los = valid_loss\n\n            torch.save({\n                'model' : model.state_dict(),\n                'epoch' : epoch,\n                'valid_loss' : valid_loss,\n                'train_loss' : losses.avg\n            }, best_model_path)\n\n\ndef valid(model, val_loader, criterion):\n    losses = AverageMeter()\n    v_iou = 0\n    v_dice = 0\n    model.eval()\n    with torch.no_grad():\n\n        for i, (input, target) in enumerate(val_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            output = model(input_var)\n            loss = criterion(output, target_var)\n\n            losses.update(loss.item(), input_var.size(0))\n\n            pred = F.sigmoid(output)\n            target_mask = target_var\n            pred[pred&gt;0.5] = 1\n            pred[pred&lt;=0.5] = 0\n            target_mask[target_mask&gt;0.5] = 1\n            target_mask[target_mask&lt;=0.5] = 0\n\n            v_dice += dice_coeff(pred, target_mask)\n            v_iou = iou(pred, target_mask)\n\n    return {'valid_loss': losses.avg, 'v_dice': v_dice, 'v_iou': v_iou}\n\n\n\n\n\n# model folder\nmodel_dir = '/root/2024winter/DL_tutorial/posts/model_weights'\nos.makedirs(model_dir, exist_ok=True)\n\n# data folder\ndata_dir = '/root/2024winter/DL_tutorial/posts/crack_segmentation_dataset/train'\nDIR_IMG = os.path.join(data_dir, 'images')\nDIR_MASK = os.path.join(data_dir, 'masks')\n\n# Device 할당\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlr = 0.001\nmomentum = 0.9\nweight_decay = 1e-4\nbatch_size = 8\nnum_workers = 8\nn_epoch = 10\n\n\n# 모델 할당\nmodel = UNet(num_classes=1)\n\n# Optimizer 정의\noptimizer = torch.optim.SGD(model.parameters(), lr,\n                           momentum=momentum,\n                           weight_decay=weight_decay)\n\n# 손실 함수 정의\ncriterion = nn.BCEWithLogitsLoss().to(device)\n\nchannel_means = [0.485, 0.456, 0.406]\nchannel_stds  = [0.229, 0.224, 0.224]\n\ntrain_tfms = transforms.Compose([transforms.ToTensor(),\n                                transforms.Resize(256),\n                                transforms.Normalize(channel_means, channel_stds)])\n\nval_tfms = transforms.Compose([transforms.ToTensor(),\n                              transforms.Resize(256),\n                              transforms.Normalize(channel_means, channel_stds)])\n\nmask_tfms = transforms.Compose([transforms.ToTensor(),\n                               transforms.Resize(256)])\n\ndataset = CrackDatasets(img_dir=DIR_IMG,\n                       img_transform=train_tfms,\n                       mask_dir=DIR_MASK,\n                       mask_transform=mask_tfms)\n\ntrain_size = int(0.85*len(dataset))\nvalid_size = len(dataset) - train_size\n\n# train / val 분할\ntrain_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n\n# 데이터로더\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers)\nvalid_loader = DataLoader(valid_dataset, batch_size, shuffle=False, num_workers=num_workers)\n\nmodel.to(device)\n\n#train\ntrain(train_loader, model, criterion, optimizer,\n      valid_loader, model_dir, n_epoch, batch_size, lr, device)\n\nStarted training model from epoch 0\ntrain miou : 0.16975 train dice score : 0.23192\nvalid_loss = 0.10602\nvalid miou : 0.00013 valid dice score : 0.21843\ntrain miou : 0.27445 train dice score : 0.34929\nvalid_loss = 0.09925\nvalid miou : 0.00038 valid dice score : 0.26572\ntrain miou : 0.32347 train dice score : 0.41108\nvalid_loss = 0.08654\nvalid miou : 0.00108 valid dice score : 0.37338\ntrain miou : 0.36215 train dice score : 0.44759\nvalid_loss = 0.08282\nvalid miou : 0.00079 valid dice score : 0.47123\ntrain miou : 0.38905 train dice score : 0.48338\nvalid_loss = 0.07262\nvalid miou : 0.00085 valid dice score : 0.49158\ntrain miou : 0.40778 train dice score : 0.50428\nvalid_loss = 0.07109\nvalid miou : 0.00097 valid dice score : 0.52032\ntrain miou : 0.42189 train dice score : 0.52392\nvalid_loss = 0.06942\nvalid miou : 0.00078 valid dice score : 0.54654\ntrain miou : 0.43467 train dice score : 0.53903\nvalid_loss = 0.06694\nvalid miou : 0.00077 valid dice score : 0.52760\ntrain miou : 0.44633 train dice score : 0.55094\nvalid_loss = 0.06965\nvalid miou : 0.00069 valid dice score : 0.55299\ntrain miou : 0.45546 train dice score : 0.55663\nvalid_loss = 0.06245\nvalid miou : 0.00043 valid dice score : 0.55632\n\n\nEpoch 0:   0%|                                                                                 | 0/8168 [00:00&lt;?, ?it/s]/root/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\nEpoch 0: 100%|███████████████████████████████████████████████████████▉| 8160/8168 [01:18&lt;00:00, 88.84it/s, loss=0.20300]Epoch 0: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:24&lt;00:00, 96.69it/s, loss=0.20300]\nEpoch 1: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:11&lt;00:00, 120.05it/s, loss=0.09282]Epoch 1: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:17&lt;00:00, 106.00it/s, loss=0.09282]\nEpoch 2: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:12&lt;00:00, 120.32it/s, loss=0.08378]Epoch 2: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:17&lt;00:00, 105.32it/s, loss=0.08378]\nEpoch 3: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:11&lt;00:00, 118.50it/s, loss=0.07736]Epoch 3: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:15&lt;00:00, 107.60it/s, loss=0.07736]\nEpoch 4: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:13&lt;00:00, 107.44it/s, loss=0.07263]Epoch 4: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.28it/s, loss=0.07263]\nEpoch 5: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:17&lt;00:00, 114.78it/s, loss=0.06974]Epoch 5: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:23&lt;00:00, 97.74it/s, loss=0.06974]\nEpoch 6: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:15&lt;00:00, 111.50it/s, loss=0.06713]Epoch 6: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:21&lt;00:00, 100.49it/s, loss=0.06713]\nEpoch 7: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:23&lt;00:00, 111.52it/s, loss=0.06493]Epoch 7: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:29&lt;00:00, 91.64it/s, loss=0.06493]\nEpoch 8: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:14&lt;00:00, 111.54it/s, loss=0.06300]Epoch 8: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.57it/s, loss=0.06300]\nEpoch 9: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:14&lt;00:00, 119.27it/s, loss=0.06161]Epoch 9: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.92it/s, loss=0.06161]\n\n\n\nfor images, masks in train_loader:\n    print(images.shape, masks.shape)\n    break\n\ntorch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n\n\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# 저장된 모델 웨이트로 경로 지정\nmodel_dir = '/root/2024winter/DL_tutorial/posts/model_weights'\ntitle=\"Loss Graph\"\n\nweights_paths = [path for path in Path(model_dir).glob('*.pth')]\nweights_paths = sorted(weights_paths)\nprint(weights_paths)\nepochs = []\ntr_losses = []\nvl_losses = []\n\nfor w_path in weights_paths:\n    if 'epoch' not in w_path.stem:\n        continue\n    # load the min loss so far\n    parts = w_path.stem.split('_')\n    epoch = int(parts[-1])\n    epochs.append(epoch)\n    state = torch.load(w_path)\n    val_los = state['valid_loss']\n    train_loss = float(state['train_loss'])\n    tr_losses.append(train_loss)\n    vl_losses.append(val_los)\n\nsorted_idx = np.argsort(epochs)\ntr_losses = [tr_losses[idx] for idx in sorted_idx]\nvl_losses = [vl_losses[idx] for idx in sorted_idx]\nplt.plot(tr_losses[1:], label='train_loss')\nplt.plot(vl_losses[1:], label='valid_loss')\nplt.title(title)\nplt.legend()\nplt.show()\n\n[PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_best.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_0.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_1.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_2.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_3.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_4.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_5.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_6.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_7.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_8.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_9.pth')]\n\n\n\n\n\n\n\n\n\n\nimport cv2\ndef evaluate(test_loader, model, output_path):\n    e_iou = 0\n    e_dice = 0\n    model.eval()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(test_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            output = model(input_var)\n            pred = F.sigmoid(output)\n            target_mask = target_var\n\n            pred[pred&gt;0.5]=1\n            pred[pred&lt;=0.5]=0\n            target_mask[target_mask&gt;0.5]=1\n            target_mask[target_mask&lt;=0.5]=0\n            \n            cv2.imwrite(f'{output_path}/predicts/{i}.png', tensor2image(pred))\n            cv2.imwrite(f'{output_path}/masks/{i}.png', tensor2image(target_mask))\n\n            e_dice += dice_coeff(pred, target_mask)\n            e_iou += iou(pred, target_mask)\n\n    print(f'test miou : {e_iou/len(test_loader):.5f} test dice score : {e_dice/len(test_loader):.5f}')\n\n\ndef denormalize(image):\n    #IMAGENET_MEAN, IMAGENET_STD = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n    #image = np.clip(255.0 * (image * IMAGENET_STD + IMAGENET_MEAN), 0, 255)\n    image = image * 255.0\n    return image\n\ndef tensor2image(tensor_list):\n    img = tensor_list[0].detach().cpu().numpy()\n    img = np.transpose(img, (1,2,0))\n    img = denormalize(img)\n    #img = img.astype('float32')\n    #img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    img = img.astype(np.uint8).copy()\n    return img\n\n\n# 테스트 데이터셋\ntest_data_dir = '/root/2024winter/DL_tutorial/posts/crack_segmentation_dataset/test'\ntest_img_path = os.path.join(test_data_dir, 'images')\ntest_mask_path = os.path.join(test_data_dir, 'masks')\n\ntest_tfms = transforms.Compose([transforms.ToTensor(),\n                               transforms.Normalize(channel_means, channel_stds)])\nmask_tfms = transforms.Compose([transforms.ToTensor()])\n\ntest_dataset = CrackDatasets(img_dir=test_img_path, img_transform=test_tfms,\n                            mask_dir=test_mask_path, mask_transform=mask_tfms)\ntest_loader = DataLoader(test_dataset, 1, shuffle=False, num_workers=num_workers)\n\n# 모델 할당\nmodel = UNet(num_classes=1)\n\nbest_model_path = os.path.join(*[model_dir, 'model_best.pth'])\nif best_model_path is not None:\n    state = torch.load(best_model_path)\n    epoch = state['epoch']\n    model.load_state_dict(state['model'])\nmodel.to(device)\n\nos.makedirs('/root/2024winter/DL_tutorial/posts/results/predicts', exist_ok=True)\nos.makedirs('/root/2024winter/DL_tutorial/posts/results/masks', exist_ok=True)\n\noutput_path = '/root/2024winter/DL_tutorial/posts/results'\nevaluate(test_loader, model, output_path)\n\ntest miou : 0.37293 test dice score : 0.49206"
  },
  {
    "objectID": "posts/CV_Semantic_Segmentation.html#unet",
    "href": "posts/CV_Semantic_Segmentation.html#unet",
    "title": "CV_Semantic_Segmentaion_0",
    "section": "",
    "text": "# !jar xvf pavement_crack.zip\n\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nimport torchvision.transforms as transforms\n\nimport os, zipfile\nimport tqdm\nimport random\nimport shutil\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport scipy.ndimage as ndimage\nimport matplotlib.pyplot as plt\n\n\n# 재현성을 위한 랜덤시드 고정\nrandom_seed = 2024\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\n\n\n\n# 클래스 정의\nclass CrackDatasets(Dataset):\n    def __init__(self, img_dir, mask_dir, img_transform, mask_transform):\n        self.img_dir = img_dir\n        self.img_transform = img_transform\n        self.img_files = []\n        self.mask_dir = mask_dir\n        self.mask_transform = mask_transform\n        self.mask_files = []\n        self.seed = np.random.randint(2024)\n\n        for img_name in os.listdir(self.img_dir):\n            if img_name.split('.')[-1] in ('png', 'jpg'):\n                self.img_files.append(os.path.join(self.img_dir, img_name))\n                self.mask_files.append(os.path.join(self.mask_dir, img_name))\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_files[i])\n        if self.img_transform is not None:\n            random.seed(self.seed)\n            img = self.img_transform(img)\n\n        mask = Image.open(self.mask_files[i]).convert('L')\n        if self.mask_transform is not None:\n            mask = self.mask_transform(mask)\n\n        return img, mask\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\n\n\ndef dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float=1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\ndef iou(y_true, y_pred, epsilon: float = 1e-6):\n    intersection = (y_true * y_pred).sum()\n    union = y_true.sum() + y_pred.sum() - intersection\n    return (intersection + epsilon) / (union + epsilon)\n\n\n\n\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n\n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                             nn.ReLU(),\n                             nn.BatchNorm2d(num_features=out_channels),\n                             nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                             nn.ReLU(),\n                             nn.BatchNorm2d(num_features=out_channels))\n        return block\n\n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) #[-1, 64, 256, 256]\n        contracting_12_out = self.contracting_12(contracting_11_out) #[-1, 64, 128, 128]\n\n        contracting_21_out = self.contracting_21(contracting_12_out) #[-1, 128, 128, 128]\n        contracting_22_out = self.contracting_22(contracting_21_out) #[-1, 128, 64, 64]\n\n        contracting_31_out = self.contracting_31(contracting_22_out) #[-1, 256, 64, 64]\n        contracting_32_out = self.contracting_32(contracting_31_out) #[-1, 256, 32, 32]\n\n        contracting_41_out = self.contracting_41(contracting_32_out) #[-1, 512, 32, 32]\n        contracting_42_out = self.contracting_42(contracting_41_out) #[-1, 512, 16, 16]\n\n        middle_out = self.middle(contracting_42_out) #[-1, 512, 32, 32]\n\n        expansive_11_out = self.expansive_11(middle_out) #[-1, 512, 32, 32]\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1))\n        # [-1, 1024, 32, 32] -&gt; # [-1, 512, 32, 32]\n        \n        expansive_21_out = self.expansive_21(expansive_12_out) #[-1, 256, 64, 64]\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1))\n        # [-1, 512, 64, 64] -&gt; # [-1, 256, 64, 64]\n        \n        expansive_31_out = self.expansive_31(expansive_22_out) #[-1, 128, 128, 128]\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1))\n        # [-1, 256, 128, 128] -&gt; # [-1, 128, 128, 128]\n        \n        expansive_41_out = self.expansive_41(expansive_32_out)\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out,contracting_11_out), dim=1))\n        #[-1, 128, 256, 256] -&gt; # [1, 64, 256,256]\n        \n        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n        return output_out\n\n\n\n\nclass AverageMeter(object):\n    #Computes and stores the average and current value\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(optimizer, epoch, lr):\n    #Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n    lr = lr * (0.1 **(epoch//30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef find_latest_model_path(dir):\n    model_paths = []\n    epochs = []\n    for path in Path(dir).glob('*.pth'):\n        if 'epoch' not in path.stem:\n            continue\n        model_paths.append(path)\n        parts = path.stem.split('_')\n        epoch = int(parts[-1])\n        epochs.append(epoch)\n\n    if len(epochs) &gt; 0:\n        epochs = np.array(epochs)\n        max_idx = np.argmax(epochs)\n        return model_paths[max_idx]\n    else:\n        return None\n\ndef save_check_point(state, is_best, file_name='checkpoint.pth'):\n    torch.save(state, file_name)\n    if is_best:\n        shutil.copy(file_name, 'model_best.pth')\n\ndef calc_crack_pixel_weight(mask_dir):\n    avg_w = 0.0\n    n_files = 0\n    for path in Path(mask_dir).glob('*.*'):\n        n_files += 1\n        m = ndimage.imread(path)\n        ncrack = np.sum((m&gt;0)[:])\n        w = float(ncrack) / (m.shape[0]*m.shape[1])\n        avg_w = avg_w + (1-w)\n\n    return avg_w / (1.0 - avg_w)\n\n\n\n\n\n\ndef train(train_loader, model, criterion, optimizer, valid_loader, model_dir, n_epoch, batch_size, lr, device):\n\n    latest_model_path = find_latest_model_path(model_dir)\n    best_model_path = os.path.join(*[model_dir, 'model_best.pth'])\n\n    if latest_model_path is not None:\n        state = torch.load(latest_model_path)\n        epoch = state['epoch']\n        model.load_state_dict(state['model'])\n        epoch = epoch\n\n        # if latest model path does exist, best_model_path should exists as well\n        assert Path(best_model_path).exists() == True, f'best model path {best_model_path} doest not exist'\n        # load the min loss so far\n        best_state = torch.load(latest_model_path)\n        min_val_los = best_state['valid_loss']\n\n        print(f'Restored model at epoch {epoch}. Min Validation loss so far is: {min_val_los}')\n        epoch += 1\n        print(f'Started training model from epoch {epoch}')\n    else:\n        print('Started training model from epoch 0')\n        epoch = 0\n        min_val_los = 9999\n\n    valid_losses = []\n    for epoch in range(epoch, n_epoch):\n        adjust_learning_rate(optimizer, epoch, lr)\n\n        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n        tq.set_description(f'Epoch {epoch}')\n\n        losses = AverageMeter()\n        t_iou = 0\n        t_dice = 0\n\n        model.train()\n        for i, (input, target) in enumerate(train_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            masks_pred = model(input_var)\n            pred = F.sigmoid(masks_pred)\n            target_mask = target_var\n\n            pred[pred&gt;0.5] = 1\n            pred[pred&lt;=0.5] = 0\n            target_mask[target_mask&gt;0.5] = 1\n            target_mask[target_mask&lt;=0.5] = 0\n\n            masks_prob_flat = masks_pred.view(-1)\n            true_masks_flat = target_var.view(-1)\n\n            loss = criterion(masks_prob_flat, true_masks_flat)\n            losses.update(loss)\n            tq.set_postfix(loss='{:.5f}'.format(losses.avg))\n            tq.update(batch_size)\n\n            t_dice += dice_coeff(pred, target_mask)\n            t_iou += iou(pred, target_mask)\n\n            # compute gradient and do SGD step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f'train miou : {t_iou/len(train_loader):.5f} train dice score : {t_dice/len(train_loader):.5f}')\n        valid_metrics = valid(model, valid_loader, criterion)\n        valid_loss = valid_metrics['valid_loss']\n        valid_dice = valid_metrics['v_dice']\n        valid_iou = valid_metrics['v_iou']\n        valid_losses.append(valid_loss)\n        print(f'valid_loss = {valid_loss:.5f}')\n        print(f'valid miou : {valid_iou/len(valid_loader):.5f} valid dice score : {valid_dice/len(valid_loader):.5f}')\n        tq.close()\n\n        # save the model of the current epoch\n        epoch_model_path = os.path.join(*[model_dir, f'model_epoch_{epoch}.pth'])\n        torch.save({\n            'model': model.state_dict(),\n            'epoch': epoch,\n            'valid_loss': valid_loss,\n            'train_loss': losses.avg\n        }, epoch_model_path)\n\n        if valid_loss &lt; min_val_los:\n            min_val_los = valid_loss\n\n            torch.save({\n                'model' : model.state_dict(),\n                'epoch' : epoch,\n                'valid_loss' : valid_loss,\n                'train_loss' : losses.avg\n            }, best_model_path)\n\n\ndef valid(model, val_loader, criterion):\n    losses = AverageMeter()\n    v_iou = 0\n    v_dice = 0\n    model.eval()\n    with torch.no_grad():\n\n        for i, (input, target) in enumerate(val_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            output = model(input_var)\n            loss = criterion(output, target_var)\n\n            losses.update(loss.item(), input_var.size(0))\n\n            pred = F.sigmoid(output)\n            target_mask = target_var\n            pred[pred&gt;0.5] = 1\n            pred[pred&lt;=0.5] = 0\n            target_mask[target_mask&gt;0.5] = 1\n            target_mask[target_mask&lt;=0.5] = 0\n\n            v_dice += dice_coeff(pred, target_mask)\n            v_iou = iou(pred, target_mask)\n\n    return {'valid_loss': losses.avg, 'v_dice': v_dice, 'v_iou': v_iou}\n\n\n\n\n\n# model folder\nmodel_dir = '/root/2024winter/DL_tutorial/posts/model_weights'\nos.makedirs(model_dir, exist_ok=True)\n\n# data folder\ndata_dir = '/root/2024winter/DL_tutorial/posts/crack_segmentation_dataset/train'\nDIR_IMG = os.path.join(data_dir, 'images')\nDIR_MASK = os.path.join(data_dir, 'masks')\n\n# Device 할당\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlr = 0.001\nmomentum = 0.9\nweight_decay = 1e-4\nbatch_size = 8\nnum_workers = 8\nn_epoch = 10\n\n\n# 모델 할당\nmodel = UNet(num_classes=1)\n\n# Optimizer 정의\noptimizer = torch.optim.SGD(model.parameters(), lr,\n                           momentum=momentum,\n                           weight_decay=weight_decay)\n\n# 손실 함수 정의\ncriterion = nn.BCEWithLogitsLoss().to(device)\n\nchannel_means = [0.485, 0.456, 0.406]\nchannel_stds  = [0.229, 0.224, 0.224]\n\ntrain_tfms = transforms.Compose([transforms.ToTensor(),\n                                transforms.Resize(256),\n                                transforms.Normalize(channel_means, channel_stds)])\n\nval_tfms = transforms.Compose([transforms.ToTensor(),\n                              transforms.Resize(256),\n                              transforms.Normalize(channel_means, channel_stds)])\n\nmask_tfms = transforms.Compose([transforms.ToTensor(),\n                               transforms.Resize(256)])\n\ndataset = CrackDatasets(img_dir=DIR_IMG,\n                       img_transform=train_tfms,\n                       mask_dir=DIR_MASK,\n                       mask_transform=mask_tfms)\n\ntrain_size = int(0.85*len(dataset))\nvalid_size = len(dataset) - train_size\n\n# train / val 분할\ntrain_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n\n# 데이터로더\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers)\nvalid_loader = DataLoader(valid_dataset, batch_size, shuffle=False, num_workers=num_workers)\n\nmodel.to(device)\n\n#train\ntrain(train_loader, model, criterion, optimizer,\n      valid_loader, model_dir, n_epoch, batch_size, lr, device)\n\nStarted training model from epoch 0\ntrain miou : 0.16975 train dice score : 0.23192\nvalid_loss = 0.10602\nvalid miou : 0.00013 valid dice score : 0.21843\ntrain miou : 0.27445 train dice score : 0.34929\nvalid_loss = 0.09925\nvalid miou : 0.00038 valid dice score : 0.26572\ntrain miou : 0.32347 train dice score : 0.41108\nvalid_loss = 0.08654\nvalid miou : 0.00108 valid dice score : 0.37338\ntrain miou : 0.36215 train dice score : 0.44759\nvalid_loss = 0.08282\nvalid miou : 0.00079 valid dice score : 0.47123\ntrain miou : 0.38905 train dice score : 0.48338\nvalid_loss = 0.07262\nvalid miou : 0.00085 valid dice score : 0.49158\ntrain miou : 0.40778 train dice score : 0.50428\nvalid_loss = 0.07109\nvalid miou : 0.00097 valid dice score : 0.52032\ntrain miou : 0.42189 train dice score : 0.52392\nvalid_loss = 0.06942\nvalid miou : 0.00078 valid dice score : 0.54654\ntrain miou : 0.43467 train dice score : 0.53903\nvalid_loss = 0.06694\nvalid miou : 0.00077 valid dice score : 0.52760\ntrain miou : 0.44633 train dice score : 0.55094\nvalid_loss = 0.06965\nvalid miou : 0.00069 valid dice score : 0.55299\ntrain miou : 0.45546 train dice score : 0.55663\nvalid_loss = 0.06245\nvalid miou : 0.00043 valid dice score : 0.55632\n\n\nEpoch 0:   0%|                                                                                 | 0/8168 [00:00&lt;?, ?it/s]/root/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\nEpoch 0: 100%|███████████████████████████████████████████████████████▉| 8160/8168 [01:18&lt;00:00, 88.84it/s, loss=0.20300]Epoch 0: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:24&lt;00:00, 96.69it/s, loss=0.20300]\nEpoch 1: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:11&lt;00:00, 120.05it/s, loss=0.09282]Epoch 1: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:17&lt;00:00, 106.00it/s, loss=0.09282]\nEpoch 2: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:12&lt;00:00, 120.32it/s, loss=0.08378]Epoch 2: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:17&lt;00:00, 105.32it/s, loss=0.08378]\nEpoch 3: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:11&lt;00:00, 118.50it/s, loss=0.07736]Epoch 3: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:15&lt;00:00, 107.60it/s, loss=0.07736]\nEpoch 4: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:13&lt;00:00, 107.44it/s, loss=0.07263]Epoch 4: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.28it/s, loss=0.07263]\nEpoch 5: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:17&lt;00:00, 114.78it/s, loss=0.06974]Epoch 5: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:23&lt;00:00, 97.74it/s, loss=0.06974]\nEpoch 6: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:15&lt;00:00, 111.50it/s, loss=0.06713]Epoch 6: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:21&lt;00:00, 100.49it/s, loss=0.06713]\nEpoch 7: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:23&lt;00:00, 111.52it/s, loss=0.06493]Epoch 7: 100%|████████████████████████████████████████████████████████| 8168/8168 [01:29&lt;00:00, 91.64it/s, loss=0.06493]\nEpoch 8: 100%|██████████████████████████████████████████████████████▉| 8160/8168 [01:14&lt;00:00, 111.54it/s, loss=0.06300]Epoch 8: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.57it/s, loss=0.06300]\nEpoch 9: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:14&lt;00:00, 119.27it/s, loss=0.06161]Epoch 9: 100%|███████████████████████████████████████████████████████| 8168/8168 [01:20&lt;00:00, 101.92it/s, loss=0.06161]\n\n\n\nfor images, masks in train_loader:\n    print(images.shape, masks.shape)\n    break\n\ntorch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n\n\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# 저장된 모델 웨이트로 경로 지정\nmodel_dir = '/root/2024winter/DL_tutorial/posts/model_weights'\ntitle=\"Loss Graph\"\n\nweights_paths = [path for path in Path(model_dir).glob('*.pth')]\nweights_paths = sorted(weights_paths)\nprint(weights_paths)\nepochs = []\ntr_losses = []\nvl_losses = []\n\nfor w_path in weights_paths:\n    if 'epoch' not in w_path.stem:\n        continue\n    # load the min loss so far\n    parts = w_path.stem.split('_')\n    epoch = int(parts[-1])\n    epochs.append(epoch)\n    state = torch.load(w_path)\n    val_los = state['valid_loss']\n    train_loss = float(state['train_loss'])\n    tr_losses.append(train_loss)\n    vl_losses.append(val_los)\n\nsorted_idx = np.argsort(epochs)\ntr_losses = [tr_losses[idx] for idx in sorted_idx]\nvl_losses = [vl_losses[idx] for idx in sorted_idx]\nplt.plot(tr_losses[1:], label='train_loss')\nplt.plot(vl_losses[1:], label='valid_loss')\nplt.title(title)\nplt.legend()\nplt.show()\n\n[PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_best.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_0.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_1.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_2.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_3.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_4.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_5.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_6.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_7.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_8.pth'), PosixPath('/root/2024winter/DL_tutorial/posts/model_weights/model_epoch_9.pth')]\n\n\n\n\n\n\n\n\n\n\nimport cv2\ndef evaluate(test_loader, model, output_path):\n    e_iou = 0\n    e_dice = 0\n    model.eval()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(test_loader):\n            input_var = Variable(input).to(device)\n            target_var = Variable(target).to(device)\n\n            output = model(input_var)\n            pred = F.sigmoid(output)\n            target_mask = target_var\n\n            pred[pred&gt;0.5]=1\n            pred[pred&lt;=0.5]=0\n            target_mask[target_mask&gt;0.5]=1\n            target_mask[target_mask&lt;=0.5]=0\n            \n            cv2.imwrite(f'{output_path}/predicts/{i}.png', tensor2image(pred))\n            cv2.imwrite(f'{output_path}/masks/{i}.png', tensor2image(target_mask))\n\n            e_dice += dice_coeff(pred, target_mask)\n            e_iou += iou(pred, target_mask)\n\n    print(f'test miou : {e_iou/len(test_loader):.5f} test dice score : {e_dice/len(test_loader):.5f}')\n\n\ndef denormalize(image):\n    #IMAGENET_MEAN, IMAGENET_STD = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n    #image = np.clip(255.0 * (image * IMAGENET_STD + IMAGENET_MEAN), 0, 255)\n    image = image * 255.0\n    return image\n\ndef tensor2image(tensor_list):\n    img = tensor_list[0].detach().cpu().numpy()\n    img = np.transpose(img, (1,2,0))\n    img = denormalize(img)\n    #img = img.astype('float32')\n    #img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    img = img.astype(np.uint8).copy()\n    return img\n\n\n# 테스트 데이터셋\ntest_data_dir = '/root/2024winter/DL_tutorial/posts/crack_segmentation_dataset/test'\ntest_img_path = os.path.join(test_data_dir, 'images')\ntest_mask_path = os.path.join(test_data_dir, 'masks')\n\ntest_tfms = transforms.Compose([transforms.ToTensor(),\n                               transforms.Normalize(channel_means, channel_stds)])\nmask_tfms = transforms.Compose([transforms.ToTensor()])\n\ntest_dataset = CrackDatasets(img_dir=test_img_path, img_transform=test_tfms,\n                            mask_dir=test_mask_path, mask_transform=mask_tfms)\ntest_loader = DataLoader(test_dataset, 1, shuffle=False, num_workers=num_workers)\n\n# 모델 할당\nmodel = UNet(num_classes=1)\n\nbest_model_path = os.path.join(*[model_dir, 'model_best.pth'])\nif best_model_path is not None:\n    state = torch.load(best_model_path)\n    epoch = state['epoch']\n    model.load_state_dict(state['model'])\nmodel.to(device)\n\nos.makedirs('/root/2024winter/DL_tutorial/posts/results/predicts', exist_ok=True)\nos.makedirs('/root/2024winter/DL_tutorial/posts/results/masks', exist_ok=True)\n\noutput_path = '/root/2024winter/DL_tutorial/posts/results'\nevaluate(test_loader, model, output_path)\n\ntest miou : 0.37293 test dice score : 0.49206"
  },
  {
    "objectID": "posts/CV_Semantic_Segmentation.html#커스텀-데이터-다운로드",
    "href": "posts/CV_Semantic_Segmentation.html#커스텀-데이터-다운로드",
    "title": "CV_Semantic_Segmentaion_0",
    "section": "#### 커스텀 데이터 다운로드",
    "text": "#### 커스텀 데이터 다운로드\n\n!unzip /root/2024winter/DL_tutorial/posts/Semantic_Segmentation_Custom.zip\n\n\nCVAT 마스크 라벨 포맷 변경\n\n.png $$ .jpg\n\nfrom PIL import Image\nroot_dir = '/root/2024winter/DL_tutorial/posts/Semantic_Custom_Datasets/masks'\nfor img in os.listdir(root_dir):\n    if img.split('.')[-1] in ('png', 'jpg'):\n        im = Image.open(os.path.join(root_dir, img))\n        im.save(os.path.join(root_dir, img.split('.')[0]+'.jpg'))\n        os.remove(os.path.join(root_dir, img))\n\n\n\n커스텀 데이터셋 성능 평가\n\ntest_data_dir = '/root/2024winter/DL_tutorial/posts/Semantic_Custom_Datasets'\ntest_img_path = os.path.join(test_data_dir, 'images')\ntest_mask_path = os.path.join(test_data_dir, 'masks')\n\ntest_tfms = transforms.Compose([transforms.ToTensor(),\n                               transforms.Normalize(channel_means, channel_stds)])\nmask_tfms = transforms.Compose([transforms.ToTensor()])\n\ntest_dataset = CrackDatasets(img_dir=test_img_path, img_transform=test_tfms, mask_dir=test_mask_path, mask_transform=mask_tfms)\ntest_loader = DataLoader(test_dataset, 1, shuffle=False, num_workers=num_workers)\n\n# 모델 할당\nmodel = UNet(num_classes=1)\nbest_model_path = os.path.join(*[model_dir, 'model_best.pth'])\n\nif best_model_path is not None:\n    state = torch.load(best_model_path)\n    epoch = state['epoch']\n    model.load_state_dict(state['model'])\n\nmodel.to(device)\n\n# 결과물 저장 폴더 생성\nos.makedirs('/root/2024winter/DL_tutorial/posts/custom_results/predicts', exist_ok=True)\nos.makedirs('/root/2024winter/DL_tutorial/posts/custom_results/masks', exist_ok=True)\n\noutput_path = '/root/2024winter/DL_tutorial/posts/custom_results'\nevaluate(test_loader, model, output_path)\n\nFileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/anaconda3/envs/dl/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/anaconda3/envs/dl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/anaconda3/envs/dl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in &lt;listcomp&gt;\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_203176/810415501.py\", line 23, in __getitem__\n    mask = Image.open(self.mask_files[i]).convert('L')\n  File \"/root/anaconda3/envs/dl/lib/python3.10/site-packages/PIL/Image.py\", line 3218, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/root/2024winter/DL_tutorial/posts/Semantic_Custom_Datasets/masks/CFD_001.jpg'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DL_tutorial",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 13, 2024\n\n\nCV_Instance_Segmentation_0\n\n\n이정재 \n\n\n\n\nFeb 12, 2024\n\n\nCV_Semantic_Segmentaion_0\n\n\n이정재 \n\n\n\n\nFeb 9, 2024\n\n\nCV_ObjectDetection_0\n\n\n이정재 \n\n\n\n\nFeb 5, 2024\n\n\nCV_classification_0\n\n\n이정재 \n\n\n\n\nJan 31, 2024\n\n\nLMTM\n\n\n이정재 \n\n\n\n\nJan 30, 2024\n\n\nVGGNet\n\n\n이정재 \n\n\n\n\nJan 29, 2024\n\n\nAlexNet\n\n\n이정재 \n\n\n\n\nJan 28, 2024\n\n\nANN_2\n\n\n이정재 \n\n\n\n\nJan 27, 2024\n\n\nANN_1\n\n\n이정재 \n\n\n\n\n\nNo matching items"
  }
]