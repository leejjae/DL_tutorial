[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LMTM.html",
    "href": "posts/LMTM.html",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\n영화 리뷰 텍스트(many)를 입력으로 받아 긍정 또는 부정(one)을 출력하는 구조\nEmbedding: 영화 리뷰(text)를 벡터로 변환하는 연산\nLSTM: 시계열 데이터를 처리하기 위한 구조\nLinear: 결과 출력"
  },
  {
    "objectID": "posts/LMTM.html#sentimental-analysis",
    "href": "posts/LMTM.html#sentimental-analysis",
    "title": "LMTM",
    "section": "",
    "text": "many to one\n\n영화 리뷰 텍스트(many)를 입력으로 받아 긍정 또는 부정(one)을 출력하는 구조\nEmbedding: 영화 리뷰(text)를 벡터로 변환하는 연산\nLSTM: 시계열 데이터를 처리하기 위한 구조\nLinear: 결과 출력"
  },
  {
    "objectID": "posts/LMTM.html#step-1-load-libraries-datsets",
    "href": "posts/LMTM.html#step-1-load-libraries-datsets",
    "title": "LMTM",
    "section": "Step 1 : Load libraries & Datsets",
    "text": "Step 1 : Load libraries & Datsets\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport os\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom collections import Counter\n\n\ndata = pd.read_csv('exercise4.csv')\ndata.head()\n\n\n\n\n\n\n\n\nprocessed\nlabel\n\n\n\n\n0\nOne reviewer mentioned watching Oz episode hoo...\n1\n\n\n1\nA wonderful little production . The filming te...\n1\n\n\n2\nI thought wonderful way spend time hot summer ...\n1\n\n\n3\nBasically family little boy Jake think zombie ...\n0\n\n\n4\nPetter Mattei Love Time Money visually stunnin...\n1\n\n\n\n\n\n\n\n\ndata['processed'][0]\n\n'One reviewer mentioned watching Oz episode hooked . They right , exactly happened . The first thing struck Oz brutality unflinching scene violence , set right word GO . Trust , show faint hearted timid . This show pull punch regard drug , sex violence . Its hardcore , classic use word . It called OZ nickname given Oswald Maximum Security State Penitentary . It focus mainly Emerald City , experimental section prison cell glass front face inwards , privacy high agenda . Em City home many . . Aryans , Muslims , gangsta , Latinos , Christians , Italians , Irish . . . . scuffle , death stare , dodgy dealing shady agreement never far away . I would say main appeal show due fact go show dare . Forget pretty picture painted mainstream audience , forget charm , forget romance . . . OZ mess around . The first episode I ever saw struck nasty surreal , I say I ready , I watched , I developed taste Oz , got accustomed high level graphic violence . Not violence , injustice crooked guard sold nickel , inmate kill order get away , well mannered , middle class inmate turned prison bitch due lack street skill prison experience Watching Oz , may become comfortable uncomfortable viewing . . . . thats get touch darker side .'\n\n\n\ndata['processed'] = data['processed'].str.lower().replace(r\"[^a-zA-Z ]\", \"\", regex=True)\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\n- 사전생성\n\n리뷰 문장에 들어있는 단어들을 추출하고, 각각의 단어에 숫자를 부여하는 작업\n[‘one’, ‘reviewer’, ‘mentioned’, ‘watching’, ‘oz’, ‘episode’, ‘hooked’]\n\n\n# 문장에 포함된 단어 토큰화\nreviews = data['processed'].values\nwords = ' '.join(reviews).split()\nwords[:10]\n\n['one',\n 'reviewer',\n 'mentioned',\n 'watching',\n 'oz',\n 'episode',\n 'hooked',\n 'they',\n 'right',\n 'exactly']\n\n\n\ncounter = Counter(words)\nvocab = sorted(counter, key=counter.get, reverse=True)\nint2word = dict(enumerate(vocab, 1))\nint2word[0] = '&lt;PAD&gt;'\nword2int = {word: id for id, word in int2word.items()}\n#word2int\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\n\n- 리뷰 인코딩\n\n리뷰에 포함된 단어를 숫자형태로 변환하는 작업\n{‘i’: 1, ‘movie’: 2, ‘film’: 3, ‘the’: 4, ‘one’: 5, ‘like’: 6, ‘it’: 7, ‘time’: 8, ‘this’: 9, ‘good’: 10, ‘character’: 11,…}\n\n\nreviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n\n100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:02&lt;00:00, 24930.36it/s]\n\n\n\nreviews_enc[0][1:10]\n\n[1095, 972, 74, 2893, 186, 2982, 119, 114, 538]\n\n\n\ndata['processed'][0]\n\n'one reviewer mentioned watching oz episode hooked  they right  exactly happened  the first thing struck oz brutality unflinching scene violence  set right word go  trust  show faint hearted timid  this show pull punch regard drug  sex violence  its hardcore  classic use word  it called oz nickname given oswald maximum security state penitentary  it focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many   aryans  muslims  gangsta  latinos  christians  italians  irish     scuffle  death stare  dodgy dealing shady agreement never far away  i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audience  forget charm  forget romance    oz mess around  the first episode i ever saw struck nasty surreal  i say i ready  i watched  i developed taste oz  got accustomed high level graphic violence  not violence  injustice crooked guard sold nickel  inmate kill order get away  well mannered  middle class inmate turned prison bitch due lack street skill prison experience watching oz  may become comfortable uncomfortable viewing     thats get touch darker side '\n\n\n\nword2int['one'], word2int['reviewer'], word2int['mentioned']\n\n(5, 1095, 972)\n\n\n\ndata['encoded'] = reviews_enc\ndata['encoded']\n\n0        [5, 1095, 972, 74, 2893, 186, 2982, 119, 114, ...\n1        [45, 311, 53, 247, 4, 1270, 1633, 16086, 78, 8...\n2        [1, 97, 311, 28, 1053, 8, 763, 1343, 2345, 112...\n3        [591, 130, 53, 221, 3123, 33, 565, 3653, 608, ...\n4        [57645, 9676, 39, 8, 203, 1993, 1312, 3, 37, 3...\n                               ...                        \n49995    [1, 97, 2, 114, 10, 191, 7, 1413, 128, 26, 913...\n49996    [22, 40, 22, 307, 22, 50, 2847, 869, 545, 1364...\n49997    [1, 3168, 4064, 34678, 7571, 269, 4234, 4064, ...\n49998    [1, 86, 2839, 825, 369, 348, 9292, 5, 9, 208, ...\n49999    [264, 5, 5445, 109, 1941, 2, 213, 328, 123, 44...\nName: encoded, Length: 50000, dtype: object\n\n\n\n\n- 길이 맞춰주기(padding or trim)\n\n신경망의 입력으로 사용하기 위해 일정 길이만큼 맞춰주는 작업\n길이가 긴 문장은 잘라주고(trim), 길이가 짧은 문장은 채워주는(padding) 작업\n\n\ndef pad_features(reviews, pad_id, seq_length=128):\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        features[i, :len(row)] = np.array(row)[:seq_length]\n\n    return features\n\nseq_length = 256\nfeatures = pad_features(reviews_enc, pad_id=word2int['&lt;PAD&gt;'], seq_length=seq_length)\n\nassert len(features) == len(reviews_enc)\nassert len(features[0]) == seq_length\n\n\nnp.full((5,3),2)\n\narray([[2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2],\n       [2, 2, 2]])\n\n\n\nword2int['&lt;PAD&gt;']\n\n0\n\n\n\nlabels = data['label'].to_numpy()\nlabels\n\narray([1, 1, 1, ..., 0, 0, 0])\n\n\n\n\n- 데이터 분할\n\n# train test split\ntrain_size = .8\nsplit_id = int(len(features) * train_size)\ntrain_x, test_x, train_y, test_y = features[:split_id], features[split_id:], labels[:split_id], labels[split_id:]\n\nsplit_id = int(len(train_x) * train_size)\ntrain_x, valid_x, train_y, valid_y = train_x[:split_id], train_x[split_id:], train_y[:split_id], train_y[split_id:]\n\nprint('Train X shape: {}, Valid X shape: {}, Test X shape: {}'.format(train_x.shape, valid_x.shape, test_x.shape))\nprint('Train y shape: {}, Valid y shape: {}, Test y shape: {}'.format(train_y.shape, valid_y.shape, test_y.shape))\n\nTrain X shape: (32000, 256), Valid X shape: (8000, 256), Test X shape: (10000, 256)\nTrain y shape: (32000,), Valid y shape: (8000,), Test y shape: (10000,)"
  },
  {
    "objectID": "posts/LMTM.html#step-2-create-dataloader",
    "href": "posts/LMTM.html#step-2-create-dataloader",
    "title": "LMTM",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# set hyperparameter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nlr = 0.001\nbatch_size = 128\nvocab_size = len(word2int)\nembedding_size = 256\ndropout = 0.25\n\nepochs = 8\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'epochs': epochs\n}\n\nes_limit = 5\n\ncuda\n\n\n\ntrainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalidset = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntestset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\ntrainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\nvalloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\ntestloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
  },
  {
    "objectID": "posts/LMTM.html#step-3-set-network-structure",
    "href": "posts/LMTM.html#step-3-set-network-structure",
    "title": "LMTM",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_size=400):\n        super(LSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, 512, 2, dropout=0.25, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(512, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = x.long()\n        x = self.embedding(x)\n        o, _ = self.lstm(x)\n        o = o[:, -1, :]\n        o = self.dropout(o)\n        o = self.fc(o)\n        o = self.sigmoid(o)\n\n        return o"
  },
  {
    "objectID": "posts/LMTM.html#step-4-create-model-instance",
    "href": "posts/LMTM.html#step-4-create-model-instance",
    "title": "LMTM",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\nmodel = LSTMClassifier(vocab_size, embedding_size).to(device)\nprint(model)\n\nLSTMClassifier(\n  (embedding): Embedding(96140, 256)\n  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
  },
  {
    "objectID": "posts/LMTM.html#step-5-model-compile",
    "href": "posts/LMTM.html#step-5-model-compile",
    "title": "LMTM",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\ncriterion = nn.BCELoss()\noptim = Adam(model.parameters(), lr=lr)"
  },
  {
    "objectID": "posts/LMTM.html#step-6-set-train-loop",
    "href": "posts/LMTM.html#step-6-set-train-loop",
    "title": "LMTM",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train(model, trainloader):\n    model.train()\n\n    train_loss = 0\n    train_acc = 0\n\n    for id, (X, y) in enumerate(trainloader):\n        X, y = X.to(device), y.to(device)\n        optim.zero_grad()\n        y_pred = model(X)\n        loss = criterion(y_pred.squeeze(), y.float())\n        loss.backward()\n        optim.step()\n\n        train_loss += loss.item()\n        y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n        equals = y_pred == y\n        acc = torch.mean(equals.type(torch.FloatTensor))\n        train_acc += acc.item()\n\n    history['train_loss'].append(train_loss / len(trainloader))\n    history['train_acc'].append(train_acc / len(trainloader))\n\n    return train_loss, train_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-7-set-test-loop",
    "href": "posts/LMTM.html#step-7-set-test-loop",
    "title": "LMTM",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef validation(model, valloader):\n    model.eval()\n\n    val_loss = 0\n    val_acc = 0\n\n    with torch.no_grad():\n        for id, (X,y) in enumerate(valloader):\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss = criterion(y_pred.squeeze(), y.float())\n\n            val_loss += loss.item()\n\n            y_pred = torch.tensor([1 if i == True else 0 for i in y_pred &gt; 0.5], device=device)\n            equals = y_pred == y\n            acc = torch.mean(equals.type(torch.FloatTensor))\n            val_acc += acc.item()\n\n        history['val_loss'].append(val_loss / len(valloader))\n        history['val_acc'].append(val_acc / len(valloader))\n\n    return val_loss, val_acc"
  },
  {
    "objectID": "posts/LMTM.html#step-8-run-model",
    "href": "posts/LMTM.html#step-8-run-model",
    "title": "LMTM",
    "section": "Step 8 : Run Model",
    "text": "Step 8 : Run Model\n\n# train loop\nepochloop = tqdm(range(epochs), desc='Training')\n\n# early stop trigger\nes_trigger = 0\nval_loss_min = torch.inf\n\nfor e in epochloop:\n    train_loss, train_acc = train(model, trainloader)\n    val_loss, val_acc = validation(model, valloader)\n    epochloop.write(f'Epoch[{e+1}/{epochs}] Train Loss: {train_loss / len(trainloader):.3f}, Train Acc: {train_acc / len(trainloader):.3f}, Val Loss: {val_loss / len(valloader):.3f}, Val Acc: {val_acc / len(valloader):.3f}')\n\n    # save model if validation loss decrease\n    if val_loss / len(valloader) &lt;= val_loss_min:\n        torch.save(model.state_dict(), './sentiment_lstm.pt')\n        val_loss_min=val_loss / len(valloader)\n        es_trigger = 0\n\n    else:\n        es_trigger += 1\n\n    # early stop\n    if es_trigger &gt;= es_limit:\n        epochloop.write(f'Early stopped at Epoch-{e+1}')\n        history['epochs'] = e+1\n        break\n\nTraining:   0%|                                                                                   | 0/8 [00:12&lt;?, ?it/s]Training:  25%|██████████████████▊                                                        | 2/8 [00:23&lt;01:10, 11.75s/it]Training:  25%|██████████████████▊                                                        | 2/8 [00:32&lt;01:10, 11.75s/it]Training:  38%|████████████████████████████▏                                              | 3/8 [00:42&lt;00:53, 10.67s/it]Training:  50%|█████████████████████████████████████▌                                     | 4/8 [00:51&lt;00:40, 10.24s/it]Training:  62%|██████████████████████████████████████████████▉                            | 5/8 [01:01&lt;00:29,  9.82s/it]Training:  88%|█████████████████████████████████████████████████████████████████▋         | 7/8 [01:11&lt;00:09,  9.95s/it]Training: 100%|███████████████████████████████████████████████████████████████████████████| 8/8 [01:21&lt;00:00, 10.20s/it]\n\n\nEpoch[1/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.496\nEpoch[2/8] Train Loss: 0.694, Train Acc: 0.506, Val Loss: 0.692, Val Acc: 0.514\nEpoch[3/8] Train Loss: 0.669, Train Acc: 0.570, Val Loss: 0.650, Val Acc: 0.706\nEpoch[4/8] Train Loss: 0.478, Train Acc: 0.800, Val Loss: 0.601, Val Acc: 0.772\nEpoch[5/8] Train Loss: 0.448, Train Acc: 0.811, Val Loss: 0.405, Val Acc: 0.818\nEpoch[6/8] Train Loss: 0.314, Train Acc: 0.879, Val Loss: 0.357, Val Acc: 0.847\nEpoch[7/8] Train Loss: 0.237, Train Acc: 0.914, Val Loss: 0.384, Val Acc: 0.842\nEpoch[8/8] Train Loss: 0.203, Train Acc: 0.932, Val Loss: 0.426, Val Acc: 0.856\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_acc'][:history['epochs']], label='Train Acc')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Acc')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot loss\nplt.figure(figsize=(6,4))\nplt.plot(range(history['epochs']), history['train_loss'][:history['epochs']], label='Train Loss')\nplt.plot(range(history['epochs']), history['val_acc'][:history['epochs']], label='Val Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/VGGNet.html",
    "href": "posts/VGGNet.html",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet 구조 살펴보기\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#vggnet",
    "href": "posts/VGGNet.html#vggnet",
    "title": "VGGNet",
    "section": "",
    "text": "VGGNet 구조 살펴보기\n\n\n\n\nVGG"
  },
  {
    "objectID": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "href": "posts/VGGNet.html#step-1-load-libraries-datasets",
    "title": "VGGNet",
    "section": "Step 1 : Load libraries & Datasets",
    "text": "Step 1 : Load libraries & Datasets\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-2-data-preprocessing",
    "href": "posts/VGGNet.html#step-2-data-preprocessing",
    "title": "VGGNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\n불러온 이미지의 증강을 통해 학습 정확도를 향상시키도록 합니다.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform,\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\ntrain_img.data.shape\n\n(50000, 32, 32, 3)"
  },
  {
    "objectID": "posts/VGGNet.html#step-3-set-hyperparameters",
    "href": "posts/VGGNet.html#step-3-set-hyperparameters",
    "title": "VGGNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 32\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/VGGNet.html#step-4-create-dataloader",
    "href": "posts/VGGNet.html#step-4-create-dataloader",
    "title": "VGGNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\ntrain_loader = DataLoader(train_img, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img, batch_size = batch_sizes, shuffle = False)"
  },
  {
    "objectID": "posts/VGGNet.html#step-5-set-network-structure",
    "href": "posts/VGGNet.html#step-5-set-network-structure",
    "title": "VGGNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\n# Model\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 360),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(360, 100),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(100, 10),\n        )\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),  # 추가\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n                \n        return nn.Sequential(*layers)"
  },
  {
    "objectID": "posts/VGGNet.html#step-6-create-model-instance",
    "href": "posts/VGGNet.html#step-6-create-model-instance",
    "title": "VGGNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\nmodel = VGG('VGG16').to(device)\nprint(model)\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=360, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=360, out_features=100, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=100, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/VGGNet.html#step-7-model-compile",
    "href": "posts/VGGNet.html#step-7-model-compile",
    "title": "VGGNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
  },
  {
    "objectID": "posts/VGGNet.html#step-8-set-train-loop",
    "href": "posts/VGGNet.html#step-8-set-train-loop",
    "title": "VGGNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}   [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/VGGNet.html#step-9-set-test-loop",
    "href": "posts/VGGNet.html#step-9-set-test-loop",
    "title": "VGGNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y  = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/VGGNet.html#step-10-run-model",
    "href": "posts/VGGNet.html#step-10-run-model",
    "title": "VGGNet",
    "section": "Step 10 : Run model",
    "text": "Step 10 : Run model\n\nfor i in range(epochs):\n    print(f\"Epoch {i+1} \\n---------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n---------------------------\nloss: 2.336054   [    0]/50000\nloss: 2.201652   [ 3200]/50000\nloss: 1.913308   [ 6400]/50000\nloss: 1.833156   [ 9600]/50000\nloss: 2.025965   [12800]/50000\nloss: 1.458568   [16000]/50000\nloss: 1.454589   [19200]/50000\nloss: 1.586300   [22400]/50000\nloss: 1.777550   [25600]/50000\nloss: 1.902973   [28800]/50000\nloss: 1.534656   [32000]/50000\nloss: 1.808721   [35200]/50000\nloss: 1.449842   [38400]/50000\nloss: 1.357022   [41600]/50000\nloss: 1.378457   [44800]/50000\nloss: 1.557457   [48000]/50000\nTest Error: \n Accuracy: 50.6%, Avg loss: 1.338251\n\nEpoch 2 \n---------------------------\nloss: 1.564054   [    0]/50000\nloss: 1.571676   [ 3200]/50000\nloss: 1.477239   [ 6400]/50000\nloss: 1.520172   [ 9600]/50000\nloss: 1.256163   [12800]/50000\nloss: 1.066607   [16000]/50000\nloss: 1.460073   [19200]/50000\nloss: 1.077838   [22400]/50000\nloss: 1.215548   [25600]/50000\nloss: 0.884830   [28800]/50000\nloss: 1.028723   [32000]/50000\nloss: 1.288996   [35200]/50000\nloss: 1.299563   [38400]/50000\nloss: 0.939404   [41600]/50000\nloss: 0.955451   [44800]/50000\nloss: 1.098657   [48000]/50000\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.923354\n\nEpoch 3 \n---------------------------\nloss: 0.796024   [    0]/50000\nloss: 0.677388   [ 3200]/50000\nloss: 0.617442   [ 6400]/50000\nloss: 1.173936   [ 9600]/50000\nloss: 0.786573   [12800]/50000\nloss: 0.798586   [16000]/50000\nloss: 1.181702   [19200]/50000\nloss: 0.897227   [22400]/50000\nloss: 0.735924   [25600]/50000\nloss: 1.028793   [28800]/50000\nloss: 0.834691   [32000]/50000\nloss: 1.081767   [35200]/50000\nloss: 0.828031   [38400]/50000\nloss: 1.046338   [41600]/50000\nloss: 0.828228   [44800]/50000\nloss: 1.146716   [48000]/50000\nTest Error: \n Accuracy: 70.4%, Avg loss: 0.880181\n\nEpoch 4 \n---------------------------\nloss: 0.617879   [    0]/50000\nloss: 0.876245   [ 3200]/50000\nloss: 0.673582   [ 6400]/50000\nloss: 0.556679   [ 9600]/50000\nloss: 0.699025   [12800]/50000\nloss: 1.006697   [16000]/50000\nloss: 0.683750   [19200]/50000\nloss: 1.277821   [22400]/50000\nloss: 0.562071   [25600]/50000\nloss: 0.686587   [28800]/50000\nloss: 0.873322   [32000]/50000\nloss: 0.719097   [35200]/50000\nloss: 0.457578   [38400]/50000\nloss: 0.514047   [41600]/50000\nloss: 0.729195   [44800]/50000\nloss: 0.858265   [48000]/50000\nTest Error: \n Accuracy: 75.3%, Avg loss: 0.712957\n\nEpoch 5 \n---------------------------\nloss: 0.855375   [    0]/50000\nloss: 0.854320   [ 3200]/50000\nloss: 0.695397   [ 6400]/50000\nloss: 0.525759   [ 9600]/50000\nloss: 0.381186   [12800]/50000\nloss: 0.587416   [16000]/50000\nloss: 0.511339   [19200]/50000\nloss: 1.319725   [22400]/50000\nloss: 0.649993   [25600]/50000\nloss: 0.508207   [28800]/50000\nloss: 0.585140   [32000]/50000\nloss: 0.794928   [35200]/50000\nloss: 0.799448   [38400]/50000\nloss: 0.417046   [41600]/50000\nloss: 0.498251   [44800]/50000\nloss: 0.779942   [48000]/50000\nTest Error: \n Accuracy: 76.6%, Avg loss: 0.682496\n\nEpoch 6 \n---------------------------\nloss: 0.719160   [    0]/50000\nloss: 0.627115   [ 3200]/50000\nloss: 0.255042   [ 6400]/50000\nloss: 0.400026   [ 9600]/50000\nloss: 0.737379   [12800]/50000\nloss: 0.741243   [16000]/50000\nloss: 0.726986   [19200]/50000\nloss: 0.266388   [22400]/50000\nloss: 0.633677   [25600]/50000\nloss: 0.482972   [28800]/50000\nloss: 0.444857   [32000]/50000\nloss: 0.513320   [35200]/50000\nloss: 0.529961   [38400]/50000\nloss: 0.784853   [41600]/50000\nloss: 0.560646   [44800]/50000\nloss: 0.426722   [48000]/50000\nTest Error: \n Accuracy: 75.4%, Avg loss: 0.737327\n\nEpoch 7 \n---------------------------\nloss: 0.456941   [    0]/50000\nloss: 0.552954   [ 3200]/50000\nloss: 0.588921   [ 6400]/50000\nloss: 0.359172   [ 9600]/50000\nloss: 0.380740   [12800]/50000\nloss: 0.230270   [16000]/50000\nloss: 0.544868   [19200]/50000\nloss: 0.470449   [22400]/50000\nloss: 0.716484   [25600]/50000\nloss: 0.427520   [28800]/50000\nloss: 0.485696   [32000]/50000\nloss: 0.250514   [35200]/50000\nloss: 0.619605   [38400]/50000\nloss: 0.534625   [41600]/50000\nloss: 0.294415   [44800]/50000\nloss: 0.676517   [48000]/50000\nTest Error: \n Accuracy: 81.8%, Avg loss: 0.560385\n\nEpoch 8 \n---------------------------\nloss: 0.584514   [    0]/50000\nloss: 0.433411   [ 3200]/50000\nloss: 0.360651   [ 6400]/50000\nloss: 0.707992   [ 9600]/50000\nloss: 0.449344   [12800]/50000\nloss: 0.380623   [16000]/50000\nloss: 0.333079   [19200]/50000\nloss: 0.420316   [22400]/50000\nloss: 0.414326   [25600]/50000\nloss: 0.539709   [28800]/50000\nloss: 0.425368   [32000]/50000\nloss: 0.610167   [35200]/50000\nloss: 0.427243   [38400]/50000\nloss: 0.787724   [41600]/50000\nloss: 0.561038   [44800]/50000\nloss: 0.456995   [48000]/50000\nTest Error: \n Accuracy: 81.5%, Avg loss: 0.566010\n\nEpoch 9 \n---------------------------\nloss: 0.383341   [    0]/50000\nloss: 0.381802   [ 3200]/50000\nloss: 0.258570   [ 6400]/50000\nloss: 0.421782   [ 9600]/50000\nloss: 0.455991   [12800]/50000\nloss: 0.531303   [16000]/50000\nloss: 0.758386   [19200]/50000\nloss: 0.285010   [22400]/50000\nloss: 0.314713   [25600]/50000\nloss: 0.419822   [28800]/50000\nloss: 0.278820   [32000]/50000\nloss: 0.553399   [35200]/50000\nloss: 0.416081   [38400]/50000\nloss: 0.365547   [41600]/50000\nloss: 0.686296   [44800]/50000\nloss: 0.814931   [48000]/50000\nTest Error: \n Accuracy: 83.5%, Avg loss: 0.502169\n\nEpoch 10 \n---------------------------\nloss: 0.275047   [    0]/50000\nloss: 0.573386   [ 3200]/50000\nloss: 0.737843   [ 6400]/50000\nloss: 0.478916   [ 9600]/50000\nloss: 0.429536   [12800]/50000\nloss: 0.238580   [16000]/50000\nloss: 0.406505   [19200]/50000\nloss: 0.228436   [22400]/50000\nloss: 0.370529   [25600]/50000\nloss: 0.344406   [28800]/50000\nloss: 0.301163   [32000]/50000\nloss: 0.257651   [35200]/50000\nloss: 0.833092   [38400]/50000\nloss: 0.897587   [41600]/50000\nloss: 0.397286   [44800]/50000\nloss: 0.637342   [48000]/50000\nTest Error: \n Accuracy: 84.8%, Avg loss: 0.467348\n\nDone!"
  },
  {
    "objectID": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "href": "posts/VGGNet.html#cifar-classifierpretrained-vggnet",
    "title": "VGGNet",
    "section": "## CIFAR Classifier(Pretrained VGGNet)",
    "text": "## CIFAR Classifier(Pretrained VGGNet)\nImageNet 데이터로 학습한 VGGNet을 사용하여 주어진 데이터 셋에서 사용할 수 있도록 Fine tuning 해봅니다."
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html",
    "href": "posts/CV_ObjectDetection_0.html",
    "title": "CV_ObjectDetection_0",
    "section": "",
    "text": "# Pascal VOC 2007 데이터 다운로드\n!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n\n--2024-02-09 07:27:15--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n--2024-02-09 07:27:16--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 460032000 (439M) [application/x-tar]\nSaving to: ‘VOCtrainval_06-Nov-2007.tar.3’\n\nVOCtrainval_06-Nov- 100%[===================&gt;] 438.72M  15.0MB/s    in 30s     \n\n2024-02-09 07:27:47 (14.5 MB/s) - ‘VOCtrainval_06-Nov-2007.tar.3’ saved [460032000/460032000]\n\n--2024-02-09 07:27:47--  http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nResolving pjreddie.com (pjreddie.com)... 162.0.215.52\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar [following]\n--2024-02-09 07:27:47--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\nConnecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 451020800 (430M) [application/x-tar]\nSaving to: ‘VOCtest_06-Nov-2007.tar.3’\n\nVOCtest_06-Nov-2007 100%[===================&gt;] 430.13M  16.1MB/s    in 26s     \n\n2024-02-09 07:29:06 (16.3 MB/s) - ‘VOCtest_06-Nov-2007.tar.3’ saved [451020800/451020800]\nfrom pathlib import Path\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval').mkdir(parents=True, exist_ok=True)\nPath('/root/2024winter/DL_tutorial/posts/pascal_datasets/test').mkdir(parents=True, exist_ok=True)\nroot=Path('/root/2024winter/DL_tutorial/posts/pascal_datasets')\n\nfor path1 in ('images', 'labels'):\n    for path2 in ('train2007', 'val2007', 'test2007'):\n        new_path = root / 'VOC' / path1 / path2\n        new_path.mkdir(parents=True, exist_ok=True)\n# 데이터셋 압축 해제\n!tar -xvf VOCtrainval_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/ &gt; /dev/null 2&gt;&1\n!tar -xvf VOCtest_06-Nov-2007.tar -C /root/2024winter/DL_tutorial/posts/pascal_datasets/test/ &gt; /dev/null 2&gt;&1\n# XML 형식을 YOLO Format으로 변경해주는 깃허브 클론 \n!git clone https://github.com/ssaru/convert2Yolo.git\n\nfatal: destination path 'convert2Yolo' already exists and is not an empty directory.\n# 필요 라이브러리 설치\n%cd convert2Yolo\n%pip install -qr requirements.txt\n\n[Errno 2] No such file or directory: 'convert2Yolo'\n/root/2024 winter/DL_tutorial/posts/yolov5\nERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\nNote: you may need to restart the kernel to use updated packages.\n!wget 'https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79'\n\n--2024-02-08 00:13:13--  https://drive.google.com/uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.google.com (drive.google.com)... 142.250.206.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|142.250.206.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79 [following]\n--2024-02-08 00:13:14--  https://drive.usercontent.google.com/download?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.76.129, 2404:6800:400a:80e::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.76.129|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 134 [application/octet-stream]\nSaving to: ‘uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79’\n\nuc?id=1WCodEuV9giZ9 100%[===================&gt;]     134  --.-KB/s    in 0s      \n\n2024-02-08 00:13:15 (13.2 MB/s) - ‘uc?id=1WCodEuV9giZ9SmjHzfCbJ8GuBaa0Gc79’ saved [134/134]\n# trainval 데이터 yolo format 변환\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n# test 데이터 yolo format 변환\n!python3 /root/2024winter/DL_tutorial/posts/convert2Yolo/example.py --datasets VOC --img_path /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/JPEGImages/ --label /root/2024winter/DL_tutorial/posts/pascal_datasets/test/VOCdevkit/VOC2007/Annotations/ --convert_output_path /root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/test2007 --img_type \".jpg\" --manifest_path /root/2024winter/DL_tutorial/posts/ --cls_list_file /root/2024winter/DL_tutorial/posts/convert2Yolo/voc.names\n\n\nVOC Parsing:   |████████████████████████████████████████| 100.0% (5011/5011)  Complete\n\n\nYOLO Generating:|████████████████████████████████████████| 100.0% (5011/5011)  Completeerating:|█████████-------------------------------| 22.7% (1136/5011)  Complete\n\n\nYOLO Saving:   |████████████████████████████████████████| 100.0% (5011/5011)  Complete\n\n\nVOC Parsing:   |████████████████████████████████████████| 100.0% (4952/4952)  Complete\n\n\nYOLO Generating:|████████████████████████████████████████| 100.0% (4952/4952)  Complete\n\n\nYOLO Saving:   |████████████████████████████████████████| 100.0% (4952/4952)  Complete\n# Pasal VOC 제공 파일로 train, val 라벨 분할\nimport shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets/trainval/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\nwith open(path) as f:\n    image_ids = f.read().strip().split()\n    for id in image_ids:\n        ori_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/train2007'\n        mv_path = '/root/2024winter/DL_tutorial/posts/pascal_datasets/VOC/labels/val2007'\n        shutil.move(f\"{ori_path}/{id}.txt\", f\"{mv_path}/{id}.txt\")\n# train / val / test 이미지 파일 복사\nimport os, shutil\npath = '/root/2024winter/DL_tutorial/posts/pascal_datasets'\nfor folder, subset in ('trainval', 'train2007'), ('trainval', 'val2007'), ('test', 'test2007'):\n    ex_imgs_path = f'{path}/{folder}/VOCdevkit/VOC2007/JPEGImages'\n    label_path = f'{path}/VOC/labels/{subset}'\n    img_path = f'{path}/VOC/images/{subset}'\n    print(subset,\": \", len(os.listdir(label_path)))\n    for lbs_list in os.listdir(label_path):\n        shutil.move(os.path.join(ex_imgs_path,lbs_list.split('.')[0]+'.jpg'), os.path.join(img_path,lbs_list.split('.')[0]+'.jpg'))\n    \n\ntrain2007 :  2501\nval2007 :  2510\ntest2007 :  4952"
  },
  {
    "objectID": "posts/CV_ObjectDetection_0.html#커스텀-데이터-준비",
    "href": "posts/CV_ObjectDetection_0.html#커스텀-데이터-준비",
    "title": "CV_ObjectDetection_0",
    "section": "커스텀 데이터 준비",
    "text": "커스텀 데이터 준비\n\n# 라벨링한 커스텀 데이터셋 다운로드\n!wget 'https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ'\n\n--2024-02-08 09:16:46--  https://drive.google.com/uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ [following]\n--2024-02-08 09:16:46--  https://drive.usercontent.google.com/download?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.222.33, 2404:6800:4004:818::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.222.33|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 443876 (433K) [application/octet-stream]\nSaving to: ‘uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ’\n\nuc?id=1avRwo9y3M1Op 100%[===================&gt;] 433.47K  1.34MB/s    in 0.3s    \n\n2024-02-08 09:16:48 (1.34 MB/s) - ‘uc?id=1avRwo9y3M1OpCh3tYTAD75Bolb-xlEJZ’ saved [443876/443876]\n\n\n\n\n# 파일 압축 해제\nimport zipfile\nwith zipfile.ZipFile(\"04_detection_custom_datasets.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"04_detection_custom_datasets\")\n\n\n# Yolo 6.2 release version 클론\n%cd /root/2024\\ winter/DL_tutorial/posts\n! git clone -b v6.2 https://github.com/ultralytics/yolov5.git\n%cd yolov5\n# 필수 라이브러리 다운로드\n!pip install numpy==1.23.0\n%pip install -qr requirements.txt\nimport torch\nimport utils\ndisplay = utils.notebook_init()\n\nYOLOv5 🚀 v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\n\nSetup complete ✅ (128 CPUs, 503.7 GB RAM, 1001.9/1757.9 GB disk)\n\n\n\n# Weights & Biases 셋팅\n%pip install -q wandb\nimport wandb\nwandb.login()\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nNote: you may need to restart the kernel to use updated packages.\n\n\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n# 재현을 위한 랜덤 시드 고정\nimport random\nimport numpy as np\nimport torch\n\nseed = 2024\ndeterministic = True\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nif deterministic:\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n%cd /root/2024winter/DL_tutorial/posts/yolov5\n# Pasal VOC 데이터로 YOLO v5 훈련\n!python train.py --img 640 --batch 32 --epochs 30 --data custom_voc.yaml --weights '' --cfg yolov5s.yaml --seed 2024\n\n/root/2024winter/DL_tutorial/posts/yolov5\nwandb: Currently logged in as: dlwjdwo1109. Use `wandb login --relogin` to force relogin\ntrain: weights=, cfg=yolov5s.yaml, data=custom_voc.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=2024, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\ngithub: skipping check (offline), for updates see https://github.com/ultralytics/yolov5\nrequirements: Pillow==7.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install Pillow==7.2.0' skipped (offline)\nrequirements: cycler==0.10.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install cycler==0.10.0' skipped (offline)\nrequirements: kiwisolver==1.0.1 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install kiwisolver==1.0.1' skipped (offline)\nrequirements: matplotlib==2.2.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install matplotlib==2.2.2' skipped (offline)\nrequirements: numpy==1.14.3 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install numpy==1.14.3' skipped (offline)\nrequirements: pyparsing==2.2.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pyparsing==2.2.0' skipped (offline)\nrequirements: python-dateutil==2.7.2 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install python-dateutil==2.7.2' skipped (offline)\nrequirements: pytz==2018.4 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install pytz==2018.4' skipped (offline)\nrequirements: six==1.11.0 not found and is required by YOLOv5, attempting auto-update...\nrequirements: 'pip install six==1.11.0' skipped (offline)\nYOLOv5 🚀 v6.2-0-gd3ea0df8 Python-3.10.13 torch-1.12.1 CUDA:0 (NVIDIA A100-SXM4-80GB MIG 7g.80gb, 81251MiB)\n\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nwandb: Tracking run with wandb version 0.16.3\nwandb: Run data is saved locally in /root/2024winter/DL_tutorial/posts/yolov5/wandb/run-20240209_073845-bf46v602\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run exalted-thunder-9\nwandb: ⭐️ View project at https://wandb.ai/dlwjdwo1109/YOLOv5\nwandb: 🚀 View run at https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\n\nDataset not found ⚠️, missing paths ['/content/pascal_datasets/VOC/images/test2007']\nTraceback (most recent call last):\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 632, in &lt;module&gt;\n    main(opt)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 528, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/train.py\", line 90, in train\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/__init__.py\", line 95, in __init__\n    self.wandb = WandbLogger(self.opt, run_id)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 187, in __init__\n    self.data_dict = check_wandb_dataset(opt.data)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/loggers/wandb/wandb_utils.py\", line 59, in check_wandb_dataset\n    return check_dataset(data_file)\n  File \"/root/2024winter/DL_tutorial/posts/yolov5/utils/general.py\", line 505, in check_dataset\n    raise Exception('Dataset not found ❌')\nException: Dataset not found ❌\nwandb: 🚀 View run exalted-thunder-9 at: https://wandb.ai/dlwjdwo1109/YOLOv5/runs/bf46v602\nwandb: ️⚡ View job at https://wandb.ai/dlwjdwo1109/YOLOv5/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODM4NjQzMg==/version_details/v0\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20240209_073845-bf46v602/logs"
  },
  {
    "objectID": "posts/ANN_2.html",
    "href": "posts/ANN_2.html",
    "title": "ANN_2",
    "section": "",
    "text": "Step 1 : Load libraries & Datasts\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n\n# FashionMNIST 데이터 불러오기\ntraining_data = datasets.FashionMNIST(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = ToTensor()\n)\n\n\ntest_data = datasets.FashionMNIST(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = ToTensor()\n)\n\n\n\nStep 2 : Create DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n\n# Device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\n\ndevice = cuda\n\n\n\n\nEDA\n\nprint(training_data, '\\n--------------------------\\n', test_data)\n\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n--------------------------\n Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f'Feature batch shape: {train_features.size()}')\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\n\n\n\nlen(training_data)\n\n60000\n\n\n\nimg, label = training_data[0]\nplt.imshow(img.squeeze(), cmap='gray')\nprint(f'label={label}')\n\nlabel=9\n\n\n\n\n\n\n\n\n\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\n\nfigure = plt.figure(figsize = (20, 8))\ncols, rows = 5, 2\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    print(labels_map[label])\n    plt.axis('off')\n    plt.imshow(img.squeeze(), cmap='gray')\nplt.show()\n\nBag\nBag\nTrouser\nAnkle Boot\nSneaker\nCoat\nCoat\nBag\nAnkle Boot\nShirt\n\n\n\n\n\n\n\n\n\n\n\nStep 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.classifier = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 10)\n        )\n    def forward(self, x):\n        x = self.flatten(x)\n        output = self.classifier(x)\n        return output\n\n\n\nStep 4 : Create Model instacne\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\n\nModel test\n\nX = torch.rand(1, 28, 28, device=device)\noutput = model(X)\nprint(f'모델 출력 결과: {output}\\n')\npred_probab = nn.Softmax(dim=1)(output)\nprint(f'Softmax 결과: {pred_probab}\\n')\ny_pred = pred_probab.argmax()\nprint(y_pred)\n\n모델 출력 결과: tensor([[ 0.2122, -0.0533,  0.4609, -0.1348, -0.2897,  0.0426,  0.2330, -0.0539,\n         -0.2371,  0.3135]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\nSoftmax 결과: tensor([[0.1144, 0.0878, 0.1468, 0.0809, 0.0693, 0.0966, 0.1169, 0.0877, 0.0730,\n         0.1267]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)\n\ntensor(2, device='cuda:0')\n\n\n\n\nStep 5 : Model compile\n\n# Loss\nloss = nn.CrossEntropyLoss()\n# Optimizer\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\nStep 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:7&gt;f} [{current:&gt;5d}]/{size:5d}')\n\n\n\nStep 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')\n\n\n\nStep 8 : Run model\n\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n--------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\nprint(\"Done\")\n\nEpoch 1 \n--------------------------\nloss: 0.431373 [    0]/60000\nloss: 0.587412 [ 6400]/60000\nloss: 0.583656 [12800]/60000\nloss: 0.526607 [19200]/60000\nloss: 0.425904 [25600]/60000\nloss: 0.559104 [32000]/60000\nloss: 0.255318 [38400]/60000\nloss: 0.401609 [44800]/60000\nloss: 0.359597 [51200]/60000\nloss: 0.415519 [57600]/60000\nTest Error: \n Accuracy: 84.0%, Avg loss: 0.441000\n\nEpoch 2 \n--------------------------\nloss: 0.396597 [    0]/60000\nloss: 0.282736 [ 6400]/60000\nloss: 0.348952 [12800]/60000\nloss: 0.394269 [19200]/60000\nloss: 0.309174 [25600]/60000\nloss: 0.406731 [32000]/60000\nloss: 0.255115 [38400]/60000\nloss: 0.464652 [44800]/60000\nloss: 0.266001 [51200]/60000\nloss: 0.316354 [57600]/60000\nTest Error: \n Accuracy: 86.0%, Avg loss: 0.396377\n\nEpoch 3 \n--------------------------\nloss: 0.343163 [    0]/60000\nloss: 0.417957 [ 6400]/60000\nloss: 0.361057 [12800]/60000\nloss: 0.290214 [19200]/60000\nloss: 0.439867 [25600]/60000\nloss: 0.573623 [32000]/60000\nloss: 0.358405 [38400]/60000\nloss: 0.280711 [44800]/60000\nloss: 0.403463 [51200]/60000\nloss: 0.364402 [57600]/60000\nTest Error: \n Accuracy: 85.9%, Avg loss: 0.389022\n\nEpoch 4 \n--------------------------\nloss: 0.269444 [    0]/60000\nloss: 0.317166 [ 6400]/60000\nloss: 0.371522 [12800]/60000\nloss: 0.389202 [19200]/60000\nloss: 0.290166 [25600]/60000\nloss: 0.557429 [32000]/60000\nloss: 0.463130 [38400]/60000\nloss: 0.320192 [44800]/60000\nloss: 0.230014 [51200]/60000\nloss: 0.221253 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.380155\n\nEpoch 5 \n--------------------------\nloss: 0.378960 [    0]/60000\nloss: 0.463637 [ 6400]/60000\nloss: 0.203711 [12800]/60000\nloss: 0.376729 [19200]/60000\nloss: 0.376216 [25600]/60000\nloss: 0.302987 [32000]/60000\nloss: 0.401179 [38400]/60000\nloss: 0.283290 [44800]/60000\nloss: 0.314023 [51200]/60000\nloss: 0.380436 [57600]/60000\nTest Error: \n Accuracy: 86.1%, Avg loss: 0.391101\n\nEpoch 6 \n--------------------------\nloss: 0.324956 [    0]/60000\nloss: 0.376954 [ 6400]/60000\nloss: 0.286267 [12800]/60000\nloss: 0.302243 [19200]/60000\nloss: 0.253790 [25600]/60000\nloss: 0.259367 [32000]/60000\nloss: 0.489078 [38400]/60000\nloss: 0.244414 [44800]/60000\nloss: 0.335011 [51200]/60000\nloss: 0.284465 [57600]/60000\nTest Error: \n Accuracy: 86.4%, Avg loss: 0.392578\n\nEpoch 7 \n--------------------------\nloss: 0.470740 [    0]/60000\nloss: 0.352301 [ 6400]/60000\nloss: 0.215639 [12800]/60000\nloss: 0.214226 [19200]/60000\nloss: 0.198628 [25600]/60000\nloss: 0.290468 [32000]/60000\nloss: 0.396786 [38400]/60000\nloss: 0.337734 [44800]/60000\nloss: 0.209183 [51200]/60000\nloss: 0.446850 [57600]/60000\nTest Error: \n Accuracy: 86.5%, Avg loss: 0.375429\n\nEpoch 8 \n--------------------------\nloss: 0.325714 [    0]/60000\nloss: 0.252096 [ 6400]/60000\nloss: 0.266274 [12800]/60000\nloss: 0.404169 [19200]/60000\nloss: 0.398270 [25600]/60000\nloss: 0.240059 [32000]/60000\nloss: 0.345725 [38400]/60000\nloss: 0.339798 [44800]/60000\nloss: 0.232539 [51200]/60000\nloss: 0.371980 [57600]/60000\nTest Error: \n Accuracy: 87.3%, Avg loss: 0.363998\n\nEpoch 9 \n--------------------------\nloss: 0.275302 [    0]/60000\nloss: 0.459145 [ 6400]/60000\nloss: 0.477752 [12800]/60000\nloss: 0.368110 [19200]/60000\nloss: 0.166666 [25600]/60000\nloss: 0.340989 [32000]/60000\nloss: 0.412024 [38400]/60000\nloss: 0.255185 [44800]/60000\nloss: 0.370468 [51200]/60000\nloss: 0.383511 [57600]/60000\nTest Error: \n Accuracy: 87.2%, Avg loss: 0.362548\n\nEpoch 10 \n--------------------------\nloss: 0.163738 [    0]/60000\nloss: 0.234758 [ 6400]/60000\nloss: 0.498840 [12800]/60000\nloss: 0.502605 [19200]/60000\nloss: 0.460932 [25600]/60000\nloss: 0.459421 [32000]/60000\nloss: 0.243460 [38400]/60000\nloss: 0.231028 [44800]/60000\nloss: 0.239964 [51200]/60000\nloss: 0.273228 [57600]/60000\nTest Error: \n Accuracy: 87.0%, Avg loss: 0.363198\n\nDone\n\n\n\n\nStep 9 : Save & load model\n\nparameter만 저장하고 불러오기\n\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n\nmodel2 = NeuralNetwork().to(device)\nprint(model2)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (classifier): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n\n\n\nmodel2.load_state_dict(torch.load('model_weights.pth'))\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel2.eval()\ntest_loop(test_dataloader, model2, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797\n\n\n\n\n\n\nModel 전체를 저장하고 불러오기\n\ntorch.save(model, 'model.pth')\n\n\nmodel3 = torch.load('model.pth')\n\n\nmodel3.eval()\ntest_loop(test_dataloader, model3, loss)\n\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334797"
  },
  {
    "objectID": "posts/ANN_1.html",
    "href": "posts/ANN_1.html",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-1-import",
    "href": "posts/ANN_1.html#step-1-import",
    "title": "ANN_1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import  TensorDataset, DataLoader"
  },
  {
    "objectID": "posts/ANN_1.html#step-2-create-dataloader",
    "href": "posts/ANN_1.html#step-2-create-dataloader",
    "title": "ANN_1",
    "section": "Step 2 : Create DataLoader",
    "text": "Step 2 : Create DataLoader\n\n# 데이터 불러오기\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['label'] = iris.target\n\n# 데이터 분할\ny = df['label']\nX = df.drop(['label'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values,\n                                                   random_state=42, stratify=y)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.int64)\ny_test = torch.tensor(y_test, dtype=torch.int64)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
  },
  {
    "objectID": "posts/ANN_1.html#step-3-set-network-structure",
    "href": "posts/ANN_1.html#step-3-set-network-structure",
    "title": "ANN_1",
    "section": "Step 3 : Set Network Structure",
    "text": "Step 3 : Set Network Structure\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.input_layer = nn.Linear(4, 16)\n        self.hidden_layer1 = nn.Linear(16, 32)\n        self.output_layer = nn.Linear(32, 3)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.relu(self.input_layer(x))\n        out = self.relu(self.hidden_layer1(out))\n        out = self.output_layer(out)\n        return out"
  },
  {
    "objectID": "posts/ANN_1.html#step-4-create-model-instance",
    "href": "posts/ANN_1.html#step-4-create-model-instance",
    "title": "ANN_1",
    "section": "Step 4 : Create Model instance",
    "text": "Step 4 : Create Model instance\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device = {device}')\nmodel = NeuralNetwork().to(device)\n\ndevice = cuda"
  },
  {
    "objectID": "posts/ANN_1.html#step-5-model-compile",
    "href": "posts/ANN_1.html#step-5-model-compile",
    "title": "ANN_1",
    "section": "Step 5 : Model compile",
    "text": "Step 5 : Model compile\n\n# 모델 컴파일\nlearning_rate = 0.001\nloss = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
  },
  {
    "objectID": "posts/ANN_1.html#step-6-set-train-loop",
    "href": "posts/ANN_1.html#step-6-set-train-loop",
    "title": "ANN_1",
    "section": "Step 6 : Set train loop",
    "text": "Step 6 : Set train loop\n\ndef train_loop(train_loader, model, loss_fn, optimizer):\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss\n        loss = loss_fn(pred, y)\n\n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss, current = loss.item(), batch * len(X)\n        print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/ANN_1.html#step-7-set-test-loop",
    "href": "posts/ANN_1.html#step-7-set-test-loop",
    "title": "ANN_1",
    "section": "Step 7 : Set test loop",
    "text": "Step 7 : Set test loop\n\ndef test_loop(test_loader, model, loss_fn):\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f'Test Error : \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:8f}\\n')"
  },
  {
    "objectID": "posts/ANN_1.html#step-8-run-model",
    "href": "posts/ANN_1.html#step-8-run-model",
    "title": "ANN_1",
    "section": "Step 8 : Run model",
    "text": "Step 8 : Run model\n\n# 모델 실행\nepochs = 10\n\nfor i in range(epochs):\n    print(f'Epoch {i+1} \\n------------------------------')\n    train_loop(train_dataloader, model, loss, optimizer)\n    test_loop(test_dataloader, model, loss)\n\nprint(\"Done!\")\n\nEpoch 1 \n------------------------------\nloss: 1.104175  [    0]/  112\nloss: 1.063668  [   10]/  112\nloss: 1.001417  [   20]/  112\nloss: 0.974181  [   30]/  112\nloss: 1.099196  [   40]/  112\nloss: 1.131157  [   50]/  112\nloss: 1.029493  [   60]/  112\nloss: 1.046669  [   70]/  112\nloss: 0.991781  [   80]/  112\nloss: 0.980483  [   90]/  112\nloss: 0.995154  [  100]/  112\nloss: 1.178336  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 1.013189\n\nEpoch 2 \n------------------------------\nloss: 0.998811  [    0]/  112\nloss: 1.072228  [   10]/  112\nloss: 0.861224  [   20]/  112\nloss: 1.145309  [   30]/  112\nloss: 0.923511  [   40]/  112\nloss: 1.029394  [   50]/  112\nloss: 1.038562  [   60]/  112\nloss: 0.923659  [   70]/  112\nloss: 1.022684  [   80]/  112\nloss: 0.942879  [   90]/  112\nloss: 1.011915  [  100]/  112\nloss: 0.944836  [   22]/  112\nTest Error : \n Accuracy: 34.2%, Avg loss: 0.978484\n\nEpoch 3 \n------------------------------\nloss: 0.973278  [    0]/  112\nloss: 0.857192  [   10]/  112\nloss: 1.025384  [   20]/  112\nloss: 0.974613  [   30]/  112\nloss: 0.966374  [   40]/  112\nloss: 0.901531  [   50]/  112\nloss: 1.019788  [   60]/  112\nloss: 0.999571  [   70]/  112\nloss: 0.854874  [   80]/  112\nloss: 1.000906  [   90]/  112\nloss: 0.900930  [  100]/  112\nloss: 1.044610  [   22]/  112\nTest Error : \n Accuracy: 39.5%, Avg loss: 0.925748\n\nEpoch 4 \n------------------------------\nloss: 1.016448  [    0]/  112\nloss: 0.965601  [   10]/  112\nloss: 0.949101  [   20]/  112\nloss: 0.891714  [   30]/  112\nloss: 0.936245  [   40]/  112\nloss: 0.809627  [   50]/  112\nloss: 0.898013  [   60]/  112\nloss: 0.915918  [   70]/  112\nloss: 0.908495  [   80]/  112\nloss: 0.837218  [   90]/  112\nloss: 0.851940  [  100]/  112\nloss: 1.014478  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.887178\n\nEpoch 5 \n------------------------------\nloss: 0.929125  [    0]/  112\nloss: 0.860350  [   10]/  112\nloss: 0.894982  [   20]/  112\nloss: 0.848951  [   30]/  112\nloss: 0.874831  [   40]/  112\nloss: 0.867019  [   50]/  112\nloss: 0.865059  [   60]/  112\nloss: 0.850359  [   70]/  112\nloss: 0.778505  [   80]/  112\nloss: 0.894542  [   90]/  112\nloss: 0.853558  [  100]/  112\nloss: 0.779387  [   22]/  112\nTest Error : \n Accuracy: 68.4%, Avg loss: 0.844504\n\nEpoch 6 \n------------------------------\nloss: 0.901229  [    0]/  112\nloss: 0.800752  [   10]/  112\nloss: 0.798276  [   20]/  112\nloss: 0.743961  [   30]/  112\nloss: 0.844832  [   40]/  112\nloss: 0.806112  [   50]/  112\nloss: 0.788561  [   60]/  112\nloss: 0.879695  [   70]/  112\nloss: 0.851877  [   80]/  112\nloss: 0.764796  [   90]/  112\nloss: 0.764707  [  100]/  112\nloss: 0.804097  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.798580\n\nEpoch 7 \n------------------------------\nloss: 0.797247  [    0]/  112\nloss: 0.836456  [   10]/  112\nloss: 0.829977  [   20]/  112\nloss: 0.725152  [   30]/  112\nloss: 0.715440  [   40]/  112\nloss: 0.670871  [   50]/  112\nloss: 0.751514  [   60]/  112\nloss: 0.811848  [   70]/  112\nloss: 0.754924  [   80]/  112\nloss: 0.847931  [   90]/  112\nloss: 0.647542  [  100]/  112\nloss: 0.773911  [   22]/  112\nTest Error : \n Accuracy: 65.8%, Avg loss: 0.743698\n\nEpoch 8 \n------------------------------\nloss: 0.745911  [    0]/  112\nloss: 0.756747  [   10]/  112\nloss: 0.697434  [   20]/  112\nloss: 0.770861  [   30]/  112\nloss: 0.675138  [   40]/  112\nloss: 0.756205  [   50]/  112\nloss: 0.699248  [   60]/  112\nloss: 0.632746  [   70]/  112\nloss: 0.680969  [   80]/  112\nloss: 0.731905  [   90]/  112\nloss: 0.697679  [  100]/  112\nloss: 0.612848  [   22]/  112\nTest Error : \n Accuracy: 73.7%, Avg loss: 0.692100\n\nEpoch 9 \n------------------------------\nloss: 0.709632  [    0]/  112\nloss: 0.619151  [   10]/  112\nloss: 0.675694  [   20]/  112\nloss: 0.595196  [   30]/  112\nloss: 0.690987  [   40]/  112\nloss: 0.679168  [   50]/  112\nloss: 0.648694  [   60]/  112\nloss: 0.642780  [   70]/  112\nloss: 0.661847  [   80]/  112\nloss: 0.611258  [   90]/  112\nloss: 0.658153  [  100]/  112\nloss: 0.781440  [   22]/  112\nTest Error : \n Accuracy: 71.1%, Avg loss: 0.642173\n\nEpoch 10 \n------------------------------\nloss: 0.675161  [    0]/  112\nloss: 0.570343  [   10]/  112\nloss: 0.511772  [   20]/  112\nloss: 0.692174  [   30]/  112\nloss: 0.523068  [   40]/  112\nloss: 0.557191  [   50]/  112\nloss: 0.715832  [   60]/  112\nloss: 0.556040  [   70]/  112\nloss: 0.620830  [   80]/  112\nloss: 0.581826  [   90]/  112\nloss: 0.604469  [  100]/  112\nloss: 0.707001  [   22]/  112\nTest Error : \n Accuracy: 81.6%, Avg loss: 0.595252\n\nDone!"
  },
  {
    "objectID": "posts/AlexNet.html",
    "href": "posts/AlexNet.html",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "href": "posts/AlexNet.html#step-1-load-libraries-datasets",
    "title": "AlexNet",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nfrom torchvision import datasets\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\n\n\n# Datasets\ntrain_img = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transforms.ToTensor(),\n)\n\ntest_img = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\nExtracting data/cifar-10-python.tar.gz to data\nFiles already downloaded and verified\n\n\n100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [00:18&lt;00:00, 9081435.00it/s]"
  },
  {
    "objectID": "posts/AlexNet.html#step-2-data-preprocessing",
    "href": "posts/AlexNet.html#step-2-data-preprocessing",
    "title": "AlexNet",
    "section": "Step 2 : Data preprocessing",
    "text": "Step 2 : Data preprocessing\n불러온 이미지의 증강을 통해 학습 정확도를 향상시키도록 합니다.\n- RandomCrop\n- RandomHorizontalFlip\n- Normalize\n\nmean = train_img.data.mean(axis=(0,1,2)) / 255\nstd = train_img.data.std(axis=(0,1,2)) / 255\nprint(f'평균: {mean}, 표준편차:{std}')\n\n평균: [0.49139968 0.48215841 0.44653091], 표준편차:[0.24703223 0.24348513 0.26158784]\n\n\n\ntrain_img.data.shape , test_img.data.shape\n\n((50000, 32, 32, 3), (10000, 32, 32, 3))\n\n\n\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n    transforms.RandomCrop(size=train_img.data.shape[1], padding=4),\n    transforms.RandomHorizontalFlip(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\n\ntrain_img2  = datasets.CIFAR10(\n    root = 'data',\n    train = True,\n    download = True,\n    transform = transform_train,\n)\n\ntest_img2 = datasets.CIFAR10(\n    root = 'data',\n    train = False,\n    download = True,\n    transform = transform_test,\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified"
  },
  {
    "objectID": "posts/AlexNet.html#step-3-set-hyperparameters",
    "href": "posts/AlexNet.html#step-3-set-hyperparameters",
    "title": "AlexNet",
    "section": "Step 3 : Set hyperparameters",
    "text": "Step 3 : Set hyperparameters\n\nepochs = 10\nbatch_sizes = 128\nlearning_rate = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using Device:\", device)\n\nUsing Device: cuda"
  },
  {
    "objectID": "posts/AlexNet.html#step-4-create-dataloader",
    "href": "posts/AlexNet.html#step-4-create-dataloader",
    "title": "AlexNet",
    "section": "Step 4 : Create DataLoader",
    "text": "Step 4 : Create DataLoader\n\n# Create DataLoader\ntrain_loader = DataLoader(train_img2, batch_size = batch_sizes, shuffle = True)\ntest_loader = DataLoader(test_img2, batch_size = batch_sizes, shuffle = True)"
  },
  {
    "objectID": "posts/AlexNet.html#eda",
    "href": "posts/AlexNet.html#eda",
    "title": "AlexNet",
    "section": "EDA",
    "text": "EDA\n\nprint(train_img, '\\n-----------------------\\n', test_img)\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n-----------------------\n Dataset CIFAR10\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\n\ntrain_features, train_labels = next(iter(train_loader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\nFeature batch shape: torch.Size([128, 3, 32, 32])\nLabels batch shape: torch.Size([128])\n\n\n\nlabels_map = {\n    0: \"plane\",\n    1: \"car\",\n    2: \"bird\",\n    3: \"cat\",\n    4: \"deer\",\n    5: \"dog\",\n    6: \"frog\",\n    7: \"horse\",\n    8: \"ship\",\n    9: \"truck\",\n}\n\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 5, 5\n\nfor i in range(1, cols * rows +1):\n    sample_idx = torch.randint(len(train_img), size=(1,)).item()\n    img, label = train_img[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis('off')\n    plt.imshow(to_pil_image(img))\nplt.show()"
  },
  {
    "objectID": "posts/AlexNet.html#step-5-set-network-structure",
    "href": "posts/AlexNet.html#step-5-set-network-structure",
    "title": "AlexNet",
    "section": "Step 5 : Set Network Structure",
    "text": "Step 5 : Set Network Structure\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 4096),\n            nn.Dropout(0.5),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return(x)"
  },
  {
    "objectID": "posts/AlexNet.html#step-6-create-model-instance",
    "href": "posts/AlexNet.html#step-6-create-model-instance",
    "title": "AlexNet",
    "section": "Step 6 : Create Model instance",
    "text": "Step 6 : Create Model instance\n\n# Create Moedl instance\nmodel = AlexNet().to(device)\nprint(model)\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=256, out_features=4096, bias=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/AlexNet.html#step-7-model-compile",
    "href": "posts/AlexNet.html#step-7-model-compile",
    "title": "AlexNet",
    "section": "Step 7 : Model compile",
    "text": "Step 7 : Model compile\n\n# loss\nloss = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "objectID": "posts/AlexNet.html#step-8-set-train-loop",
    "href": "posts/AlexNet.html#step-8-set-train-loop",
    "title": "AlexNet",
    "section": "Step 8 : Set train loop",
    "text": "Step 8 : Set train loop\n\ndef train(train_loader, model, loss_fn, optimizer):\n    model.train()\n\n    size = len(train_loader.dataset)\n\n    for batch, (X,y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n\n        # loss calculation\n        loss = loss_fn(pred, y)\n        \n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f'loss: {loss:&gt;7f}  [{current:&gt;5d}]/{size:5d}')"
  },
  {
    "objectID": "posts/AlexNet.html#step-9-set-test-loop",
    "href": "posts/AlexNet.html#step-9-set-test-loop",
    "title": "AlexNet",
    "section": "Step 9 : Set test loop",
    "text": "Step 9 : Set test loop\n\ndef test(test_loader, model, loss_fn):\n    model.eval()\n\n    size = len(test_loader.dataset)\n    num_batches = len(test_loader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss:  {test_loss:8f}\\n\")"
  },
  {
    "objectID": "posts/AlexNet.html#step-10-run-model",
    "href": "posts/AlexNet.html#step-10-run-model",
    "title": "AlexNet",
    "section": "Step 10 : Run Model",
    "text": "Step 10 : Run Model\n\nfor i in range(epochs):\n    print(f\"epochs {i+1} \\n-----------------------------\")\n    train(train_loader, model, loss, optimizer)\n    test(test_loader, model, loss)\nprint('Done!')\n\nepochs 1 \n-----------------------------\nloss: 2.300106  [    0]/50000\nloss: 1.961412  [12800]/50000\nloss: 1.877241  [25600]/50000\nloss: 1.800212  [38400]/50000\nTest Error: \n Accuracy: 31.4%, Avg loss:  1.762373\n\nepochs 2 \n-----------------------------\nloss: 1.795186  [    0]/50000\nloss: 1.576234  [12800]/50000\nloss: 1.639605  [25600]/50000\nloss: 1.649888  [38400]/50000\nTest Error: \n Accuracy: 44.0%, Avg loss:  1.494407\n\nepochs 3 \n-----------------------------\nloss: 1.534194  [    0]/50000\nloss: 1.496532  [12800]/50000\nloss: 1.524187  [25600]/50000\nloss: 1.440067  [38400]/50000\nTest Error: \n Accuracy: 49.1%, Avg loss:  1.400005\n\nepochs 4 \n-----------------------------\nloss: 1.554884  [    0]/50000\nloss: 1.389567  [12800]/50000\nloss: 1.310677  [25600]/50000\nloss: 1.434475  [38400]/50000\nTest Error: \n Accuracy: 51.9%, Avg loss:  1.308864\n\nepochs 5 \n-----------------------------\nloss: 1.384561  [    0]/50000\nloss: 1.448307  [12800]/50000\nloss: 1.549824  [25600]/50000\nloss: 1.296622  [38400]/50000\nTest Error: \n Accuracy: 54.0%, Avg loss:  1.252349\n\nepochs 6 \n-----------------------------\nloss: 1.339483  [    0]/50000\nloss: 1.216769  [12800]/50000\nloss: 1.371759  [25600]/50000\nloss: 1.374979  [38400]/50000\nTest Error: \n Accuracy: 56.6%, Avg loss:  1.201752\n\nepochs 7 \n-----------------------------\nloss: 1.369484  [    0]/50000\nloss: 1.234365  [12800]/50000\nloss: 1.124013  [25600]/50000\nloss: 1.045216  [38400]/50000\nTest Error: \n Accuracy: 55.2%, Avg loss:  1.240022\n\nepochs 8 \n-----------------------------\nloss: 1.255546  [    0]/50000\nloss: 1.335250  [12800]/50000\nloss: 1.238086  [25600]/50000\nloss: 1.284560  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.171398\n\nepochs 9 \n-----------------------------\nloss: 1.246493  [    0]/50000\nloss: 1.291452  [12800]/50000\nloss: 1.238421  [25600]/50000\nloss: 1.272063  [38400]/50000\nTest Error: \n Accuracy: 59.5%, Avg loss:  1.131185\n\nepochs 10 \n-----------------------------\nloss: 1.220213  [    0]/50000\nloss: 1.313022  [12800]/50000\nloss: 1.362849  [25600]/50000\nloss: 1.224882  [38400]/50000\nTest Error: \n Accuracy: 58.5%, Avg loss:  1.141315\n\nDone!\n\n\n/root/anaconda3/envs/py/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv2d(input, weight, bias, self.stride,"
  },
  {
    "objectID": "posts/AlexNet.html#step-11-confusion-matrix",
    "href": "posts/AlexNet.html#step-11-confusion-matrix",
    "title": "AlexNet",
    "section": "Step 11 : Confusion Matrix",
    "text": "Step 11 : Confusion Matrix\n\nimport itertools\ndef plot_confusion_matrix(cm, target_names=None, cmap=None, \n                          normalize=True, labels=True, title='Confusion matrix'):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    \n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names)\n        plt.yticks(tick_marks, target_names)\n    \n    if labels:\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n            else:\n                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                         horizontalalignment=\"center\",\n                         color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f};\\\n                         misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n\nfrom sklearn.metrics import confusion_matrix\n\nmodel.eval()\nylabel = []\nypred_label = []\n\nfor batch_idx, (inputs, targets) in enumerate(test_loader):\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    _, predicted = outputs.max(1)\n    ylabel = np.concatenate((ylabel, targets.cpu().numpy()))\n    ypred_label = np.concatenate((ypred_label, predicted.cpu().numpy()))\n\ncnf_matrix = confusion_matrix(ylabel, ypred_label)\n\n\nplot_confusion_matrix(cnf_matrix, \n                      target_names=labels_map.values(), \n                      title='Confusion matrix, trained by AlexNet')"
  },
  {
    "objectID": "posts/CV_classification_0.html",
    "href": "posts/CV_classification_0.html",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#import-library",
    "href": "posts/CV_classification_0.html#import-library",
    "title": "CV_classification_0",
    "section": "",
    "text": "# etc\nimport os, sys, zipfile\nimport glob\nimport csv\nimport cv2\nimport tqdm\nfrom typing import Tuple, List, Dict\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# torch library\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n# torchvision library\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.optim as optim"
  },
  {
    "objectID": "posts/CV_classification_0.html#dataset",
    "href": "posts/CV_classification_0.html#dataset",
    "title": "CV_classification_0",
    "section": "- Dataset",
    "text": "- Dataset\n\n# 데이터 전처리\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        #transforms.RandomChoice([\n        #    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n        #    transforms.RandomResizedCrop(224),\n        #    transforms.RandomAffine(\n        #        degrees=15, translate=(0.2, 0.2),\n        #        scale = (0.8, 1.2), shear=15, resample=Image.BILINEAR)\n        #]),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\nval_transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\n\n# 배치 사이즈와 train:validation 비율 정의\nbatch_size = 256\nval_size = 0.2\n\n# torchvision에서 제공하는 CIFAR10 학습 데이터셋 다운로드\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download=True, transform=train_transform)\nval_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                             download = True, transform=val_transform)\n\n# train 데이터에서 일정 비율 validation data 분리\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nsplit = int(np.floor(val_size * num_train))\ntrain_idx, val_idx = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n# 데이터로더 정의\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \nsampler=train_sampler, num_workers=2)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n\n\n\n                                          sampler=val_sampler, num_workers=2)\n\n# torchvision에서 제공하는 CIFAR10 테스트 데이터셋 다운로드\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                           download=True, transform=val_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                           shuffle=False, num_workers=2)\n\n# 클래스 정의\nclasses = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n# 데이터셋 확인\ntrain_dataset\n\nDataset CIFAR10\n    Number of datapoints: 50000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n           )\n\n\n\n# 이미지 데이터 시각화\ndef imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# 학습 이미지 얻기\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n# 이미지 출력\nimshow(torchvision.utils.make_grid(images))\n# 라벨 프린트\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\ncat   plane plane cat   frog  dog   deer  dog   car   bird  plane dog   ship  deer  frog  frog  horse cat   horse truck frog  horse car   plane ship  frog  horse frog  bird  horse plane truck frog  frog  deer  dog   plane car   plane truck car   dog   frog  deer  cat   frog  bird  ship  dog   ship  frog  plane plane ship  bird  car   car   cat   cat   horse truck ship  ship  plane car   truck plane dog   cat   deer  ship  dog   plane frog  horse cat   dog   bird  plane bird  dog   ship  ship  truck bird  horse horse plane truck bird  horse deer  deer  deer  horse horse dog   ship  ship  ship  cat   truck deer  dog   deer  dog   plane cat   cat   bird  car   car   horse deer  plane truck bird  plane ship  deer  ship  truck dog   horse ship  deer  cat   truck deer  dog   ship  frog  dog   car   car   plane deer  dog   plane horse dog   truck bird  dog   deer  bird  bird  bird  dog   car   dog   plane deer  horse plane truck car   plane bird  frog  plane frog  truck truck bird  bird  cat   dog   bird  dog   car   horse cat   car   frog  ship  truck frog  cat   horse plane ship  horse car   cat   cat   horse ship  truck dog   frog  plane horse plane deer  frog  ship  cat   plane cat   frog  car   bird  horse truck dog   plane horse plane frog  truck ship  cat   bird  frog  plane ship  bird  frog  car   frog  truck bird  ship  horse horse ship  bird  plane horse bird  cat   car   car   dog   frog  bird  deer  car   ship  frog  truck ship  cat   truck ship  ship  dog   ship  plane frog  dog   dog   car   plane car  \n\n\n\n# 테스트를 위한 Custom Dataset 다운로드\n!wget https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\n\n--2024-02-05 04:58:20--  https://drive.google.com/uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.google.com (drive.google.com)... 172.217.161.206, 2404:6800:400a:80b::200e\nConnecting to drive.google.com (drive.google.com)|172.217.161.206|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc [following]\n--2024-02-05 04:58:21--  https://drive.usercontent.google.com/download?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.207.97, 2404:6800:400a:805::2001\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.207.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 77122 (75K) [application/octet-stream]\nSaving to: ‘uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1’\n\nuc?id=1GTES_wxB8b-j 100%[===================&gt;]  75.31K  --.-KB/s    in 0.1s    \n\n2024-02-05 04:58:22 (506 KB/s) - ‘uc?id=1GTES_wxB8b-jsZIqHgNyV9pEgpMLtfzc.1’ saved [77122/77122]\n\n\n\n\n# 파일 압축 해제\nwith zipfile.ZipFile(\"03_classification_custom_dataset.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"03_classification_custom_dataset\")\n\n\nfor folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n    print(folder)\n\nplane\ndog\n.DS_Store\ncat\nbird\ncar\n\n\n\n# 커스텀 데이터셋 클래스\nclass CUSTOMDataset(Dataset):\n    def __init__(self, mode: str = 'test', transforms: transforms = None):\n        self.mode = mode\n        self.transforms = transforms\n        self.images = []\n        self.labels = []\n\n        for folder in os.listdir('03_classification_custom_dataset/custom_dataset'):\n            files = os.path.join('03_classification_custom_dataset/custom_dataset',folder)\n            if folder == '.DS_Store':\n                continue\n            files_path = os.listdir(files)\n            for file in files_path:\n                self.images.append(os.path.join(files,file))\n                self.labels.append(classes.index(folder))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index: int) -&gt; Tuple[Tensor]:\n        image = Image.open(self.images[index]).convert('RGB')\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        image = np.array(image)\n        label = self.labels[index]\n        return image, label\n                \n            \n\n\n# 커스텀 데이터셋 & 로더\ncustom_dataset = CUSTOMDataset('test', transforms = val_transform)\ncustom_loader = DataLoader(\n    custom_dataset, batch_size=16, shuffle=False, num_workers=2\n)"
  },
  {
    "objectID": "posts/CV_classification_0.html#모델-불러오기",
    "href": "posts/CV_classification_0.html#모델-불러오기",
    "title": "CV_classification_0",
    "section": "- 모델 불러오기",
    "text": "- 모델 불러오기\n\n# 디바이스 체그 & 할당\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ncuda\n\n\n\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\nmodel = model.to(device)\n\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0"
  },
  {
    "objectID": "posts/CV_classification_0.html#로스-함수와-옵티마이저-정의",
    "href": "posts/CV_classification_0.html#로스-함수와-옵티마이저-정의",
    "title": "CV_classification_0",
    "section": "- 로스 함수와 옵티마이저 정의",
    "text": "- 로스 함수와 옵티마이저 정의\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "posts/CV_classification_0.html#학습-검증-테스트-메소드-정의",
    "href": "posts/CV_classification_0.html#학습-검증-테스트-메소드-정의",
    "title": "CV_classification_0",
    "section": "- 학습, 검증, 테스트 메소드 정의",
    "text": "- 학습, 검증, 테스트 메소드 정의\n\ndef train(epoch):\n    train_loss = 0.0\n    model.train()\n    for i, data in enumerate(tqdm.tqdm(train_loader), 0):\n        # 입력 데이터 가져오기 data: [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # parameter gradients를 제로화\n        optimizer.zero_grad()\n\n        # 입력 이미지에 대한 출력 생성\n        outputs = model(inputs)\n\n        # 손실함수 계산 및 업데이트\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    return train_loss\n\n\ndef val():\n    val_loss = 0.0\n    val_accuracy = 0.0\n    with torch.no_grad():\n        # 모델 평가 모드 설정\n        model.eval()\n        for i, data in enumerate(tqdm.tqdm(val_loader), 0):\n            # 입력 데이터 가져오기 data: [inputs, labels]\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # 입력 이미지에 대한 출력 생성\n            outputs = model(inputs)\n\n            # 손실함수 계산\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # 예측 라벨\n            _, predicted = torch.max(outputs, 1)\n\n            # accuracy 계산\n            val_accuracy += (predicted == labels).sum().item()\n\n    return val_loss, val_accuracy\n\n\ndef test(test_loader):\n    correct = 0\n    total = 0\n    correct_class = {classname:0 for classname in classes}\n    total_class = {classname: 0 for classname in classes}\n    model.eval()\n    with torch.no_grad():\n        for data in test_loader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            # 입력 이미지에 대한 출력 생성\n            outputs = model(inputs)\n\n            # 예측 라벨\n            _, predicted = torch.max(outputs.data, 1)\n\n            # 전체 정확도 계산\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # 클래스 별 정확도 계산\n            for label, prediction in zip(labels, predicted):\n                if label == prediction:\n                    correct_class[classes[label]] += 1\n                total_class[classes[label]] += 1\n\n    # 전체 정확도 출력\n    print(f'Accuracy of the network on the 10000 test images: {100*correct//total}%')\n    # 클래스 별 정확도 출력\n    for classname, correct_count in correct_class.items():\n        if total_class[classname] == 0:\n            continue\n        accuracy = 100*float(correct_count) / total_class[classname]\n        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f}%')"
  },
  {
    "objectID": "posts/CV_classification_0.html#학습",
    "href": "posts/CV_classification_0.html#학습",
    "title": "CV_classification_0",
    "section": "- 학습",
    "text": "- 학습\n\n# 학습 epoch 설정\ntrain_epochs = 20\nbest_acc = 0.0\n\n# 모델 저장 경로 정의\nmodel_path = './cifar_resnet.pth'\nfor epoch in range(train_epochs):\n    # 학습 메소드 실행\n    train_loss = train(epoch)\n    print(f'[{epoch+1}] loss: {train_loss / len(train_loader):.3f}')\n    # 검증 메소드 실행\n    val_loss, val_acc = val()\n    valid_acc = val_acc / (len(val_loader)*batch_size)\n    print(f'[{epoch+1}] loss: {val_loss/len(val_loader):.3f} acc:{valid_acc:.3f}')\n    # 정확도가 기존 베스트를 갱신할 경우 모델 저장\n    if valid_acc &gt;= best_acc:\n        best_acc = valid_acc\n        torch.save(model.state_dict(), model_path)\n\nprint('Done!!')\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:18&lt;00:00,  8.40it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 12.78it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.41it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.88it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.06it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.33it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.16it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 12.92it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.40it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.92it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.23it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.26it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.37it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 16.28it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.61it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.54it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.49it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.96it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.89it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.95it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:14&lt;00:00, 10.85it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.73it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.81it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.84it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.24it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.87it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:13&lt;00:00, 11.55it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 14.24it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:14&lt;00:00, 11.16it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.24it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.61it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.80it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:17&lt;00:00,  8.87it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 13.50it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.11it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02&lt;00:00, 15.18it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:15&lt;00:00, 10.27it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 13.19it/s]\n100%|█████████████████████████████████████████████████████████████████████████████████| 157/157 [00:16&lt;00:00,  9.69it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03&lt;00:00, 11.73it/s]\n\n\n[1] loss: 0.581\n[1] loss: 0.673 acc:0.754\n[2] loss: 0.452\n[2] loss: 0.662 acc:0.760\n[3] loss: 0.353\n[3] loss: 0.648 acc:0.773\n[4] loss: 0.263\n[4] loss: 0.693 acc:0.773\n[5] loss: 0.204\n[5] loss: 0.745 acc:0.776\n[6] loss: 0.162\n[6] loss: 0.762 acc:0.769\n[7] loss: 0.131\n[7] loss: 0.781 acc:0.776\n[8] loss: 0.093\n[8] loss: 0.875 acc:0.777\n[9] loss: 0.080\n[9] loss: 0.899 acc:0.777\n[10] loss: 0.064\n[10] loss: 0.884 acc:0.781\n[11] loss: 0.053\n[11] loss: 0.950 acc:0.776\n[12] loss: 0.047\n[12] loss: 0.960 acc:0.781\n[13] loss: 0.039\n[13] loss: 0.964 acc:0.781\n[14] loss: 0.032\n[14] loss: 0.955 acc:0.785\n[15] loss: 0.026\n[15] loss: 1.009 acc:0.780\n[16] loss: 0.028\n[16] loss: 1.056 acc:0.782\n[17] loss: 0.028\n[17] loss: 0.996 acc:0.786\n[18] loss: 0.026\n[18] loss: 1.004 acc:0.788\n[19] loss: 0.023\n[19] loss: 1.029 acc:0.785\n[20] loss: 0.019\n[20] loss: 1.042 acc:0.785\nDone!!"
  },
  {
    "objectID": "posts/CV_classification_0.html#모델-성능-평가",
    "href": "posts/CV_classification_0.html#모델-성능-평가",
    "title": "CV_classification_0",
    "section": "- 모델 성능 평가",
    "text": "- 모델 성능 평가\n\n커스텀 데이터셋 테스트\n\nmodel_path = 'cifar_resnet.pth'\n# 모델 가중치 업로드\nmodel.load_state_dict(torch.load(model_path))\n# 테스트\ntest(custom_loader)\n\nAccuracy of the network on the 10000 test images: 68%\nAccuracy for class: plane is 100.0%\nAccuracy for class: car   is 60.0%\nAccuracy for class: bird  is 40.0%\nAccuracy for class: cat   is 80.0%\nAccuracy for class: dog   is 60.0%\n\n\n\n\n테스트 데이터셋 테스트\n\nmodel_path = 'cifar_resnet.pth'\n# 모델 가중치 업로드\nmodel.load_state_dict(torch.load(model_path))\ntest(test_loader)\n\nAccuracy of the network on the 10000 test images: 80%\nAccuracy for class: plane is 85.0%\nAccuracy for class: car   is 88.0%\nAccuracy for class: bird  is 75.9%\nAccuracy for class: cat   is 62.5%\nAccuracy for class: deer  is 77.1%\nAccuracy for class: dog   is 71.7%\nAccuracy for class: frog  is 85.6%\nAccuracy for class: horse is 84.0%\nAccuracy for class: ship  is 84.6%\nAccuracy for class: truck is 85.7%"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DL_tutorial",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 9, 2024\n\n\nCV_ObjectDetection_0\n\n\n이정재 \n\n\n\n\nFeb 5, 2024\n\n\nCV_classification_0\n\n\n이정재 \n\n\n\n\nJan 31, 2024\n\n\nLMTM\n\n\n이정재 \n\n\n\n\nJan 30, 2024\n\n\nVGGNet\n\n\n이정재 \n\n\n\n\nJan 29, 2024\n\n\nAlexNet\n\n\n이정재 \n\n\n\n\nJan 28, 2024\n\n\nANN_2\n\n\n이정재 \n\n\n\n\nJan 27, 2024\n\n\nANN_1\n\n\n이정재 \n\n\n\n\n\nNo matching items"
  }
]